# Categorical Composition: Syntax {#categories}

This chapter focuses on splitting vocabulary data into syntactic categories and analyzing consistency and variability across languages in the acquisition of these. We quantify the “noun bias” across languages. In addition, we report the degree of bias for or against verbs and closed-class words.^[An earlier version of this work was reported to BUCLD 2015 by Braginsky, Marchman, Yurovsky, & Frank.]

## Introduction

Over the first few years, young children are exposed to a "sea of words" across many different contexts and from many different people (@goodman2008, p. 516). And despite the fact that children vary tremendously in the rate at which they learn, the first words that children utter are strikingly consistent [@tardif2008,@schneider2015]: they tend to talk about important people in their life ("mom", "dad"), social routines ("hi", "uh oh"), animals ("dog", "duck"), and foods ("milk", "banana") (@goodman2008, @bates1995, @nelson1973, @clark1979).  Soon thereafter, they begin to add verbs ("go") and adjectives ("pretty") in greater proportions than earlier in development and may even begin to use closed-class forms, such as determiners ("the").  These patterns seem to suggest a developmental course that follows distinct "waves" of learning for words from different syntactic classes. That is, along with early social routines, nouns tend to predominate early vocabularies, while other types of words, such as predicates and closed class forms, are learned later.

@bates1994 characterized these patterns of vocabulary composition in the following way. Figure 4.1 shows average vocabluary composition of nominals, predicates and closed class forms as a function of children's vocabulary size for English-speaking children from the original norming study of CDI: Words & Sentences form (@fenson1993).  Note that when children only know a few words (e.g., fewer than 50 words), the nominals comprise the greatest proportion of the words that children are reported to produce, with very few predicates or closed class forms (< 10%). As the children learn the next hundred words or so, the proportion of nominals increases even more dramatically with a gradual increase in the proportion of children's vocabularies that are predicates. Closed class forms remain a much smaller proportion over the period. Yet after about 300 words or so, children tend not add nouns to their vocabularies at the same pace that they did earlier in development, reflected in the proportion of nominals tending to decrease.^[This effect may also reflect aspects of CDI form design, e.g. "running out of nouns" to learn: children may increasingly learning nouns that are not on the forms.] It is during this developmental period that proportion of predicates tend to increase, followed by a growing proportion of closed class forms. 

[Figure from Bates et al. (1994), showing developmental trends in the categorical composition of early vocabulary.](images/bates1994.png)

Why do children learn nouns before verbs and other types of words?  This question has received a great deal of attention in the literature, and we can briefly summarize some of the major issues here.  One reason for this "noun bias" could be that nouns are simply more frequent in the talk to young children.  It is well-established that children learn the words that they hear more often [e.g., @hartrisley1995].  Many observational studies of English-speaking caregivers have demonstrated that caregivers use more nouns than verbs (types or tokens) with their children [e.g., @fernald1993; @goldfield1993; @gopnik1996; @kim2000; @poulindubois1995; @tardif1997].

Other researchers have framed the "noun bias" in terms of universals about what and how different words "partition" things in the world.For example, Gentner [@gentner1978] has argued that children learn nouns before verbs because the meanings of nouns are easier to encode since they identify things that can be differentiated in the world (e.g., common everyday objects).  Verbs and other predicates, in contrast, express *relations* among things in the world.  Hence, the meanings of verbs are less accessible to children through common, everyday experiences and hence, are more difficult to map onto word forms without additional linguistic or social support. 

Other reasons that nouns might be easier than verbs for young children is that nouns tend to be less morphologically complex than verbs [e.g., @tardif1997]. For example, in many languages, nouns are typically marked only for number, whereas, verbs carry both person and tense information.  In English, at least, verbs might also be harder to learn because they tend to occur in sentence-medial position (rather than sentence final), which make verbs less salient in the input that children hear [@slobin1985, @caselli1995].

Finally, differences in children's preferences for nouns vs. verbs might result from differences in what contexts children hear nouns vs. verbs in the speech from caregivers [e.g., @choi1995, @tardif1999].  Several researchers have examined what caregivers talk about using naturalistic data of caregiver-child interactions.  For example, caregivers in some cultures tend to emphasize the names for objects, spending a great deal of time labeling objects for their children.  In other cultures, caregivers do so much less frequently, instead focusing on the actions in which those objects engage [e.g., @fernald1993; @gopnik1995]. These differences in input to children can influence the words that are salient for children, and hence, the words that they are most likely to learn.

What is the evidence that a noun bias is a universal feature of children's vocabularies? Documenting the extent to which the noun bias is universal is relevant to understanding mechanisms of language learning, in particular, the presence of conceptual biases in early acquisition and the role of cross-cultural variability in the input that children receive from caregivers. The evidence varies across languages, as well as across methodologies (for example, naturalistic observation vs. parent report). Some studies find consistent evidence for a noun bias in English, as well as in Korean and Italian [@bates1994; @au1994; @caselli1995; @kim2000]. Other studies do not find evidence of a noun bias in languages as varied as French, German, Chinese, Estonian, and Korean [@bassano2000; @bloom1993; @choi1995; @kauschke2002; @tardif1996; @tardif1999, @schults2016].  Crosslinguistic variation suggests that the words that young children learn is less a function of universal conceptual biases and more likely due to structural features of the language and characteristics of the input that children receive when engaging with caregivers. Identifying the extent of crosslinguistic variation vs. universals has been difficult since variation across studies may be due to the different methodologies that are used. For example, even within a single language, for example, Korean, parent reports of children's first words find a noun bias [e.g., @au1994], whereas, studies using direct observational methods find less evidence for this pattern [e.g., @gopnik1995].

Few studies have had the scope to directly compare the extent of the noun bias across multiple languages using a common methodology.  One notable exception is @bornstein2004, in which the researchers compared vocabulary composition in seven different languages.  We follow this approach [see also @tardif2008] using the CDI data from Wordbank. Since Wordbank has access to many more observations, our approach offers a more comprehensive approach than these earlier studies.  Moreover, we attempt to quantify the estimates of the extent to which languages show a noun bias: we develp a statistical method for quantifying the extent of the noun bias across the entire developmental range in which a particular form is used. 

## Methods

Each CDI form contains a mixture of words in different classes. We adopt the categorization of @bates1994, categorizing words into nouns, predicates (both verbs and adjectives), function words (also referred to as "closed class" words), and other words. For each child's vocabulary, we compute the proportion of the total words in each of these categories that they are reported to produce. Following the approach developed by @bates1994, for each of the languages in our sample, we plot these proportions against total vocabulary. Each dot represents a child's knowledge of a particular class, while curves show the relationship between a class and the whole vocabulary. If categories grow independently of one another, these curves should approximate the diagonal. 

```{r 4_items}
items <- items %>%
  filter(type == "word") %>%
  mutate(num_item_id = as.numeric(substr(item_id, 6, nchar(item_id))))
```

```{r 4_vocab_comp_fun}
get_vocab_comp <- function(input_language, input_form) {
  print(paste(input_language,input_form))
  
  lang_vocab_items <- filter(items, 
                             language == input_language, 
                             form == input_form) %>%
    filter(lexical_category %in% c("nouns", "predicates", "function_words"))
  
  lang_vocab_data <- get_instrument_data(instrument_language = input_language,
                                         instrument_form = input_form,
                                         items = lang_vocab_items$item_id, 
                                         iteminfo = lang_vocab_items) %>%
    mutate(value = ifelse(is.na(value), "", value),
           produces = value == "produces",
           understands = value == "produces" | value == "understands") %>%
    dplyr::select(-value) %>%
    gather(measure, value, produces, understands)
  
  num_words <- nrow(lang_vocab_items)
  
  lang_vocab_summary <- lang_vocab_data %>%
    group_by(data_id, measure, lexical_category) %>%
    summarise(num_true = sum(value),
              prop = sum(value) / n())
  
  lang_vocab_sizes <- lang_vocab_summary %>%
    summarise(vocab_num = sum(num_true),
              vocab = sum(num_true) / num_words)
  
  lang_vocab_summary %>%
    left_join(lang_vocab_sizes) %>%
    mutate(prop_vocab = num_true / vocab_num) %>%
    dplyr::select(-num_true) %>%
    mutate(language = input_language, form = input_form)
}
```

```{r 4_vocab_comp, eval=FALSE}
instruments <- instruments %>%
  filter(form %in% c("WS","WG")) %>%
  dplyr::select(language, form) %>%
  distinct()

vocab_comp_data <- map2(instruments$language,
                        instruments$form, get_vocab_comp) %>%
  bind_rows()
```

```{r 4_cached_data}
vocab_comp_data <- read_feather("data/vocab_comp_data.feather")
```

### Data 

We limit our analysis to traditional WS and WG forms for because short forms like the British English TEDS don't have category information. The sample sizes included in this analysis are given below. 

```{r 4_sample_sizes}
sample_sizes <- vocab_comp_data %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  dplyr::select(language, form, n) %>%
  distinct() 

sample_sizes %>%
  datatable
```

### Estimation Method


```{r 4_area_funs}
get_lang_lexcat_predictions <- function(lang, lexcat) {
  model <- filter(models, language == lang, lexical_category == lexcat)$model[[1]]
  data.frame(vocab = pts,
             prop = predict(model, newdata = data.frame(vocab = pts)),
             lexical_category = lexcat,
             language = lang)
}

get_lang_predictions <- function(lang) {
  bind_rows(sapply(unique(demo_data$lexical_category),
                   function(lexcat) get_lang_lexcat_predictions(lang, lexcat),
                   simplify = FALSE))
}
```

```{r 4_plot_area_demo}
demo_langs <- c("English", "Mandarin")
demo_data <- filter(vocab_comp_data, form == "WS", language %in% demo_langs) %>%
  mutate(panel = paste(language, "(data)"),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns", "Predicates", "Function Words")))
pts <- seq(0, 1, 0.01)

models <- demo_data %>%
  group_by(language, lexical_category) %>%
  do(model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1, data = .))

predictions <- bind_rows(sapply(demo_langs, get_lang_predictions, simplify = FALSE))

diagonal <- expand.grid(vocab = rep(rev(pts)),
                        language = demo_langs,
                        lexical_category = unique(demo_data$lexical_category))
diagonal$prop <- diagonal$vocab

area_poly <- bind_rows(predictions, diagonal) %>%
  mutate(panel = paste(language, "(models)"))

ggplot(filter(predictions, language == "English"), 
       aes(x = vocab, y = prop)) +
  facet_grid(. ~ lexical_category) +
  geom_line(aes(colour = lexical_category), size = 1) +
  geom_polygon(data = filter(area_poly, language == "English"),
               aes(fill = lexical_category), alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  scale_y_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_x_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_colour_solarized(guide = FALSE) +
  scale_fill_solarized(guide = FALSE) 
```



```{r 4_resample}
sample_areas <- function(d, nboot = 1000) {
  
  poly_area <- function(group_data) {
    model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1,
                data = group_data)
    return((model$solution %*% c(1/4, 1/3, 1/2) - 0.5)[1])
  }
  
  counter <- 1
  sample_area <- function(d) {
    d_frame <- d %>%
      group_by(language, form, measure) %>%
      sample_frac(replace = TRUE) %>%
      group_by(language, form, measure, lexical_category) %>%
      do(area = poly_area(.)) %>%
      mutate(area = area[1]) %>%
      rename_(.dots = setNames("area", counter))
    
    counter <<- counter + 1 # increment counter outside scope
    return(d_frame)
  }
  
  areas <- replicate(nboot, sample_area(d), simplify = FALSE)
  
  Reduce(left_join, areas) %>%
    gather(sample, area, -language, -form, -measure, -lexical_category)
}
```

```{r 4_areas, eval=FALSE}
areas <- sample_areas(vocab_comp_data, 100)
write_feather(areas,"data/vocab_comp_areas.feather")
```


```{r 4_areas2}
areas <- read_feather("data/vocab_comp_areas.feather")
area_summary <- areas %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(mean =  mean(area),
            ci_lower = ci_lower(area),
            ci_upper = ci_upper(area)) %>%
  ungroup() %>%
  mutate(language = factor(language),
         instrument = paste(language, form))

area_order <- filter(area_summary, form == "WS", measure == "produces",
                     lexical_category == "nouns")

language_levels <- area_order$language[order(area_order$mean,
                                             area_order$language,
                                             decreasing = FALSE)]

area_summary_ordered <- area_summary %>%
  filter(form %in% c("WS", "WG"),
         !(form == "WS" & measure == "understands")) %>%
  ungroup() %>%
  mutate(language = factor(language, levels = language_levels),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns", "Predicates", "Function Words")))
```

How do we estimate over-representation or under-representation of a particular vocabulary item? The figure above gives the key intuition. For a particular population, we plot the average proportion of items produced or comprehended from each category, relative to the total. This category average is plotted with. 


Resample data and find the mean and CI of the area estimate.


## Syntactic Vocabulary Composition

Base plot for looking at vocabulary composition.

```{r 4_base_plot}
base_plot <- function(input_form, input_measure) {
  vocab_comp_data %>%
    filter(form == input_form, measure == input_measure) %>%
    mutate(lexical_category = factor(lexical_category,
                                     levels = c("nouns", "predicates", "function_words"),
                                     labels = c("Nouns  ", "Predicates  ", "Function Words"))) %>%
    ggplot(aes(x = vocab, y = prop, colour = lexical_category)) +
    facet_wrap(~language) +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                       name = "Proportion of Category\n") +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                       name = "\nVocabulary Size") +
    scale_colour_solarized(name = "") +
    theme(legend.position = "top",
          legend.key = element_blank(),
          legend.background = element_rect(fill = "transparent"))
}
```

### Production: Words and Sentences

Plot WS productive vocabulary composition as a function of vocabulary size for each language.

```{r 4_plot_points_ws, fig.width=8, fig.height=8}
base_plot("WS", "produces") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3)  +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```

### Production: Words and Gestures

Plot WG productive vocabulary composition as a function of vocabulary size for each language.

These data are *much* sparser and don't really license strong inferences in many cases. (Most children produce a relatively small number of the words on WG forms).

```{r 4_plot_points_wg, fig.width=8, fig.height=8}
base_plot("WG", "produces") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3)  +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```
### Comprehension: Words and Gestures

Plot WG receptive vocabulary composition as a function of vocabulary size for each language.

```{r 4_plot_points_wg_comp, fig.width=8, fig.height=8}
base_plot("WG", "understands") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3)  +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```


### Summary across and measures

Plot each lexical category's area estimate by language, form, and measure.

```{r 4_plot_areas, fig.width=6, fig.height=6}
ggplot(area_summary_ordered,
       aes(y = language, x = mean, colour = lexical_category)) +
  facet_grid(lexical_category ~ form + measure) +
  geom_point() +
  geom_segment(aes(x = ci_lower, xend = ci_upper,
                   y = language, yend = language)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  scale_y_discrete(name = "", limits = levels(area_summary_ordered$language)) +
  xlab("\nRelative representation in early vocabulary")
```

WS only. 

```{r 4_plot_areas_WS}
ggplot(filter(area_summary_ordered, form == "WS"), 
       aes(y = language, x = mean, colour = lexical_category)) +
  facet_wrap(~lexical_category) +
  geom_point() +
  geom_segment(aes(x = ci_lower, xend = ci_upper,
                   y = language, yend = language)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  ylab("") + 
  # scale_y_discrete(name = "", limits = levels(area_summary_ordered$language)) +
  xlab("Relative representation in early vocabulary")
```

Extract CVs. 

```{r}
cvs <- area_summary_ordered %>%
  filter(!is.na(language)) %>%
  group_by(form, measure, lexical_category) %>%
  summarise(cv = abs(sd(mean) / mean(mean))) %>%
  ungroup %>%
  unite(measure, lexical_category, form, measure, sep = " ")
  
write_feather(cvs,"data/cvs/vocab_comp.feather")
```



Plot WS production vocab composition proportions. 

```{r 4_prod_vs_comp_vocab_comp}
area_scatter_data <- area_summary %>%
  filter(form=="WS", 
         lexical_category %in% c("nouns","predicates"), 
         measure == "produces") %>%
  gather(variable, value, mean, ci_lower, ci_upper) %>%
  mutate(lex_var = str_c(lexical_category, variable, sep = "_")) %>%
  dplyr::select(-lexical_category, -variable) %>%
  spread(lex_var, value)

ggplot(area_scatter_data, 
       aes(x = nouns_mean, y = predicates_mean)) + 
  geom_pointrange(aes(ymin = predicates_ci_lower, ymax = predicates_ci_upper)) + 
  geom_errorbarh(aes(xmin = nouns_ci_lower, xmax = nouns_ci_upper)) +
  geom_smooth() + 
  geom_label_repel(aes(label=language), force = 4, alpha = .8) +
  xlab("Noun bias") +
  ylab("Predicate bias")
```


<!-- ## Age effects on vocabulary composition -->

<!-- We predict that the proportion of predicates and function words in children's vocabulary should be relatively more affected by age than nouns. Concrete nouns are hypothesized to be learned initially from both co-occurrences between words [@yu2007b] and by social cues to reference to particular objects [@bloom2002]. On neither account should syntactic information be a primary information source (though of course syntax might be more informative for abstract nouns). In contrast, for other types of words, syntax should be more important for learning their meaning. -->

<!-- On the syntactic bootstrapping hypothesis [@gleitman1990;@fisher2010], verbs especially are learned by mapping the syntactic structure of utterances to the thematic structure of observed events, for example by noticing that the subject of a sentence matches the agent in one particular ongoing event but not another (``the cat is fleeing the dog'' matches *flees*(cat, dog) but not *chases*(dog,cat)). A similar argument can be made for adjectives, since identification of the modified noun is similarly critical for inferring the meaning of the modifier. And by the same logic, function words should be even harder to learn without some understanding of their syntactic role. Thus, if syntactic development is related in some way to age, we should see larger age effects on predicate and function word vocabulary than on noun vocabulary.  -->

<!-- In all four languages, the age coefficient is substantially larger for function words than for nouns. This asymmetry can be interpreted as evidence that, for two vocabulary-matched children, the older child would tend to produce relatively more function words than the younger.  -->





