The Categorical Composition of Early Vocabulary. This chapter focuses on splitting vocabulary data into syntactic and semantic categories and analyzing consistency and variability across languages in the acquisition of these. 

Syntactic category biases. We quantify the “noun bias” across languages. In addition, we report the degree of bias for or against verbs and closed-class words. Previously presented in Braginsky et al. (2015b).

Conceptual categories. We investigate the consistency of colors, spatial terms, logical terms, and other conceptual categories of interest. Some of these semantic categories have been argued to be learned as a group, so we investigate whether they are more consistently correlated within children than would be expected by chance. 

# The Categorical Composition of Early Vocabulary {#categories}

An earlier version of this work was reported to BUCLD 2015 by Braginsky, Marchman, Yurovsky, & Frank.

Early vocabulary development is typically characterized by learning of names for caregivers and common objects, while later in development, children tend to diversify their vocabulary by increasing the proportion of predicates (verbs and adjectives) and closed class words. This over-representation of nouns has been found across a number of analyses and in a variety of languages [@bates1994;@caselli1995;@bornstein2004], though not all [@tardif1996;@choi1995].

Differences in early vocabulary composition have been argued to emerge from typological differences (e.g., word order, subject drop), and from cultural practices (e.g., focus on picture book reading) [@tardif1999;@gopnik1996;@choi1995]---we are agnostic as to the source of this variability.

[Figure from Bates et al. (1994), showing developmental trends in the categorical composition of early vocabulary.](figures/bates1994.png)

## Methods

Each CDI form contains a mixture of words in different classes. We adopt the categorization of @bates1994, splitting words into nouns, predicates (verbs and adjectives), function words, and other words. For each child's vocabulary, we compute the proportion of the total words in each of these categories that they are reported to produce.

For each of the four languages in our sample, we plot these proportions against total vocabulary. Each dot represents a child's knowledge of a particular class, while curves show the relationship between a class and the whole vocabulary. If categories grow independently of one another, these curves should approximate the diagonal. This pattern is not what we observe, however: Across the languages in our sample, nouns are systematically over-represented in smaller vocabularies (shown by a curve that is above the diagonal), while function words---and to some extent, predicates---are under-represented. 

```{r database}
items <- items %>%
  filter(type == "word") %>%
  mutate(num_item_id = as.numeric(substr(item_id, 6, nchar(item_id))))
```

```{r vocab_comp_fun}
get_vocab_comp <- function(input_language, input_form) {
  print(paste(input_language,input_form))
  
  lang_vocab_items <- filter(items, 
                             language == input_language, 
                             form == input_form) %>%
    filter(lexical_category %in% c("nouns", "predicates", "function_words"))
  
  lang_vocab_data <- get_instrument_data(instrument_language = input_language,
                                         instrument_form = input_form,
                                         items = lang_vocab_items$item_id, 
                                         iteminfo = lang_vocab_items) %>%
    mutate(value = ifelse(is.na(value), "", value),
           produces = value == "produces",
           understands = value == "produces" | value == "understands") %>%
    select(-value) %>%
    gather(measure, value, produces, understands)
  
  num_words <- nrow(lang_vocab_items)
  
  lang_vocab_summary <- lang_vocab_data %>%
    group_by(data_id, measure, lexical_category) %>%
    summarise(num_true = sum(value),
              prop = sum(value) / n())
  
  lang_vocab_sizes <- lang_vocab_summary %>%
    summarise(vocab_num = sum(num_true),
              vocab = sum(num_true) / num_words)
  
  lang_vocab_summary %>%
    left_join(lang_vocab_sizes) %>%
    mutate(prop_vocab = num_true / vocab_num) %>%
    select(-num_true) %>%
    mutate(language = input_language, form = input_form)
}
```

```{r vocab_comp, eval=FALSE}
instruments <- instruments %>%
  filter(form %in% c("WS","WG")) %>%
  select(language, form) %>%
  distinct()

vocab_comp_data <- map2(instruments$language,
                        instruments$form, get_vocab_comp) %>%
  bind_rows()
```

```{r cached}
vocab_comp_data <- read_feather("data/vocab_comp_data.feather")
```

We limit our analysis to traditional WS and WG forms for now because short forms like the British English TEDS don't have category information. The sample sizes included in this analysis are given below. 

```{r sample_sizes}
sample_sizes <- vocab_comp_data %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  select(language, form, n) %>%
  distinct() 

sample_sizes %>%
  datatable
```

### Estimation Method

How do we estimate over-representation or under-representation of a particular vocabulary item?


```{r plot_area_demo, fig.width = 9, fig.height = 3.5}
demo_langs <- c("English", "Mandarin")
demo_data <- filter(vocab_comp_data, form == "WS", language %in% demo_langs) %>%
  mutate(panel = paste(language, "(data)"),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns", "Predicates", "Function Words")))
pts <- seq(0, 1, 0.01)

models <- demo_data %>%
  group_by(language, lexical_category) %>%
  do(model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1, data = .))

get_lang_lexcat_predictions <- function(lang, lexcat) {
  model <- filter(models, language == lang, lexical_category == lexcat)$model[[1]]
  data.frame(vocab = pts,
             prop = predict(model, newdata = data.frame(vocab = pts)),
             lexical_category = lexcat,
             language = lang)
}

get_lang_predictions <- function(lang) {
  bind_rows(sapply(unique(demo_data$lexical_category),
                   function(lexcat) get_lang_lexcat_predictions(lang, lexcat),
                   simplify = FALSE))
}

predictions <- bind_rows(sapply(demo_langs, get_lang_predictions, simplify = FALSE))

diagonal <- expand.grid(vocab = rep(rev(pts)),
                        language = demo_langs,
                        lexical_category = unique(demo_data$lexical_category))
diagonal$prop <- diagonal$vocab

area_poly <- bind_rows(predictions, diagonal) %>%
  mutate(panel = paste(language, "(models)"))

ggplot(filter(predictions, language == "English"), 
       aes(x = vocab, y = prop)) +
  facet_grid(. ~ lexical_category) +
  geom_line(aes(colour = lexical_category), size = 1) +
  geom_polygon(data = filter(area_poly, language == "English"),
               aes(fill = lexical_category), alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  scale_y_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_x_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_colour_solarized(guide = FALSE) +
  scale_fill_solarized(guide = FALSE) 
# +
#   theme(legend.position = c(0.061, 0.91),
#         legend.text = element_text(size = 8),
#         legend.key.height = unit(0.9, "char"),
#         legend.key.width = unit(0.88, "char"),
#         legend.background = element_rect(fill = "transparent"),
#         strip.background = element_blank(),
#         strip.text.x = element_blank())
```


Function for resampling data and computing area estimate for each sample.

```{r resample}
sample_areas <- function(d, nboot = 1000) {
  
  poly_area <- function(group_data) {
    model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1,
                data = group_data)
    return((model$solution %*% c(1/4, 1/3, 1/2) - 0.5)[1])
  }
  
  counter <- 1
  sample_area <- function(d) {
    d_frame <- d %>%
      group_by(language, form, measure) %>%
      sample_frac(replace = TRUE) %>%
      group_by(language, form, measure, lexical_category) %>%
      do(area = poly_area(.)) %>%
      mutate(area = area[1]) %>%
      rename_(.dots = setNames("area", counter))
    
    counter <<- counter + 1 # increment counter outside scope
    return(d_frame)
  }
  
  areas <- replicate(nboot, sample_area(d), simplify = FALSE)
  
  Reduce(left_join, areas) %>%
    gather(sample, area, -language, -form, -measure, -lexical_category)
}
```

Resample data and find the mean and CI of the area estimate.

```{r areas, eval=FALSE}
areas <- sample_areas(vocab_comp_data, 100)
write_feather(areas,"data/vocab_comp_areas.feather")
```


```{r areas2}
areas <- read_feather("data/vocab_comp_areas.feather")
area_summary <- areas %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(mean =  mean(area),
            ci_lower = ci_lower(area),
            ci_upper = ci_upper(area)) %>%
  ungroup() %>%
  mutate(language = factor(language),
         instrument = paste(language, form))

area_order <- filter(area_summary, form == "WS", measure == "produces",
                     lexical_category == "nouns")

language_levels <- area_order$language[order(area_order$mean,
                                             area_order$language,
                                             decreasing = FALSE)]

area_summary_ordered <- area_summary %>%
  filter(form %in% c("WS", "WG"),
         !(form == "WS" & measure == "understands")) %>%
  ungroup() %>%
  mutate(language = factor(language, levels = language_levels),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns", "Predicates", "Function Words")))
```


## Syntactic Vocabulary Composition

Base plot for looking at vocabulary composition.

```{r base_plot}
base_plot <- function(input_form, input_measure) {
  vocab_comp_data %>%
    filter(form == input_form, measure == input_measure) %>%
    mutate(lexical_category = factor(lexical_category,
                                     levels = c("nouns", "predicates", "function_words"),
                                     labels = c("Nouns  ", "Predicates  ", "Function Words"))) %>%
    ggplot(aes(x = vocab, y = prop, colour = lexical_category)) +
    facet_wrap(~language) +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                       name = "Proportion of Category\n") +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                       name = "\nVocabulary Size") +
    scale_colour_solarized(name = "") +
    theme(legend.position = "top",
          legend.key = element_blank(),
          legend.background = element_rect(fill = "transparent"))
}
```

### Production: Words and Sentences

Plot WS productive vocabulary composition as a function of vocabulary size for each language.

```{r plot_points_ws, fig.width=8, fig.height=8}
base_plot("WS", "produces") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3)  +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```

### Production: Words and Gestures

Plot WG productive vocabulary composition as a function of vocabulary size for each language.

These data are *much* sparser and don't really license strong inferences in many cases. (Most children produce a relatively small number of the words on WG forms).

```{r plot_points_wg, fig.width=8, fig.height=8}
base_plot("WG", "produces") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3)  +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```
### Comprehension: Words and Gestures

Plot WG receptive vocabulary composition as a function of vocabulary size for each language.

```{r plot_points_wg_comp, fig.width=8, fig.height=8}
base_plot("WG", "understands") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3)  +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```


### Summary across and measures

Plot each lexical category's area estimate by language, form, and measure.

```{r plot_areas, fig.width=6, fig.height=6}
ggplot(area_summary_ordered,
       aes(y = language, x = mean, colour = lexical_category)) +
  facet_grid(lexical_category ~ form + measure) +
  geom_point() +
  geom_segment(aes(x = ci_lower, xend = ci_upper,
                   y = language, yend = language)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  scale_y_discrete(name = "", limits = levels(area_summary_ordered$language)) +
  xlab("\nRelative representation in early vocabulary")
```

Plot WS production vocab composition proportions. 

```{r}
area_scatter_data <- area_summary %>%
  filter(form=="WS", 
         lexical_category %in% c("nouns","predicates"), 
         measure == "produces") %>%
  gather(variable, value, mean, ci_lower, ci_upper) %>%
  mutate(lex_var = str_c(lexical_category, variable, sep = "_")) %>%
  select(-lexical_category, -variable) %>%
  spread(lex_var, value)

ggplot(area_scatter_data, 
       aes(x = nouns_mean, y = predicates_mean)) + 
  geom_pointrange(aes(ymin = predicates_ci_lower, ymax = predicates_ci_upper)) + 
  geom_errorbarh(aes(xmin = nouns_ci_lower, xmax = nouns_ci_upper)) +
  geom_smooth() + 
  geom_label_repel(aes(label=language), force = 4, alpha = .8) +
  xlab("Noun bias") +
  ylab("Predicate bias")
```


## Age effects on vocabulary composition

We predict that the proportion of predicates and function words in children's vocabulary should be relatively more affected by age than nouns. Concrete nouns are hypothesized to be learned initially from both co-occurrences between words [@yu2007b] and by social cues to reference to particular objects [@bloom2002]. On neither account should syntactic information be a primary information source (though of course syntax might be more informative for abstract nouns). In contrast, for other types of words, syntax should be more important for learning their meaning.

On the syntactic bootstrapping hypothesis [@gleitman1990;@fisher2010], verbs especially are learned by mapping the syntactic structure of utterances to the thematic structure of observed events, for example by noticing that the subject of a sentence matches the agent in one particular ongoing event but not another (``the cat is fleeing the dog'' matches *flees*(cat, dog) but not *chases*(dog,cat)). A similar argument can be made for adjectives, since identification of the modified noun is similarly critical for inferring the meaning of the modifier. And by the same logic, function words should be even harder to learn without some understanding of their syntactic role. Thus, if syntactic development is related in some way to age, we should see larger age effects on predicate and function word vocabulary than on noun vocabulary. 

In all four languages, the age coefficient is substantially larger for function words than for nouns. This asymmetry can be interpreted as evidence that, for two vocabulary-matched children, the older child would tend to produce relatively more function words than the younger. 



## Semantic Category Representation

The same analyses could be done with CDI categories, which are semantic rather than lexical


