# Categorical Composition: Syntax {#categories}


This chapter focuses on splitting vocabulary data into syntactic categories and analyzing consistency and variability across languages in the acquisition of these. We quantify the “noun bias” across languages. In addition, we report the degree of bias for or against verbs and closed-class words.^[An earlier version of this work was reported to BUCLD 2015 by Braginsky, Marchman, Yurovsky, & Frank.]

## Introduction

Over the first few years, young children are exposed to a "sea of words" across many different contexts and from many different people (@goodman2008, p. 516). And despite the fact that children vary tremendously in the rate at which they learn, the first words that children utter are strikingly consistent [@tardif2008,@schneider2015]: they tend to talk about important people in their life ("mom", "dad"), social routines ("hi", "uh oh"), animals ("dog", "duck"), and foods ("milk", "banana") (@goodman2008, @bates1995, @nelson1973).  Soon thereafter, they begin to add verbs ("go") and adjectives ("pretty") in greater proportions than earlier in development and may even begin to use closed-class forms, such as determiners ("the").  These patterns of development seem to suggest a developmental course that follows distinct "waves" of learning for words from different syntactic classes. That is, along with early social routines, nouns tend to predominate early vocabularies, while other types of words, such as predicates and closed class forms, are learned later.

The composition of children's early vocabularies, including the presence of a "noun bias" has been an important subject of study because

Bates et al. (1994) characterizes vocabulary composition in the following way. Figure 4.1 shows average vocabluary composition as a function of children's vocabulary size for English-speaking children from the original norming study of CDI Words & Sentences form (Fenson et al., 1993).  Note that when children only know a few words (e.g., fewer than 50 words), the majority of the words are nouns, with very few predicates or closed class forms (< 10%). As the children learn the next hundred words or so, the prevalence of nouns increases even more dramatically with a gradual increase in the proportion of children's vocabularies that are predicates. Closed class forms remain a much smaller proportion.  After about 300 words, vocabulary composition children are not adding nouns  dominant children's first words are primarily nouns, 

[Figure from Bates et al. (1994), showing developmental trends in the categorical composition of early vocabulary.](images/bates1994.png)

Why do children learn nouns before verbs and other types of words?  One reason for this "noun bias" in children's early vocabulary composition is could be that nouns are simply more frequent in the talk to young children.  It is well-established that children learn the words that they hear more often (e.g., @hartrisley1995).  Many observational studies of English-speaking caregivers have demonstrated that caregivers use more nouns than verbs (types or tokens) with their children (e.g., @fernald1993; @goldfield1993; Gopnik, Choi, & Baumberger, 1996; Kim et al., 2000; Poulin-Dubois et al., 1995; Tardif et al., 1997).  


Nouns are also more frequent than verbs.  Some researchers have framed this "noun bias" in terms of universals about what and how different words "partition" things in the world.  For example, Gentner (@gentner1978) has argued that children learn nouns before verbs because the meanings of nouns are easier to encode since they identify things that can be differentiated in the world (e.g., common everyday objects).  Verbs and other predicates, in contrast, express *relations* among things in the world.  Hence, the meanings of verbs are less accessible to children through common, everyday experiences and hence, are more difficult to map onto word forms without additional linguistic or social support. 


Other reasons that nouns might be easier than verbs for young children is that nouns tend to be less morphologically complex than verbs (e.g., @tardif1997). For example, in many languages, nouns are typically marked only for number, whereas, verbs carry both person and tense information.  In English, at least, verbs might also be harder to learn because they tend to occur in sentence-medial position (rather than sentence final), which make verbs less salient in the input that children hear (@slobin1985, @caselli?).

Finally, differences in children's preferences for nouns vs. verbs might result from differences in how frequently or in what context children hear nouns vs. verbs in the speech from caregivers (e.g., @choi1995, @tardif1999).  Caregivers in different cultures tend to emphasize the names for objects rather than the actions in which those objects engage.

What is the evidence that a noun bias is a universal feature of children's vocabularies? (Does this come before or after the possible explanations?)  Evidence varies across languages, as well as across methodologies (naturalistic observation vs. parent report).  Here, we follow the approach of Bornstein and others (e.g., Schults - Estonian) to use a common method for collecting the data. HOwever, we go beyond these studies by having access to many more languages with many more observations.

Notes: Finally, yet other studies have found evidence for a noun bias across languages but only in later stages of vocabulary learning (i.e., after the first 50 words; Bornstein et al., 2004).

Babies first ten words - the probability that certain words from certain lexical classes appear in the first productions of children's words.  Our strategy follows from that and tries to quantify the degree to which words from different lexical classess appear in the early vocabulary. 






## Methods

Each CDI form contains a mixture of words in different classes. We adopt the categorization of @bates1994, splitting words into nouns, predicates (verbs and adjectives), function words, and other words. For each child's vocabulary, we compute the proportion of the total words in each of these categories that they are reported to produce.

For each of the four languages in our sample, we plot these proportions against total vocabulary. Each dot represents a child's knowledge of a particular class, while curves show the relationship between a class and the whole vocabulary. If categories grow independently of one another, these curves should approximate the diagonal. This pattern is not what we observe, however: Across the languages in our sample, nouns are systematically over-represented in smaller vocabularies (shown by a curve that is above the diagonal), while function words---and to some extent, predicates---are under-represented. 

```{r 4_items}
items <- items %>%
  filter(type == "word") %>%
  mutate(num_item_id = as.numeric(substr(item_id, 6, nchar(item_id))))
```

```{r 4_vocab_comp_fun}
get_vocab_comp <- function(input_language, input_form) {
  print(paste(input_language,input_form))
  
  lang_vocab_items <- filter(items, 
                             language == input_language, 
                             form == input_form) %>%
    filter(lexical_category %in% c("nouns", "predicates", "function_words"))
  
  lang_vocab_data <- get_instrument_data(instrument_language = input_language,
                                         instrument_form = input_form,
                                         items = lang_vocab_items$item_id, 
                                         iteminfo = lang_vocab_items) %>%
    mutate(value = ifelse(is.na(value), "", value),
           produces = value == "produces",
           understands = value == "produces" | value == "understands") %>%
    dplyr::select(-value) %>%
    gather(measure, value, produces, understands)
  
  num_words <- nrow(lang_vocab_items)
  
  lang_vocab_summary <- lang_vocab_data %>%
    group_by(data_id, measure, lexical_category) %>%
    summarise(num_true = sum(value),
              prop = sum(value) / n())
  
  lang_vocab_sizes <- lang_vocab_summary %>%
    summarise(vocab_num = sum(num_true),
              vocab = sum(num_true) / num_words)
  
  lang_vocab_summary %>%
    left_join(lang_vocab_sizes) %>%
    mutate(prop_vocab = num_true / vocab_num) %>%
    dplyr::select(-num_true) %>%
    mutate(language = input_language, form = input_form)
}
```

```{r 4_vocab_comp, eval=FALSE}
instruments <- instruments %>%
  filter(form %in% c("WS","WG")) %>%
  dplyr::select(language, form) %>%
  distinct()

vocab_comp_data <- map2(instruments$language,
                        instruments$form, get_vocab_comp) %>%
  bind_rows()
```

```{r 4_cached_data}
vocab_comp_data <- read_feather("data/vocab_comp_data.feather")
```

We limit our analysis to traditional WS and WG forms for now because short forms like the British English TEDS don't have category information. The sample sizes included in this analysis are given below. 

```{r 4_sample_sizes}
sample_sizes <- vocab_comp_data %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  dplyr::select(language, form, n) %>%
  distinct() 

sample_sizes %>%
  datatable
```

### Estimation Method

How do we estimate over-representation or under-representation of a particular vocabulary item?

```{r 4_area_funs}
get_lang_lexcat_predictions <- function(lang, lexcat) {
  model <- filter(models, language == lang, lexical_category == lexcat)$model[[1]]
  data.frame(vocab = pts,
             prop = predict(model, newdata = data.frame(vocab = pts)),
             lexical_category = lexcat,
             language = lang)
}

get_lang_predictions <- function(lang) {
  bind_rows(sapply(unique(demo_data$lexical_category),
                   function(lexcat) get_lang_lexcat_predictions(lang, lexcat),
                   simplify = FALSE))
}
```



```{r 4_plot_area_demo, fig.width = 9, fig.height = 3.5}
demo_langs <- c("English", "Mandarin")
demo_data <- filter(vocab_comp_data, form == "WS", language %in% demo_langs) %>%
  mutate(panel = paste(language, "(data)"),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns", "Predicates", "Function Words")))
pts <- seq(0, 1, 0.01)

models <- demo_data %>%
  group_by(language, lexical_category) %>%
  do(model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1, data = .))

predictions <- bind_rows(sapply(demo_langs, get_lang_predictions, simplify = FALSE))

diagonal <- expand.grid(vocab = rep(rev(pts)),
                        language = demo_langs,
                        lexical_category = unique(demo_data$lexical_category))
diagonal$prop <- diagonal$vocab

area_poly <- bind_rows(predictions, diagonal) %>%
  mutate(panel = paste(language, "(models)"))

ggplot(filter(predictions, language == "English"), 
       aes(x = vocab, y = prop)) +
  facet_grid(. ~ lexical_category) +
  geom_line(aes(colour = lexical_category), size = 1) +
  geom_polygon(data = filter(area_poly, language == "English"),
               aes(fill = lexical_category), alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  scale_y_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_x_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_colour_solarized(guide = FALSE) +
  scale_fill_solarized(guide = FALSE) 
# +
#   theme(legend.position = c(0.061, 0.91),
#         legend.text = element_text(size = 8),
#         legend.key.height = unit(0.9, "char"),
#         legend.key.width = unit(0.88, "char"),
#         legend.background = element_rect(fill = "transparent"),
#         strip.background = element_blank(),
#         strip.text.x = element_blank())
```


Function for resampling data and computing area estimate for each sample.

```{r 4_resample}
sample_areas <- function(d, nboot = 1000) {
  
  poly_area <- function(group_data) {
    model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1,
                data = group_data)
    return((model$solution %*% c(1/4, 1/3, 1/2) - 0.5)[1])
  }
  
  counter <- 1
  sample_area <- function(d) {
    d_frame <- d %>%
      group_by(language, form, measure) %>%
      sample_frac(replace = TRUE) %>%
      group_by(language, form, measure, lexical_category) %>%
      do(area = poly_area(.)) %>%
      mutate(area = area[1]) %>%
      rename_(.dots = setNames("area", counter))
    
    counter <<- counter + 1 # increment counter outside scope
    return(d_frame)
  }
  
  areas <- replicate(nboot, sample_area(d), simplify = FALSE)
  
  Reduce(left_join, areas) %>%
    gather(sample, area, -language, -form, -measure, -lexical_category)
}
```

Resample data and find the mean and CI of the area estimate.

```{r 4_areas, eval=FALSE}
areas <- sample_areas(vocab_comp_data, 100)
write_feather(areas,"data/vocab_comp_areas.feather")
```


```{r 4_areas2}
areas <- read_feather("data/vocab_comp_areas.feather")
area_summary <- areas %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(mean =  mean(area),
            ci_lower = ci_lower(area),
            ci_upper = ci_upper(area)) %>%
  ungroup() %>%
  mutate(language = factor(language),
         instrument = paste(language, form))

area_order <- filter(area_summary, form == "WS", measure == "produces",
                     lexical_category == "nouns")

language_levels <- area_order$language[order(area_order$mean,
                                             area_order$language,
                                             decreasing = FALSE)]

area_summary_ordered <- area_summary %>%
  filter(form %in% c("WS", "WG"),
         !(form == "WS" & measure == "understands")) %>%
  ungroup() %>%
  mutate(language = factor(language, levels = language_levels),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns", "Predicates", "Function Words")))
```


## Syntactic Vocabulary Composition

Base plot for looking at vocabulary composition.

```{r 4_base_plot}
base_plot <- function(input_form, input_measure) {
  vocab_comp_data %>%
    filter(form == input_form, measure == input_measure) %>%
    mutate(lexical_category = factor(lexical_category,
                                     levels = c("nouns", "predicates", "function_words"),
                                     labels = c("Nouns  ", "Predicates  ", "Function Words"))) %>%
    ggplot(aes(x = vocab, y = prop, colour = lexical_category)) +
    facet_wrap(~language) +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                       name = "Proportion of Category\n") +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                       name = "\nVocabulary Size") +
    scale_colour_solarized(name = "") +
    theme(legend.position = "top",
          legend.key = element_blank(),
          legend.background = element_rect(fill = "transparent"))
}
```

### Production: Words and Sentences

Plot WS productive vocabulary composition as a function of vocabulary size for each language.

```{r 4_plot_points_ws, fig.width=8, fig.height=8}
base_plot("WS", "produces") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3)  +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```

### Production: Words and Gestures

Plot WG productive vocabulary composition as a function of vocabulary size for each language.

These data are *much* sparser and don't really license strong inferences in many cases. (Most children produce a relatively small number of the words on WG forms).

```{r 4_plot_points_wg, fig.width=8, fig.height=8}
base_plot("WG", "produces") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3)  +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```
### Comprehension: Words and Gestures

Plot WG receptive vocabulary composition as a function of vocabulary size for each language.

```{r 4_plot_points_wg_comp, fig.width=8, fig.height=8}
base_plot("WG", "understands") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3)  +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```


### Summary across and measures

Plot each lexical category's area estimate by language, form, and measure.

```{r 4_plot_areas, fig.width=6, fig.height=6}
ggplot(area_summary_ordered,
       aes(y = language, x = mean, colour = lexical_category)) +
  facet_grid(lexical_category ~ form + measure) +
  geom_point() +
  geom_segment(aes(x = ci_lower, xend = ci_upper,
                   y = language, yend = language)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  scale_y_discrete(name = "", limits = levels(area_summary_ordered$language)) +
  xlab("\nRelative representation in early vocabulary")
```

Plot WS production vocab composition proportions. 

```{r 4_prod_vs_comp_vocab_comp}
area_scatter_data <- area_summary %>%
  filter(form=="WS", 
         lexical_category %in% c("nouns","predicates"), 
         measure == "produces") %>%
  gather(variable, value, mean, ci_lower, ci_upper) %>%
  mutate(lex_var = str_c(lexical_category, variable, sep = "_")) %>%
  dplyr::select(-lexical_category, -variable) %>%
  spread(lex_var, value)

ggplot(area_scatter_data, 
       aes(x = nouns_mean, y = predicates_mean)) + 
  geom_pointrange(aes(ymin = predicates_ci_lower, ymax = predicates_ci_upper)) + 
  geom_errorbarh(aes(xmin = nouns_ci_lower, xmax = nouns_ci_upper)) +
  geom_smooth() + 
  geom_label_repel(aes(label=language), force = 4, alpha = .8) +
  xlab("Noun bias") +
  ylab("Predicate bias")
```


## Age effects on vocabulary composition

We predict that the proportion of predicates and function words in children's vocabulary should be relatively more affected by age than nouns. Concrete nouns are hypothesized to be learned initially from both co-occurrences between words [@yu2007b] and by social cues to reference to particular objects [@bloom2002]. On neither account should syntactic information be a primary information source (though of course syntax might be more informative for abstract nouns). In contrast, for other types of words, syntax should be more important for learning their meaning.

On the syntactic bootstrapping hypothesis [@gleitman1990;@fisher2010], verbs especially are learned by mapping the syntactic structure of utterances to the thematic structure of observed events, for example by noticing that the subject of a sentence matches the agent in one particular ongoing event but not another (``the cat is fleeing the dog'' matches *flees*(cat, dog) but not *chases*(dog,cat)). A similar argument can be made for adjectives, since identification of the modified noun is similarly critical for inferring the meaning of the modifier. And by the same logic, function words should be even harder to learn without some understanding of their syntactic role. Thus, if syntactic development is related in some way to age, we should see larger age effects on predicate and function word vocabulary than on noun vocabulary. 

In all four languages, the age coefficient is substantially larger for function words than for nouns. This asymmetry can be interpreted as evidence that, for two vocabulary-matched children, the older child would tend to produce relatively more function words than the younger. 





