The Categorical Composition of Early Vocabulary. This chapter focuses on splitting vocabulary data into syntactic and semantic categories and analyzing consistency and variability across languages in the acquisition of these. 

Syntactic category biases. We quantify the “noun bias” across languages. In addition, we report the degree of bias for or against verbs and closed-class words. Previously presented in Braginsky et al. (2015b).

Conceptual categories. We investigate the consistency of colors, spatial terms, logical terms, and other conceptual categories of interest. Some of these semantic categories have been argued to be learned as a group, so we investigate whether they are more consistently correlated within children than would be expected by chance. 

# The Categorical Composition of Early Vocabulary {#categories}

Early vocabulary development is typically characterized by learning of names for caregivers and common objects, while later in development, children tend to diversify their vocabulary by increasing the proportion of predicates (verbs and adjectives) and closed class words. This over-representation of nouns has been found across a number of analyses and in a variety of languages [@bates1994;@caselli1995;@bornstein2004], though not all [@tardif1996;@choi1995].

Differences in early vocabulary composition have been argued to emerge from typological differences (e.g., word order, subject drop), and from cultural practices (e.g., focus on picture book reading) [@tardif1999;@gopnik1996;@choi1995]---we are agnostic as to the source of this variability.

## Age effects on vocabulary composition

For our purposes we are interested in using these analyses of vocabulary composition to test for the same kind of age-related differences that we found in the complexity and word-form analyses. 


We predict that the proportion of predicates and function words in children's vocabulary should be relatively more affected by age than nouns. Concrete nouns are hypothesized to be learned initially from both co-occurrences between words \cite{yu2007b} and by social cues to reference to particular objects \cite{bloom2002}. On neither account should syntactic information be a primary information source (though of course syntax might be more informative for abstract nouns). In contrast, for other types of words, syntax should be more important for learning their meaning.

On the syntactic bootstrapping hypothesis \cite{gleitman1990,fisher2010}, verbs especially are learned by mapping the syntactic structure of utterances to the thematic structure of observed events, for example by noticing that the subject of a sentence matches the agent in one particular ongoing event but not another (``the cat is fleeing the dog'' matches \textsc{flees(cat, dog)} but not \textsc{chases(dog,cat)}). A similar argument can be made for adjectives, since identification of the modified noun is similarly critical for inferring the meaning of the modifier. And by the same logic, function words should be even harder to learn without some understanding of their syntactic role. Thus, if syntactic development is related in some way to age, we should see larger age effects on predicate and function word vocabulary than on noun vocabulary. 

### Results

Each CDI form contains a mixture of words in different classes. We adopt the categorization of \citeA{bates1994}, splitting words into nouns, predicates (verbs and adjectives), function words, and other words. For each child's vocabulary, we compute the proportion of the total words in each of these categories that they are reported to produce.

For each of the four languages in our sample, we plot these proportions against total vocabulary. These functions are shown in Figure \ref{fig:vocab_comp}: Each dot represents a child's knowledge of a particular class, while curves show the relationship between a class and the whole vocabulary. If categories grow independently of one another, these curves should approximate the diagonal. This pattern is not what we observe, however: Across the languages in our sample, nouns are systematically over-represented in smaller vocabularies (shown by a curve that is above the diagonal), while function words---and to some extent, predicates---are under-represented. 

Next, we measure the contribution of age to vocabulary composition. We fit a logistic model to all children's data for each word class, predicting word-class proportion as a function of total vocabulary and age (as in Analysis 1). Figure \ref{fig:coefs_vocab_comp} shows age coefficients for each of these models across languages. In all four languages, the age coefficient is substantially larger for function words than for nouns. This asymmetry can be interpreted as evidence that, for two vocabulary-matched children, the older child would tend to produce relatively more function words than the younger. In English and Norwegian, the same regularity holds for predicates, while for Spanish and Danish, predicates and nouns are more similar to one another.



We replicated previous analyses \cite{bates1994} showing an over-representation of nouns in the developing lexicon and an under-representation of predicates and function words. We also predicted that---if syntactic generalization was in some way tied to age---predicates and function words would show relatively more age influence than nouns. Although there was some cross-language variation in predicate terms, overall this prediction was confirmed across the languages we examined. Thus, this analysis provides additional evidence for a relationship between syntactic development and age, independent of the growth of the lexicon.


## Semantic Category Representation

## Lexical Category Representation

This work was reported to BUCLD 2015 by Braginsky et al. 

Load in Wordbank data.
```{r database}
items <- items %>%
  filter(type == "word") %>%
  mutate(num_item_id = as.numeric(substr(item_id, 6, nchar(item_id))))
```

Function for getting vocabulary composition data for a given instrument.
```{r vocab_comp_fun, cache=TRUE}
get_vocab_comp <- function(input_language, input_form) {
  print(paste(input_language,input_form))
  
  lang_vocab_items <- filter(items, 
                             language == input_language, 
                             form == input_form) %>%
    filter(lexical_category %in% c("nouns", "predicates", "function_words"))
  
  lang_vocab_data <- get_instrument_data(instrument_language = input_language,
                                         instrument_form = input_form,
                                         items = lang_vocab_items$item_id, 
                                         iteminfo = lang_vocab_items) %>%
    mutate(value = ifelse(is.na(value), "", value),
           produces = value == "produces",
           understands = value == "produces" | value == "understands") %>%
    select(-value) %>%
    gather(measure, value, produces, understands)
  
  num_words <- nrow(lang_vocab_items)
  
  lang_vocab_summary <- lang_vocab_data %>%
    group_by(data_id, measure, lexical_category) %>%
    summarise(num_true = sum(value),
              prop = sum(value) / n())
  
  lang_vocab_sizes <- lang_vocab_summary %>%
    summarise(vocab_num = sum(num_true),
              vocab = sum(num_true) / num_words)
  
  lang_vocab_summary %>%
    left_join(lang_vocab_sizes) %>%
    mutate(prop_vocab = num_true / vocab_num) %>%
    select(-num_true) %>%
    mutate(language = input_language, form = input_form)
}
```

Get vocabulary composition data for all instruments. (Limiting to traditional WS/WG for now because short forms like TEDS don't have category information. 

```{r vocab_comp}
instruments <- instruments %>%
  filter(form %in% c("WS","WG")) %>%
  select(language, form) %>%
  distinct()

vocab_comp_data <- map2(instruments$language,
                        instruments$form, get_vocab_comp) %>%
  bind_rows()
```

Show sample size of each instrument.
```{r sample_sizes}
sample_sizes <- vocab_comp_data %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  select(language, form, n) %>%
  distinct()
```

Base plot for looking at vocabulary composition.
```{r base_plot}
base_plot <- function(input_form, input_measure) {
  vocab_comp_data %>%
    filter(form == input_form, measure == input_measure) %>%
    mutate(lexical_category = factor(lexical_category,
                                     levels = c("nouns", "predicates", "function_words"),
                                     labels = c("Nouns  ", "Predicates  ", "Function Words"))) %>%
    ggplot(aes(x = vocab, y = prop, colour = lexical_category)) +
    facet_wrap(~language) +
    geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                       name = "Proportion of Category\n") +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                       name = "\nVocabulary Size") +
    scale_colour_solarized(name = "") +
    theme(legend.position = "top",
          legend.key = element_blank(),
          legend.background = element_rect(fill = "transparent"))
}
```

Plot WS productive vocabulary composition as a function of vocabulary size for each language.
```{r plot_points_ws, fig.width=10, fig.height=10, cache=FALSE}
base_plot("WS", "produces") + geom_jitter(size = 0.7)
```

Plot WG productive vocabulary composition as a function of vocabulary size for each language.
```{r plot_points_wg_prod, fig.width=10, fig.height=10, cache=FALSE}
base_plot("WG", "produces") + geom_jitter(size = 0.7)
```

Plot WG receptive vocabulary composition as a function of vocabulary size for each language.
```{r plot_points_wg_comp, fig.width=10, fig.height=10, cache=FALSE}
base_plot("WG", "understands") + geom_jitter(size = 0.7)
```

Plot WS productive vocabulary composition as a function of vocabulary size for each language with cubic contrained lm curves.
```{r plot_models_ws, fig.width=10, fig.height=10, cache=FALSE}
base_plot("WS", "produces") +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1)
```

Plot WG productive vocabulary composition as a function of vocabulary size for each language with cubic contrained lm curves.
```{r plot_models_wg_prod, fig.width=10, fig.height=10, cache=FALSE}
base_plot("WG", "produces") +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1)
```

Plot WG receptive vocabulary composition as a function of vocabulary size for each language with cubic contrained lm curves.
```{r plot_models_wg_comp, fig.width=10, fig.height=10, cache=FALSE}
base_plot("WG", "understands") +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1)
```

Function for resampling data and computing area estimate for each sample.
```{r resample}
sample_areas <- function(d, nboot = 1000) {
  
  poly_area <- function(group_data) {
    model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1,
                data = group_data)
    return((model$solution %*% c(1/4, 1/3, 1/2) - 0.5)[1])
  }
  
  counter <- 1
  sample_area <- function(d) {
    d_frame <- d %>%
      group_by(language, form, measure) %>%
      sample_frac(replace = TRUE) %>%
      group_by(language, form, measure, lexical_category) %>%
      do(area = poly_area(.)) %>%
      mutate(area = area[1]) %>%
      rename_(.dots = setNames("area", counter))
    
    counter <<- counter + 1 # increment counter outside scope
    return(d_frame)
  }
  
  areas <- replicate(nboot, sample_area(d), simplify = FALSE)
  
  Reduce(left_join, areas) %>%
    gather(sample, area, -language, -form, -measure, -lexical_category)
}
```

Resample data and find the mean and CI of the area estimate.
```{r areas}
areas <- sample_areas(vocab_comp_data, 1000)

area_summary <- areas %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(mean =  mean(area),
            ci_lower = ci_lower(area),
            ci_upper = ci_upper(area)) %>%
  ungroup() %>%
  mutate(language = factor(language),
         instrument = paste(language, form))

area_order <- filter(area_summary, form == "WS", measure == "produces",
                     lexical_category == "nouns")
language_levels <- area_order$language[order(area_order$mean,
                                             area_order$language,
                                             decreasing = FALSE)]

area_summary_ordered <- area_summary %>%
  filter(form %in% c("WS", "WG"),
         !(form == "WS" & measure == "understands")) %>%
  ungroup() %>%
  mutate(language = factor(language, levels = language_levels),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns", "Predicates", "Function Words")))
```

Plot each lexical category's area estimate by language, form, and measure.
```{r plot_areas, fig.width=12, fig.height=8}
ggplot(area_summary_ordered,
       aes(y = language, x = mean, colour = lexical_category)) +
  facet_grid(lexical_category ~ form + measure) +
  geom_point() +
  geom_segment(aes(x = ci_lower, xend = ci_upper,
                   y = language, yend = language)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  scale_y_discrete(name = "", limits = levels(area_summary_ordered$language)) +
  xlab("\nRelative representation in early vocabulary")
```


Plots for BUCLD poster
----------------------

Data and models.
```{r plot_points_models_ws, fig.width = 15, fig.height = 10}
sample_sizes_ordered <- sample_sizes %>%
  filter(form == "WS") %>%
  mutate(language = factor(language, levels = rev(language_levels)))

language_size_order <- unlist(map(
  levels(sample_sizes_ordered$language),
  function(language) sprintf("%s (N = %s)", language,
                             sample_sizes_ordered$n[which(sample_sizes_ordered$language == language)])
))

vocab_comp_data_ordered <- vocab_comp_data %>%
  filter(form == "WS", measure == "produces") %>%
  mutate(language = factor(language, levels = rev(language_levels)),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns  ", "Predicates  ", "Function Words"))) %>%
  left_join(sample_sizes_ordered) %>%
  mutate(language_with_size = sprintf("%s (N = %s)", language, n),
         language_with_size = factor(language_with_size, levels = language_size_order))

ggplot(vocab_comp_data_ordered, aes(x = vocab, y = prop, colour = lexical_category)) +
  facet_wrap(~language_with_size, ncol = 5) +
  geom_jitter(size = 0.7, alpha = 0.5) +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, size = 1, se = FALSE) +
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                     name = "Proportion of Category\n") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2),
                     name = "\nVocabulary Size") +
  scale_colour_solarized(name = "") +
  theme_mikabr(base_size = 20) +
  theme(legend.position = "top",
        legend.key = element_blank(),
        legend.background = element_rect(fill = "transparent"))
#ggsave("BUCLD/data_models.png", width = 15, height = 10)
```

Demo of area estimation.
```{r plot_area_demo, fig.width = 9, fig.height = 3.5}
demo_langs <- c("English", "Mandarin")
demo_data <- filter(vocab_comp_data, form == "WS", language %in% demo_langs) %>%
  mutate(panel = paste(language, "(data)"),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns", "Predicates", "Function Words")))
pts <- seq(0, 1, 0.01)

models <- demo_data %>%
  group_by(language, lexical_category) %>%
  do(model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1, data = .))

get_lang_lexcat_predictions <- function(lang, lexcat) {
  model <- filter(models, language == lang, lexical_category == lexcat)$model[[1]]
  data.frame(vocab = pts,
             prop = predict(model, newdata = data.frame(vocab = pts)),
             lexical_category = lexcat,
             language = lang)
}

get_lang_predictions <- function(lang) {
  bind_rows(sapply(unique(demo_data$lexical_category),
                   function(lexcat) get_lang_lexcat_predictions(lang, lexcat),
                   simplify = FALSE))
}

predictions <- bind_rows(sapply(demo_langs, get_lang_predictions, simplify = FALSE))

diagonal <- expand.grid(vocab = rep(rev(pts)),
                        language = demo_langs,
                        lexical_category = unique(demo_data$lexical_category))
diagonal$prop <- diagonal$vocab

area_poly <- bind_rows(predictions, diagonal) %>%
  mutate(panel = paste(language, "(models)"))

ggplot(filter(predictions, language == "English"), aes(x = vocab, y = prop)) +
  facet_grid(. ~ lexical_category) +
  geom_line(aes(colour = lexical_category), size = 1) +
  geom_polygon(data = filter(area_poly, language == "English"),
               aes(fill = lexical_category), alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  scale_y_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_x_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_colour_solarized(guide = FALSE) +
  scale_fill_solarized(guide = FALSE) +
  theme(legend.position = c(0.061, 0.91),
        legend.text = element_text(size = 8),
        legend.key.height = unit(0.9, "char"),
        legend.key.width = unit(0.88, "char"),
        legend.background = element_rect(fill = "transparent"),
        strip.background = element_blank(),
        strip.text.x = element_blank())
#ggsave("BUCLD/area_demo.png", width = 9, height = 3.5)
```

WS area estimates.
```{r plot_areas_ws, fig.width = 8, fig.height = 11}
ggplot(filter(area_summary_ordered, form == "WS"),
       aes(y = language, x = mean, col = lexical_category)) +
  facet_grid(lexical_category ~ .) +
  geom_point() +
  geom_segment(aes(x = ci_lower, xend = ci_upper,
                   y = language, yend = language)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  scale_y_discrete(name = "", limits = levels(area_summary_ordered$language)) +
  xlab("\nRelative representation in early vocabulary") +
  theme_mikabr(base_size = 20)
#ggsave("BUCLD/diffs.png", width = 8, height = 11)
```


## Age effects

Get vocabulary composition data for all languages.
```{r vocab_summary}
get.vocab.composition <- function(lang) {
  
  lang.vocab.items <- filter(items, language == lang, form == "WS", type == "word") %>%
    filter(lexical_category != "other") %>%
    rename(column = item.id) %>%
    mutate(item.id = as.numeric(substr(column, 6, nchar(column))))
  
  lang.instrument.table <- filter(instrument.tables, language == lang,
                                  form == "WS")$table[[1]]
  
  lang.vocab.data <- get.instrument.data(lang.instrument.table,
                                         lang.vocab.items$column) %>%
    left_join(select(lang.vocab.items, item.id, lexical_category, item, definition)) %>%
    mutate(value = ifelse(is.na(value), "", value),
           value = get.value("word", value))
  
  num.words <- nrow(lang.vocab.items)
  
  lang.admins <- admins %>%
    filter(language == lang) %>%
    select(data_id, age, age.group, language)
  
  lang.vocab.summary <- left_join(lang.vocab.data, lang.admins) %>%
    filter(age > 15 & age < 32) %>%
    group_by(data_id, age, age.group, language, lexical_category) %>%
    summarise(sum = sum(value),
              diff = length(value) - sum,
              mean = sum / length(value))
  
  lang.vocab.sizes <- lang.vocab.summary %>%
    summarise(vocab.mean = sum(sum) / num.words)
  
  return(left_join(lang.vocab.summary, lang.vocab.sizes))
    
  }

vocab.composition <- bind_rows(sapply(languages, get.vocab.composition,
                                      simplify = FALSE)) %>%
  filter(lexical_category != "unknown",
         lexical_category != "other") %>%
  mutate(lexical_category = factor(lexical_category,
                                   levels=c("nouns", "predicates", "function_words"),
                                   labels=c("Nouns", "Predicates", "Function Words")))
```

Fit vocabulary composition models and use them to predict data.
```{r vocab_predict}
vocab.models <- vocab.composition %>%
  group_by(language, lexical_category) %>%
  do(model = glm(cbind(sum, diff) ~ vocab.mean,
                 data = ., family = "binomial"))

get.vocab.model <- function(lang, cat) {
  return(filter(vocab.models, language == lang, lexical_category == cat)$model[[1]])
  }

vocab.predicted.data <- vocab.composition %>%
  group_by(language, lexical_category) %>%
  mutate(predicted = inv.logit(predict.lm(get.vocab.model(unique(language),
                                                         unique(lexical_category)),
                                         data = ., family = "binomial")))
```

Plot vocabulary composition as a function of vocabulary size for each language with model prediction curves.
```{r vocab_data_plot, fig.width=10, fig.height=2.9}
ggplot(vocab.predicted.data,
       aes(x=vocab.mean, y=mean, colour=lexical_category, label=lexical_category)) +
  geom_jitter(alpha=0.15, size=.75) +
  geom_line(aes(y=predicted),size=0.65) + 
  facet_wrap(~ language, nrow = 1, ncol = 4) +
  scale_y_continuous(limits = c(0, 1.05), breaks = seq(0,1,.2),
                     name = "Proportion of Category\n") +
  scale_x_continuous(limits = c(0,1), breaks = seq(0,1,.2),
                     name = "\nVocabulary Size") +
  theme_bw(base_size=12) + 
  theme(legend.position = c(0.065,0.83),
        legend.text = element_text(size=9),
        legend.title = element_text(size=9, lineheight=unit(0.7, "char")),
        legend.key.height = unit(0.8, "char"),
        legend.key.width = unit(0.3, "cm"),
        legend.key = element_blank(),
        legend.background = element_rect(fill="transparent"),
        text = element_text(family=font)) +
  scale_color_brewer(palette = "Set2", name = "Lexical Category")
```

Fit vocabulary composition models and get their AICs and age coefficients.
```{r vocab_models}
vocab.model.metrics <- vocab.composition %>%
  group_by(language, lexical_category) %>%
  do(model.vocab = glm(cbind(sum, diff) ~ vocab.mean, data = .,
                       family="binomial"),
     model.vocab.age = glm(cbind(sum, diff) ~ vocab.mean + age, data = .,
                           family="binomial")) %>%
  mutate(AIC.vocab = AIC(model.vocab),
         AIC.vocab.age = AIC(model.vocab.age),
         deltaAIC = AIC.vocab - AIC.vocab.age,
         age.coef = coef(model.vocab.age)["age"],
         age.se = se.coef(model.vocab.age)["age"])
```

Show AICs of vocabulary composition models.
```{r vocab_aic}
kable(select(vocab.model.metrics,
             language, lexical_category, AIC.vocab, AIC.vocab.age, deltaAIC))
```

Plot age effect coefficients for each language and lexical category.
```{r vocab_coef_plot, fig.width=6, fig.height=4}
ggplot(vocab.model.metrics, 
       aes(x=language, y=age.coef, fill=lexical_category)) + 
  geom_bar(position="dodge", stat="identity") + 
  geom_linerange(aes(ymin=age.coef-1.96*age.se, ymax=age.coef+1.96*age.se), 
                 position = position_dodge(width=.9)) +
  ylab("Age effect coefficient") + 
  xlab("") +
  theme_bw(base_size = 14) +
  scale_fill_brewer(palette = "Set2",
                    name = "Lexical Category") +
  theme(legend.position = c(0.17,0.85),
        legend.text = element_text(size=13),
        legend.title = element_text(size=13),
        legend.key.height = unit(1.2, "char"),
        legend.key = element_blank(),
        legend.background = element_rect(fill="transparent"),
        text = element_text(family=font))
```

