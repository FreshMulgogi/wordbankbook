# Estimating Age of Acquisition {#appendix-aoa}

In this Appendix, we systematically compare methods for estimating age of acquisition, using the English Words and Sentences data as a case study.

```{r appaoa-load_data}
# eng_ws <- read_feather("data/psychometrics/eng_ws_raw_data.feather")
load("data/psychometrics/eng_ws_raw_data.Rds")
```

All the AOA curves.

```{r appaoa-means}
ms <- eng_ws %>%
  group_by(definition, age, category) %>%
  summarise(prop = mean(value=="produces", na.rm=TRUE), 
            num_true = sum(value=="produces", na.rm=TRUE), 
            num_false = sum(value!="produces", na.rm=TRUE), 
            n = sum(c(num_true,num_false))) %>%
  filter(!is.na(category))

ggplot(ms, aes(age, prop, col = definition)) + 
  geom_line() + 
  facet_wrap(~category) +
  scale_colour_solarized(guide=FALSE)
```

Let's compare methods. 

## Empirical quantiles

First try empirical quantiles. This loses many observations that never go above 50%. 

```{r appaoa-empirical}
empirical_aoas <- ms %>%
  group_by(definition, category) %>%
  summarise(empirical_aoa = min(age[prop > .5]))

qplot(empirical_aoa, data = empirical_aoas)
qplot(empirical_aoa, facets = ~ category, data = empirical_aoas)
```

## Basic GLM

Now let's try the basic GLM version. 

```{r appaoa-glm}
fit_glm <- function(data) {
  model <- glm(cbind(num_true, num_false) ~ age, family = "binomial", 
               data = data)
  fit <- predict(model, newdata = data.frame(age = 0:36), se.fit = TRUE)
  aoa <- -model$coefficients[["(Intercept)"]] / model$coefficients[["age"]]
  
  data.frame(definition = data$definition[1],
             category = data$category[1],
             glm_aoa = aoa)
}

glm_aoas <- ms %>%
  split(.$definition) %>%
  map(fit_glm) %>%
  bind_rows

qplot(glm_aoa, data = glm_aoas)
qplot(glm_aoa, facets = ~ category, data = glm_aoas)
```

Compare. The GLM AOA gives some very early values, but it is OK with a lot of data (as in the case of English).

```{r appaoa-glm_compare}
aoas <- full_join(empirical_aoas, glm_aoas)

ggplot(aoas, aes(x = empirical_aoa, y = glm_aoa, col = category)) + 
  geom_point() + 
  geom_text(aes(label = definition), nudge_y = 3) +
  scale_colour_solarized(guide=FALSE) 

aoas %>%
  gather(measure, aoa, empirical_aoa, glm_aoa) %>%
  ggplot(aes(x = aoa)) + facet_grid(.~measure) + geom_histogram()
```

## Robust GLM


```{r appaoa-rglm}
fit_rglm <- function(data) {
  model <- robustbase::glmrob(cbind(num_true, num_false) ~ age, family = "binomial", 
               data = data)
  fit <- predict(model, newdata = data.frame(age = 0:36), se.fit = TRUE)
  aoa <- -model$coefficients[["(Intercept)"]] / model$coefficients[["age"]]
  
  data.frame(definition = data$definition[1],
             category = data$category[1],
             rglm_aoa = aoa)
}

rglm_aoas <- ms %>%
  split(.$definition) %>%
  map(fit_rglm) %>%
  bind_rows

qplot(rglm_aoa, data = rglm_aoas)
qplot(rglm_aoa, data = rglm_aoas) + 
  xlim(c(0,50))
qplot(rglm_aoa, facets = ~ category, data = rglm_aoas)
```

Looks totally reasonable except for the two crazy ones. 

```{r appaoa-rglm_outliers}
rglm_aoas$definition[rglm_aoas$rglm_aoa < 0] 
```


## Bayes GLM

Can we do a `arm::bayesglm` to regularize things a bit? Let's explore a single curve. Basically, `arm::bayesglm` does *exactly* the same thing as GLM because of the huge amount of data, no matter what prior. Let's try also robust GLM.   

```{r appaoa-bayes_sample}
target_words <- 
  sample(unique(ms$definition), 20)
# c((c("pattycake","mommy*","bye", "tomorrow","play pen", "today",
#                   "child", "all","night night","brother", "daddy", "bye", "when"),) 
                  

do.fits <- function(data) {
  glm_model <- glm(cbind(num_true, num_false) ~ age, 
                   family = "binomial", 
                   data = data)
  bayes_model <- arm::bayesglm(cbind(num_true, num_false) ~ age, 
                          family = binomial(link="logit"),
                          prior.mean = .25, 
                          prior.scale = c(.01), 
                          prior.mean.for.intercept = 0, 
                          prior.scale.for.intercept = 2.5,
                          prior.df=Inf, 
                          data = data)
  rob_model <- robustbase::glmrob(cbind(num_true, num_false) ~ age, 
                      family = binomial(link="logit"), data = data)
  
  
  fits <- data.frame(age = 0:36) %>%
    mutate(glm = predict(glm_model, 
                         type = "response", 
                         data.frame(age = age)), 
           bayes = predict(bayes_model, 
                           type = "response", 
                           data.frame(age = age)), 
           robust = predict(rob_model, 
                            type = "response", 
                            data.frame(age = age)), 
           definition = data$definition[1]) %>%
    left_join(data) 
  
  return(fits)
}

mms <- ms %>%
  filter(definition %in% target_words) %>%
  group_by(definition) %>%
  do(do.fits(.)) 

ggplot(mms, aes(x = age)) + 
  geom_line(aes(y = glm), col = "red") + 
  geom_line(aes(y = bayes), col = "blue") +
  geom_line(aes(y = robust), col = "green") + 
  scale_colour_manual(values = c("glm" = "red", 
                                 "bayes" = "blue", 
                                 "robustglm" = "green"), 
                      name = "model") +
  geom_point(aes(y = prop), col = "black") + 
  ylim(c(0,1)) + 
  facet_wrap(~definition)
```

Now apply this more broadly. Note that the 50% point for logistic regression = $- \beta_1 / \beta_2$.

```{r appaoa-bayes_all}
fit_bglm <- function(data) {
  model <- arm::bayesglm(cbind(num_true, num_false) ~ age, family = "binomial", 
                    prior.mean = .25, 
                          prior.scale = c(.01), 
                          prior.mean.for.intercept = 0, 
                          prior.scale.for.intercept = 2.5,
                          prior.df=Inf,
                    data = data)
  aoa <- -model$coefficients[["(Intercept)"]] / model$coefficients[["age"]]
  
  data.frame(definition = data$definition[1],
             category = data$category[1],
             bglm_aoa = aoa)
}

bglm_aoas <- ms %>%
  split(.$definition) %>%
  map(fit_bglm) %>%
  bind_rows
```

`arm::bayesglm` is working very well here, basically moving Mommy and Daddy a bit more conservative but otherwise preserving most of the curve. 

```{r appaoa-bayes_plot}
aoas <- left_join(bglm_aoas, glm_aoas)
ggplot(aoas, aes(x = bglm_aoa, y = glm_aoa)) + 
  geom_point() + 
  ggrepel::geom_label_repel(data = filter(aoas, abs(bglm_aoa - glm_aoa) > 1.5), 
                   aes(label = definition)) +
  xlab("Bayes GLM") + 
  ylab("Standard GLM") + 
  ylim(-3,36) + 
  xlim(-3,36) + 
  geom_abline(lty = 2, col = "red")
```





`arm::bayesglm` with these made-up parameters fixes some of the issues, produces a better-looking distribution with fewer crazy outliers. 

## Hierarchical GLM model

Set some stan settings. 

```{r appaoa-stan_setup}
rstan::rstan_options(auto_write = TRUE,mc.cores = parallel::detectCores())
# ms <- read_csv("engdata.csv")
```

The model. Constraints:

* No negative slopes - we don't get worse at words. 
* Strong prior on slopes, they should have mean and sd around .25 (empirical)
* Much weaker prior on intercepts, though yoked their SDs (perhaps too tightly?


```{r appaoa-stan_model}
hierarchical_logit <-'
data {                            
  int<lower=1> W; // number of words
  int<lower=1> A; // number of ages
  vector[A] age; // subject ages
  int<lower=0> produces[W,A]; // count data
  int<lower=0> attempts[W,A]; // count data
}

parameters {
  real mu_i;             // intercept mean
  real<lower=0> sigma_i; // intercept SD
  
  real<lower=0> mu_s;             // slope mean
  real<lower=0> sigma_s; // slope SD

  vector[W] intercept; // word means
  vector[W] slope; // subject means
}

transformed parameters {
  matrix[W,A] p;

  for (w in 1:W) 
    for (a in 1:A)
      p[w,a] = intercept[w] + (slope[w] * age[a]);
}

model {
  mu_i ~ normal(0, 10); 
  sigma_i ~ normal(0, 1);

  mu_s ~ normal(0, .25);
  sigma_s ~ normal(0, .25);

  intercept ~ normal(mu_i, sigma_i);
  slope ~ normal(mu_s, sigma_s);
  
  for (w in 1:W) 
    for (a in 1:A)
      produces[w,a] ~ binomial_logit(attempts[w,a], p[w,a]);
}
'
```

Now reformat the data to stan format and compute. 

```{r appaoa-stan_prep}
model.data <- ms %>%
  ungroup %>%
  mutate(word = definition, 
         n = num_true,
         N = num_true + num_false) %>%
  # filter(word %in% c("daddy*","mommy*","no","bye")) %>%
  select(word, age, category, n, N)

ages <- unique(model.data$age)
n.words <- length(unique(model.data$word))

dat <- list(age = ages,
            produces = matrix(model.data$n, 
                              nrow=n.words, ncol=length(ages), byrow = TRUE),
            attempts = matrix(model.data$N, 
                              nrow=n.words, ncol=length(ages), byrow = TRUE),
            W = n.words, 
            A = length(ages))
```

```{r appaoa-stan_fit, eval=FALSE}
samps <- rstan::stan(model_code = hierarchical_logit, 
              cores = 4, 
              data = dat, iter = 200, warmup=100, chains = 4, 
              pars = c("mu_i", "sigma_i", "mu_s", "sigma_s", "slope", "intercept"))
save(samps, "data/appendix_aoa_estimation/aoa-samps.Rds")
```

Diagnostics.

```{r appaoa-stan-diagnostics, eval=FALSE}
load("data/appendix_aoa_estimation/aoa-samps.Rds")
rstan::traceplot(samps, pars = c("mu_i","sigma_i","mu_s","sigma_s"), 
                 inc_warmup = TRUE)
```

Explore parameters. 

```{r appaoa-stan_params, eval=FALSE}
coefs <- data.frame(summary(samps)$summary)
coefs$name <- rownames(coefs)

word_ids <- model.data %>% 
  group_by(word) %>% 
  summarise(category = category[1]) %>%
  mutate(word_id = 1:n())

words <- coefs %>% 
  filter(str_detect(name, "slope") | str_detect(name, "intercept")) %>%
  separate(name, c("variable", "word_id"), "\\[") %>%
  mutate(word_id = as.numeric(str_replace(word_id, "]", ""))) %>%
  left_join(word_ids) %>%
  select(mean, variable, word, category) %>%
  spread(variable, mean) %>%
  mutate(aoa = intercept / slope) %>%
  arrange(aoa) %>%
  mutate(word = factor(word, 
                        levels = word,
                        labels = word))
```

and plot again:

```{r appaoa-stan_plot, eval=FALSE}
age_range <- 0:36
preds <- words %>%
  group_by(word, category) %>%
  do(data.frame(age = age_range, 
                pred.prop = boot::inv.logit(.$intercept + age_range * .$slope))) %>%
  left_join(model.data) %>%
  mutate(prop = n/N) 

ggplot(filter(preds, word %in% target_words), aes(x = age, y = prop)) + 
  geom_point() + 
  facet_wrap(~word) + 
  geom_line(aes(x = age, y = pred.prop))
```

Check the histogram.

```{r appaoa-stan_hist, eval=FALSE}
hglm.aoas <- words %>%
  mutate(aoa = -intercept / slope)

qplot(aoa, geom = "blank", data = hglm.aoas) + 
  geom_histogram(aes(y = ..density..)) + 
  geom_density(col = "red")
```

What are the crazy ones? 

```{r appaoa-stan_outliers, eval=FALSE}
hglm.aoas$word[hglm.aoas$aoa <10]
```

Don't know what's going on with these but everything else looks good. 

## Full comparison between models

Merge with previous data frame. 

```{r appaoa-compare, eval=FALSE}
all.preds <- left_join(mms, 
                       preds %>% 
                         ungroup %>%
                         rename(hglm = pred.prop, 
                                definition = word) %>% 
                         select(definition, age, hglm)) %>%
  select(-num_true, -num_false)

ggplot(all.preds, aes(x = age)) + 
  geom_line(aes(y = glm), col = "red") + 
  geom_line(aes(y = bayes), col = "blue") +
  geom_line(aes(y = robust), col = "green") + 
  geom_line(aes(y = hglm), col = "orange") + 
  geom_point(aes(y = prop), col = "black") + 
  ylim(c(0,1)) + 
  facet_wrap(~definition) + 
  scale_colour_manual(values = c(glm = "red", 
                                 bayes = "blue", 
                                 robustglm = "green", 
                                 hglm = "orange"), 
                      name = "model")

```


AOA distribution

```{r appaoa-compare_dist, eval=FALSE}

aoas <- full_join(full_join(glm_aoas, full_join(bglm_aoas, rglm_aoas)), empirical_aoas) %>%
  full_join(hglm.aoas %>% rename(definition = word, 
                               hglm_aoa = aoa) %>% 
            select(-intercept, -slope))
  
```

Plot.

```{r appaoa-compare_plot, eval=FALSE}
qplot(aoa,facets = ~ measure, 
      data = aoas %>% gather(measure, aoa, ends_with("aoa")))
```


And pairs plots. 

```{r appaoa-compare_pairs, eval=FALSE}
pair_data <- aoas %>% 
  select(ends_with("aoa")) %>%
          mutate(empirical_aoa = ifelse(is.infinite(empirical_aoa), 
                                       NA, empirical_aoa)) %>%
  filter(glm_aoa > 0, glm_aoa < 50, rglm_aoa > 0, rglm_aoa < 50)

GGally::ggpairs(pair_data) 
```

## Sparsity simulations

BGLM with fairly restricted priors appeared to perform well, giving us reasonably constrained estimates for early words and otherwise being very correlated with glm and the more computation-heavy hglm. 

One consideration remains: does it perform well with very sparse data? 

Examples of this process with 100 children actually look much stronger overall. 

```{r appaoa-sparcity}
ids <- eng_ws %>%
  group_by(data_id) %>%
  count %>%
  ungroup %>%
  sample_n(size = 100, replace = TRUE) %>%
  pull(data_id)

ms_sparse <- eng_ws %>%
  filter(data_id %in% ids) %>%
  group_by(definition, age, category) %>%
  summarise(prop = mean(value=="produces", na.rm=TRUE), 
            num_true = sum(value=="produces", na.rm=TRUE), 
          num_false = sum(value!="produces", na.rm=TRUE), 
            n = sum(c(num_true,num_false))) %>%
  filter(!is.na(category))

bglm_aoas_sparse <- ms_sparse %>%
  split(.$definition) %>%
  map(fit_bglm) %>%
  bind_rows

glm_aoas_sparse <- ms_sparse %>%
  split(.$definition) %>%
  map(fit_glm) %>%
  bind_rows

aoas <- left_join(bglm_aoas_sparse, glm_aoas_sparse)
ggplot(aoas, aes(x = bglm_aoa, y = glm_aoa)) + 
  geom_point() + 
  ggrepel::geom_label_repel(data = filter(aoas, abs(bglm_aoa - glm_aoa) > 1.5), 
                   aes(label = definition)) +
  xlab("Bayes GLM") + 
  ylab("Standard GLM") + 
  geom_abline(lty = 2, col = "red")
```
## Conclusion

`arm::bayesglm` with hand-tuned priors seems to perform pretty well. 

To Do:
* Test with WG comprehension
* Test with another language
