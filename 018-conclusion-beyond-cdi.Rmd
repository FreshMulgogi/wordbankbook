# Beyond the CDI {#conclusion-beyond-cdi}

the CDI has limitations that we've discussed - and it's also overkill for some research
we think that "new journaling" - e.g. wordful, speechome, seedlings - has a lot of potential
there's a missing role for in-the-moment interaction (clark), online interpretation (fernald & marchman), and for more longitudinal tracking
but the promise of "bigger data" psychology is bigger than the CDI - and the approach to consistency and variability is more general as well


## Methodological morals

As we noted in Chapter \@ref(intro-theory), psychology has recently been plagued by concerns about reproducibility [e.g., @hardwicke2018] and replicability [e.g., @osc2015]. Our work here was inspired by considering these issues and their impact on the field of language development. The ultimate goal of research in this area is to create a quantitative theory that allows for precise predictions and principled explanations of developmental phenomena. Such a theory cannot be built on a series of non-reproducible findings and binary conclusions [@frank2017].

Wordbank is a reply to this situation: by compiling the extant CDI datasets into a single open database, researchers can reproduce previous and new research conclusions that use these data. The analyses we report here using the Wordbank data are computationally reproducible through the availability of the code necessary to build the book and all its figures and analyses. In addition, by seeking a level of scale beyond previous efforts, we have attempted to avoid the variability inherent in "small-N" studies. 

Further, our work is built on the notion of replication. Nearly every one of the preceding substantive chapters is in some sense a "replication" of previous work --- an analysis was taken from previous work with a particular CDI dataset and applied (sometimes with technical modifications) to other data. Yet the result is not a judgement on the original; we do not declare a binary success or failure of the replication attempt. Instead, we are interested in the *degree* to which a particular quantitative estimate varies across languages and cultures. 

This sort of analysis is superficially similar to the idea of "hidden moderators" that has plagued the replication debate [@van-bavel2016]. But, this effort has largely been an effort to contextualize failures to replicate particular experimental effects by invoking unknown sources of variability across contexts. In contrast, our efforts here allow us to quantify variation across "replications" of the same effect and use these estimates as the signal --- rather than an un-measured source of noise.

One notable feature of our analytic strategy is that we try to rely very little on binary decision-theoretic inferences using null hypothesis significance testing. There are a handful _p_-values in occasional analyses, but few of these appear in any prominent inferential conclusion. Instead, our goal has been to measure quantities of interest with high precision, looking for statistical estimates that relate to particular theoretical goals. For example, the existence of a noun bias is a fascinating observation, but this observation gives limited leverage to differentiate theories. The magnitude of a noun bias provides more leverage for quantitative theorizing. And the distribution of magnitudes across many of the world's languages gives greater leverage still. Our hope is that, by generalizing and applying many influential analyses by many contributors, our work here can affect theory more broadly.
