# Measurement Properties of the CDI {#psychometrics}

Many researchers are initially shocked to hear that one of the most important methods for studying child language is parent report. Yet, as we argued in Chapter \@ref(intro_practical), alternative methods like naturalistic observation or lab experiments can be biased, and are quite costly to revisit at scale. Thus, the goal of this chapter is to revisit the strengths and weaknesses of parent report in depth, since the remainder of our manuscript depends on the use of CDI data. 


## Limitations of parent report

Although the standardization of parent reports using the CDI contributes to the availability of large amounts of data in a comparable format, there are significant limitations to the parent report methodology that are important to understand [@tomasello1994; @feldman2000]. To do so, it is useful to reflect on what it means when a parent reports that their child "understands" or "understands and says" a word. In an ideal world, the parent's responses would be an unbiased reflection of their observations of their child's language development. For example, when asked if their child produces the word *dog*, a parent is likely recalling situations in which their child has used the word *dog* correctly, and then reporting on the success or failure of this process of recollection. Of course, this judgment clearly depends on the parent's ability to accurately judge that the child intended to say the word *dog*, that the child's target word form was *dog*, and that the child has some meaning for the word form *dog* that at least approximates the expected meaning. There are also a number of other sources of information that the parent might bring to bear on these judgments. 

<!-- The figure below uses graphical model notation to depict the multiple sources of information that could be reflected in parent report [@jordan2004]. -->

```{r psycho-parent_model, fig.cap="The statistical structure of parent report.", eval=FALSE}
knitr::include_graphics("tex/parent_model.png", auto_pdf = FALSE)
```

```{r psycho-parent_sketch, fig.cap="The intuitive structure of parent report."}
knitr::include_graphics("images/parent_report_sketch.png")
```

<!-- The nodes in this model reflect variables of interest -- shaded gray if they are observed, and hollow if they are latent variables that the parent does not have direct access. The edges in this graph represent statistical dependencies among these variables. For each word on the CDI, the parent is asked to report $O_{c}$: have they observed their child producing the word? This observed variable depends on a latent variable $K_{c}$: whether their child knows the meaning of this word. This knowledge in turn depends in three other variables: $O_{p}$, the parent's productions of this word in the past; $d$, the difficulty of learning this word; and $\mu_{c}$, the child's general linguistic ability. This general ability in turn depends on the parent's linguistic ability $\mu_{p}$ and age-related factors $\alpha_{c}$. In addition, the CDI does not ask about every word in the child's vocabulary, only items on the form $i$. Because all of these other variables are statistically related to the child's observed productions, parents could be be bringing any or all of them to bear in their responses. -->

The figure shows a sketch of the process of parent report. For each word on the CDI, the parent is asked to report whether their child has produced or comprehended the word. This report could depend on direct recall of a particular case when their child actually produced or showed comprehension. But in addition to these factors, parents probably draw on their general assessment of the difficulty of the word and on their overall assessment of the child's linguistic abilities. As even this simple sketch shows, parent report judgments are based on a fairly complex set of factors. And hence there are legitimate concerns about the ability of parents to provide detailed and specific knowledge about their children's language. We discuss specific concerns below. 

First, parents may be biased observers generally. Most parents do not have specialized training in language development, and may not be sensitive to subtle aspects of language structure and use. Further, a natural pride in the child and a failure to critically test their impressions may cause parents to overestimate the child's ability ($\mu_{c}$); conversely, frustration in the case of delayed language may lead to underestimates. Parent report is most likely to be accurate under three general conditions: (1) when assessment is limited to current behaviors, (2) when assessment is focused on emergent behaviors, and (3) when a primarily recognition format is used. Each of these conditions acts to reduce demands on the respondent's memory. For example, parents are better able to choose from a list of items that are likely candidates, rather than requiring that the parents generate the list themselves. In addition, parents are likely to be better able to report on their child's language at the present time than at times past and when their child is actively learning the particular words on the list (e.g., names for animals). 

Second, parent reports likely suffer from a number of biases that interact with sub-portions of the forms and the ages of the target children. For example, it is likely that parents may have more difficulty reporting on children's comprehension or production of function words (e.g., *so*, *then*, *if*) than content words (e.g., *baby*, *house*)--relying more on their estimates of the words general difficulty ($d$). Moreover, in typically-developing samples, parents can track their child's receptive vocabulary to about 16-18 months, after which it is too large to monitor. Expressive vocabulary can be monitored until about 2.5 - 3 years, after which the number of words a child can say becomes too large. 

The CDI instruments capitalize on the greater ease of recognition, as contrasted with recall, to help offset these memory limitations. That is, it is better to ask parents to report on their child's vocabulary by selecting words from a list of possible words rather than having them write down all the words they can recall hearing their child use (or, even worse, asking the global question: “Does your child know at least 50 words?"). In addition, asking parents to reflect on their child's language abilities may be particularly difficult for early comprehension. As @tomasello1994 point out, for the youngest children, especially 8 - 10 month olds, vocabulary comprehension scores can be surprisingly high and likely implausible, possibly reflecting a lack of clarity in what the term “understands” means for parents of children at this young age.

Third, there is some evidence that variability in reporting biases may be moderated by factors such as SES [@feldman2000; @fenson2000; @feldman2005]. Some studies suggest that parents from some SES groups may be more likely to underestimate child's abilities [@roberts1999], while others report that parents from lower-SES groups may over-estimate children's abilities, especially comprehension at younger ages [@goldfield1990; @feldman2000). Later studies have shown that, for children over 2 years, patterns of validity were consistent in lower and higher-SES groups [@feldman2005; @reese2000]. Thus, SES-differences could reflect valid delays in children’s language development that parallel those obtained with different methods, such as naturalistic observation or standardized tests (e.g., @hammer2010). A unique and important extension of this work for Wordbank is to examine the consistency of SES differences across a large number of languages and hence, language communities which may vary in the social impact of SES gradients.

Fourth, as discussed above, the items on the original CDI instruments were chosen to be a representative sample of vocabulary for the appropriate age and language [@fenson1994]. The checklists contain some words that most, including the youngest, children are able to understand or produce, some words that are understood or produced by the "average" child, and some which only children who are relatively more advanced will understand or produce. This structure ensures that the list has the psychometric property of capturing individual differences in vocabulary both across younger and older children and across children of different developmental levels. Validity of the CDIs has been demonstrated in reference to both standardized tests and naturalistic language sampling (see Chapter 4 of @fenson2007). Note that the checklists were not originally constructed with the intention that responses on individual items would be reliable. While item-level responses provide useful information about patterns of words that children are likely to understand or produce, responses on the vocabulary checklist do not necessarily license the conclusion that a child would respond appropriately when asked "can you say ____?" by an experimenter in a confrontation naming task. Nonetheless, if parents' observations at the item level reflect any signal--even in the context of significant influence from other factors--then this signal should be observable by aggregating together data from many children.

Fifth, while the lengths of the vocabulary checklists on the CDIs may give the impression that they yield an estimate of the child's full vocabulary, in fact the vocabulary size estimates only reflect a child's relative standing compared to other children assessed with the same list of words[$i$, see @mayor2011 for discussion]. Such estimates should not be misconstrued as a comprehensive estimate of the child's vocabulary knowledge, as CDI scores likely understate the size of a child's "true" vocabulary substantially, especially for older children [@mayor2011]. Moreover, while it is tempting to ask parents to indicate additional words that their child might be able to understand or say, users should be aware that including those items introduces bias in the estimates.

Sixth, when a parent reports on a word on the vocabulary checklist, there is no information about the actual form of the word used, and hence, these vocabulary estimates can say little about phonological development (e.g. segmental v. suprasegmental approaches to the analysis of speech). Parents are instructed that they should check that a child can produce a word even if it is pronounced in the child's "special way," and only approximates the adult form. 

Finally, we also gain little information about the frequency with which children use a particular word in their spontaneous speech, nor can we know the range of contexts in which individual lexical items are used (e.g., is that word used productively vs. in a memorized chunk of speech). Thus, the vocabulary size that is captured by the CDIs reflects the number of different word types (not tokens) that the child is able to understand or produce, with little information about nuances in meaning that might be reflected in actual usage. 

In sum, despite these limitations, when used appropriately, the CDI instruments yield reliable and valid estimates of total vocabulary size. Because the instruments were designed to minimize bias by targeting current behaviors and asking parents about highly salient features of their child's abilities, they have proven to be an important tool in the field. Dozens of studies demonstrate concurrent and predictive relations with naturalistic and observational measures, in both typically-developing and at-risk populations [e.g., @dale1996; @thal2000; @marchman2002]. In addition, a variety of recent work has shown that individual item-level responses can yield exciting new insights, for example about the growth patterns of semantic networks when aggregated across children [@hills2009; @hills2010]. Such analyses have the potential to be even more powerful when applied to larger samples and across languages as they are within Wordbank.



In this Chapter, we examine the psychometric properties of the CDI through the lens of Item Response Theory (IRT). In brief, IRT provides a set of models for estimating the measurement properties of tests consisting of multiple items. These models assume that individuals vary on some latent trait, and that each item in a test measures this latent trait [see @baker2001 for detailed introduction]. 


## IRT Modeling

IRT models vary in their parameterization. In the simplest (Rasch) IRT model, each item has a difficulty parameter that controls how likely a test-taker with a particular ability will be to get a correct answer. In the more sophisticated two-parameter model, each item also has a discrimination parameter that controls how much response probabilities vary with varying abilities. Good items will tend to have high discrimination parameters across a range of difficulties so as to identify test-takers at a range of abilities. 

IRT models are a useful tool for constructing and evaluating CDI instruments, as they can help to identify items that perform poorly in estimating underlying ability. For example,  @weber2018 used IRT to identify poorly-performing items in a CDI instrument for Wolof (a language spoken in Senegal). IRT can also be used in the construction in computer-adaptive testing [@makransky2016]. 

This appendix examines IRT models as a window into the psychometric properties of the CDI. In the first section, we explore latent factor scores using the English WS data. In the second section, we examine individual items and find generally positive measurement properties, although with some items at ceiling (included via carry-over from the Words and Gestures form). In the third section, we look at differences between comprehension and production in the WG form. In the fourth section, we look at the properties of the instrument by word category in both WS and WG. 

Overall, the conclusions of our analysis are that: 

* Latent factor scores may have some advantages relative to raw scores in capturing individuals' abilities, but for the purposes of the analyses we perform in the main body of the manuscript, they may carry some risks as well; hence we do not adopt them more generally. 
* In general, CDI WS items tend to perform well, but from a pure psychometric perspective there are a number of items that could be removed from the English WS form.
* Comprehension items in general tend to have less discrimination than production, suggesting that they are not as clear indicators of children's underlying abilities. 
* Function words tend to have lower discrimination than other items but the lexical class differences are not huge and do not interact with whether they are measured using production vs. comprehension. 

## Preliminary Estimation 

```{r psycho-irt_prelims_ws}
eng_ws <- read_feather("data/eng_ws_raw_data.feather")

d_ws <- eng_ws %>%
  mutate(produces = value == "produces") %>%
  filter(!is.na(category)) %>%
  select(data_id, produces, age, production, sex, definition) 

d_wide_ws <- d_ws %>%
  mutate(produces = as.numeric(produces)) %>%
  select(definition, produces, data_id) %>%
  spread(definition, produces)
  
d_mat_ws <- d_wide_ws %>%
  select(-data_id) %>% 
  data.frame %>%
  data.matrix

colnames(d_mat_ws) <- sort(unique(d_ws$definition))
rownames(d_mat_ws) <- d_wide_ws$data_id

# Requires no empty rows - `personfit` doesn't work with `removeEmptyRows=TRUE` even though the model fit will work that way. 

d_mat_ws <- d_mat_ws[complete.cases(d_mat_ws),]
```

Practically, we use the `mirt` package [@chalmer2012,@chalmers2016] to estimate the parameters of a four-parameter IRT model. The four-parameter model supplements the standard two-parameter model with two parameters corresponding to floor and ceiling performance for a particular item. Items with high rates of guessing or universal acceptance across test takers would tend to have abnormal values on these bounds. Our goal in this first analysis is simply to examine parameter estimates across individuals and items. 

```{r psycho-fit_irt_ws, eval = FALSE}
mod_4pl <- mirt(d_mat_ws, 1, itemtype='4PL', verbose=TRUE)

coefs_4pl <- as_data_frame(coef(mod_4pl, simplify = TRUE)$items) %>%
  mutate(definition = rownames(coef(mod_4pl, simplify = TRUE)$items))
fscores_4pl <- data_frame(data_id = rownames(d_mat_ws), 
                             ability = fscores(mod_4pl, method = "MAP")[,1])

save(file = "data/eng_ws_mod_4pl.Rds", "mod_4pl","fscores_4pl", "coefs_4pl")
```

```{r psycho-load_irt_data_ws}
load("data/eng_ws_mod_4pl.Rds")
```

```{r psycho-irt_summary}
d_ws_summary <- d_ws %>%
  group_by(data_id, sex, age) %>%
  summarise(production = production[1]) %>%
  right_join(fscores_4pl %>%
               mutate(data_id = as.numeric(data_id))) %>%
  filter(!is.na(sex))
```

```{r psycho-irt_summary_plot}
d_ws_summary %>%
  gather(measure, value, production, ability, age) %>%
  mutate(measure = fct_recode(measure, 
                              Production = "production", 
                              `Latent Ability` = "ability", 
                              `Age (months)` = "age") %>%
           fct_relevel("Age (months)", "Production")) %>%
  ggplot(aes(x = value)) + 
  geom_histogram(bins = 15) + 
  facet_wrap(~measure, scales="free_x")
```

We begin by estimating this model with data from `r nrow(d_mat_ws)` from the English (American) WG dataset. We first examine the histograms of latent ability scores and compare them with chronological age (because sampling is non-uniform) and raw production scores. As can be seen above, latent ability shows a peak in the middle of the range and generally fewer cases distributed at floor and ceiling. This shift indicates first, that floor and ceiling effects in the form are partially rectified by the model, and second, that many children in the middle of the form's range are not well-distinguished from one another. 

One question regarding these scores is whether they should be used in place of proportion scores for some of the estimation problems we encounter throughout the rest of the book. These latent ability scores might be overall better reflections of children's vocabulary than raw proportions in the best case. Nevertheless, we do not adopt them, for two reasons.

```{r psycho-gender_plot}
d_ws_gender <- d_ws_summary %>%
  gather(measure, value, production, ability) %>%
  mutate(measure = fct_recode(measure, 
                              `Raw Production` = "production", 
                              `Latent Ability` = "ability") %>%
           fct_relevel("Raw Production")) 

ggplot(d_ws_gender, aes(x = age, y = value, col = sex)) + 
  geom_jitter(alpha = .1) + 
  geom_smooth() + 
  facet_wrap(~measure, scales="free_y") + 
  ylab("Parameter Value") + xlab("Age (Months)")
```

```{r psycho-gender_summary}
d_ws_gender_summary <- d_ws_gender %>%
  group_by(measure, age) %>%
  summarise(female_advantage = mean(value[sex == "Female"]) - 
              mean(value[sex == "Male"])) %>%
  group_by(measure) %>%
  summarise(cv = sd(female_advantage) / mean(female_advantage))
  
```

First, they do not perform better empirically. The analysis above shows gender differences (cf. Chapter \@ref(#demographics)) by both measures. Surprisingly, there appear to be limited differences in the curves recovered by the analyses. For example, for raw production, the coefficient of variation across age in the gender differences is `r signif(d_ws_gender_summary$cv[d_ws_gender_summary$measure == "Raw Production"], 2)`. For the latent ability estimates, the CV is `r signif(d_ws_gender_summary$cv[d_ws_gender_summary$measure == "Latent Ability"], 2)`. Thus, based on the assumption that gender differences are constant across age, there is no evidence for better measurement. Of course, this assumption may be wrong, but this analysis does not yield evidence in favor of adoption. (Perhaps a smaller sample might yield different results; it could be that raw scores are stable due to the large amount of data in this analysis). 

Second, there are other negatives associated with swapping an imperfect but straightforward measure (raw scores) to a model-derived measure (latent ability). Interpretation clearly suffers if we use the model-derived measure, since readers will not be able to map scores back to actual behavior in terms of the checklist. In addition, model estimation issues across instruments introduce further difficulties in interpretation. Most obviously model estimates with smaller datasets may vary in unpredictable ways; similarly, the presence of poorly-performing items (see below) in certain datasets may lead to systematic issues in the latent estimates for those datasets. 

## Item effects in WS

```{r psycho-items_ws}
ggplot(coefs_4pl,  
       aes(x = a1, y = d)) + 
  geom_point(alpha = .3) + 
  geom_text_repel(data = filter(coefs_4pl, 
                                abs(a1) > 3.3 | abs(d) > 4), 
                  aes(label = definition), size = 3) + 
  xlab("Discrimination") + 
  ylab("Difficulty")
```


Our next analysis examines items in the same WS dataset. The plot above shows item discrimination and difficulty, with outlying items labeled. Visual inspection shows a long tail of items with limited discrimination and low difficulty (e.g., *mommy*, *ball*, *bye*, etc.). These are clearly those items that are produced by nearly all of the children in the sample -- they do not discriminate because they are passed by all children in the sample. If the only goal of the instrument were discrimination of different ability levels, they could likely be removed. On the lower right hand side of the plot, the remainder of items are clumped, with discrimination above zero and somewhat higher difficulty. The right-hand tip of this triangle shows the most diagnostic words (e.g., *run*, *kitchen*, and *table*), all of which effectively distinguish between the upper and lower groups of children in the sample. Finally, at the bottom of this triangle is a large cluster of words that are quite difficult. Some of these do not show good discrimination (e.g., *country*), since it is likely too difficult for nearly all children in the sample. 


```{r psycho-disc_hist}
ggplot(coefs_4pl,  
       aes(x = a1)) + 
  geom_histogram(binwidth = .2) + 
  xlab("Discrimination")
```

We can follow up on the question of word inclusion by examining the distribution of discrimination parameters alone. Overall, this plot suggests that there are some items that could be dropped without major penalty. In particular, along with the very easy words in the left tail, there is a large cluster of words that are perhaps too difficult to be useful for most children. 


```{r psycho-bounds_plot}
ggplot(coefs_4pl,  
       aes(x = g, y = u)) + 
  geom_point() + 
  geom_text_repel(data = filter(coefs_4pl, 
                                abs(g) > .4 | u < .75), 
                  aes(label = definition), size = 3) + 
  xlab("Lower bound (high base rate)") + 
  ylab("Upper bound (not known by many)")
```

As a separate check on this analysis, we can examine the upper and lower bounds estimated for particular words. These bounds show words that are known by only a small number of children at ceiling or have a very high floor (respectively). Examining those with a very low ceiling, we see items that are likely to be quite idiosyncratic, for a variety of reasons. For example, *babysitter*, *camping*, and *basement* likely vary by children's home experiences (further mediated by access to resources, parenting practices, and circumstances). In contrast, genital items (e.g. *vagina\**) vary by gender (see Chapter \@ref(words-demographics)). Examining those items with a very high base rate shows a similar set to those with very low discrimination patterns, suggesting that the four-parameter model may have fit these words as having a high chance level with essentially no discrimination ability. 

```{r psycho-weirds}
weird_words <- c("grrr", "woof woof", "moo","yum yum")

d_ws %>%
  filter(definition %in% weird_words) %>%
  group_by(definition, age) %>%
  summarise(produces = mean(produces, na.rm=TRUE)) %>%
  ggplot(aes(x = age, y = produces, col = definition))+ 
  geom_line() +
  ylim(0,1) +
  ylab("Proportion Producing") + 
  xlab("Age (months)")
  
```

Finally, we see items like *grrr*, *woof woof*, *moo*, and *yum yum*. These items show limited development across the age range of the instrument. Although most parents report children saying *yum yum* early, it is likely that some families either do not say  *yum yum* or do not report it on the form as a signal of gustatory pleasure (perhaps because their 30-month-old already says *delicious*). 



## Production and comprehension


```{r psycho-prod_comp}
eng_wg <- read_feather("data/eng_wg_data.feather")

d_wg <- eng_wg %>%
  mutate(produces = value == "produces", 
         understands = value == "understands") %>%
  filter(!is.na(category)) %>%
  select(data_id, produces, understands, age, production, sex, definition) 

d_wide_wg_produces <- d_wg %>%
  mutate(produces = as.numeric(produces)) %>%
  select(definition, produces, data_id) %>%
  spread(definition, produces)

d_wide_wg_understands <- d_wg %>%
  mutate(understands = as.numeric(understands)) %>%
  select(definition, understands, data_id) %>%
  spread(definition, understands)

d_mat_wg_produces <- d_wide_wg_produces %>%
  select(-data_id) %>% 
  data.frame %>%
  data.matrix

d_mat_wg_understands <- d_wide_wg_understands %>%
  select(-data_id) %>% 
  data.frame %>%
  data.matrix

colnames(d_mat_wg_produces) <- sort(unique(d_wg$definition))
rownames(d_mat_wg_produces) <- d_wide_wg_produces$data_id
colnames(d_mat_wg_understands) <- sort(unique(d_wg$definition))
rownames(d_mat_wg_understands) <- d_wide_wg_understands$data_id

# Requires no empty rows - `personfit` doesn't work with `removeEmptyRows=TRUE` even though the model fit will work that way. 

d_mat_wg_produces <- d_mat_wg_produces[complete.cases(d_mat_wg_produces) & rowSums(d_mat_wg_produces) > 0,]
d_mat_wg_understands <- d_mat_wg_understands[complete.cases(d_mat_wg_understands) & rowSums(d_mat_wg_understands) > 0,]
```

```{r psycho-fit_irt_wg, eval = FALSE}
mod_4pl_wg_produces <- mirt(d_mat_wg_produces, 1, itemtype='4PL', verbose=TRUE)
mod_4pl_wg_understands <- mirt(d_mat_wg_understands, 1, itemtype='4PL', verbose=TRUE)

coefs_4pl_wg_produces <- as_data_frame(coef(mod_4pl_wg_produces, 
                                            simplify = TRUE)$items) %>%
  mutate(definition = rownames(coef(mod_4pl_wg_produces, simplify = TRUE)$items))

coefs_4pl_wg_understands <- as_data_frame(coef(mod_4pl_wg_understands, 
                                            simplify = TRUE)$items) %>%
  mutate(definition = rownames(coef(mod_4pl_wg_understands, simplify = TRUE)$items))

save(file = "data/eng_wg_mods_4pl.Rds", 
     "mod_4pl_wg_produces",
     "mod_4pl_wg_understands",
     "coefs_4pl_wg_produces",
     "coefs_4pl_wg_understands")
```

```{r psycho-load_irt_data_wg}
load("data/eng_wg_mods_4pl.Rds")

coefs_4pl_wg <- bind_rows(coefs_4pl_wg_produces %>%
                            mutate(measure = "produces"),
                          coefs_4pl_wg_understands %>%
                            mutate(measure = "understands"))

wg_comp_prod <- coefs_4pl_wg %>%
  select(a1, d, measure) %>%
  gather(parameter, value, a1, d) %>%
  mutate(parameter = fct_recode(parameter, 
                                Discrimination = "a1", 
                                Difficulty = "d") %>%
           relevel("Difficulty")) 

ggplot(wg_comp_prod, 
       aes(x = value)) +
  geom_histogram(binwidth = .5) + 
  facet_grid(measure ~ parameter) + 
  xlim(-5,5)

wg_comp_prod_summary <- wg_comp_prod %>%
  group_by(measure, parameter) %>%
  summarise(value = mean(value))
```

We next use IRT to estimate whether there are differences between production and comprehension, using WG data. The plot above shows IRT parameter values for discrimination and difficulty for production and comprehension (a few extreme parameter values are truncated in the plot for ease of seeing general trends). There are clear distribution differences on both measures. Difficulty is much higher (negative values) for production relative to comprehension, reflecting the expected asymmetry. 

We also see clear evidence that in general comprehension items show less discrimination, consistent with the hypothesis that production behavior is a clearer signal of children's underlying knowledge than assumed comprehension. This pattern is visible in two ways. First, discrimination values are negative for more items in comprehension, indicating items that are not measuring ability. Second, and more importantly, mean discrimination is substantially lower for comprehension relative to production (`r signif(wg_comp_prod_summary$value[wg_comp_prod_summary$parameter == "Discrimination" & wg_comp_prod_summary$measure == "understands"],2)` vs. `r signif(wg_comp_prod_summary$value[wg_comp_prod_summary$parameter == "Discrimination" & wg_comp_prod_summary$measure == "produces"],2)`). Very few comprehension items have discrimination is greater than 2. 

This pattern of findings -- lower discrimination values for comprehension -- could be due to at least two possibilities. One is that parents are better reporters of production than comprehension, and hence these items are more discriminative of true behavior. The source of error in this case would be parents' mistaken belief that their child understands a word. The second is that comprehension is a fundamentally more variable construct and that, hence, individual word knowledge consistent with understanding could be due to partial knowledge. Here the source of error is variance in how well children know the meanings of words. We cannot distinguish between these two models, but they have very different underlying implications for the CDI. On the first model, comprehension is simply hard to measure with parent report. On the second, comprehension is perhaps a richer and more nuanced behavior than production. 

## Properties by lexical category

One hypothesis that we have often speculated about is the question of whether there are special psychometric issues with particular word classes. For example, do parents struggle especially to identify whether children produce or understand function words? 


```{r psycho-lexcat}
coefs_4pl <- coefs_4pl %>%
  left_join(items %>% 
               filter(language == "English (American)", 
                      form == "WS")) 

class_summary <- coefs_4pl %>%
  group_by(lexical_class) %>%
  summarise(sd_a1 = sd(a1, na.rm=TRUE), 
            a1 = mean(a1))

a <- ggplot(coefs_4pl, 
       aes(x = a1, y = d, col = lexical_class)) + 
  geom_point(alpha = .3) + 
  geom_text_repel(data = filter(coefs_4pl, 
                                abs(a1) > 3.3 | abs(d) > 4), 
                  aes(label = definition), size = 2) + 
  xlab("Discrimination") + 
  ylab("Difficulty")

b <- ggplot(coefs_4pl, 
       aes(x = a1, fill = lexical_class)) + 
  geom_histogram() + 
  xlab("Discrimination") +
  xlim(0,4)

grid.arrange(a, b)
```


The plot above reproduces the WG by-item analyses above but with color given by lexical class. Many of the easy, non-discriminating items are found in the "other" section. In contrast, the hardest items tend to be function words. These items tend to have lower discrimination on average (`r signif(class_summary$a1[class_summary$lexical_class == "function_words"],2)`) compared with nouns (`r signif(class_summary$a1[class_summary$lexical_class == "nouns"],2)`), adjectives (`r signif(class_summary$a1[class_summary$lexical_class == "adjectives"],2)`), and especially verbs (`r signif(class_summary$a1[class_summary$lexical_class == "verbs"],2)`). Nevertheless, the situation is not dire: most have a discrimination parameter above one. Thus, although function words are not the most discriminative items on the CDI WS, these items still appear to encode valid signal about children's abilities.


```{r psycho-lexcat_sumamry}
coefs_4pl_wg <- coefs_4pl_wg %>%
  left_join(items %>% 
               filter(language == "English (American)", 
                      form == "WG")) 

wg_comp_prod <- coefs_4pl_wg %>%
  select(a1, d, measure, lexical_class) %>%
  gather(parameter, value, a1, d) %>%
  mutate(parameter = fct_recode(parameter, 
                                Discrimination = "a1", 
                                Difficulty = "d") %>%
           relevel("Difficulty")) 

class_summary <- wg_comp_prod %>%
  group_by(lexical_class, measure, parameter) %>%
  summarise(mean = mean(value), 
            sd = sd(value))

# 
# ggplot(wg_comp_prod, 
#        aes(x = value, fill = lexical_class)) +
#   geom_histogram(binwidth = .5) + 
#   facet_grid(measure ~ parameter) + 
#   xlim(-5,5)

# wg_comp_prod_summary <- wg_comp_prod %>%
#   group_by(measure, parameter) %>%
#   summarise(value = mean(value))

ggplot(filter(class_summary, parameter=="Discrimination"), 
       aes(x = lexical_class, y = mean, col = lexical_class)) + 
  geom_pointrange(aes(ymin = mean - sd, ymax = mean + sd)) + 
  coord_flip() + 
  facet_grid(measure ~ .) + 
  geom_hline(yintercept = 0, lty = 2) + 
  ylab("Discrimination")

```

In our last analysis, we turn to the WG data. The plot above shows the mean and standard deviation for discrimination parameter values. In production, the higher discrimination shown on the whole (above) is likely due to the strong performance of nouns. In contrast, mean discrimination for other words is low. This pattern may be due to the overall sparsity of early production data for non-noun items. In comprehension, in contrast, there is a moderate level of discrimination for all classes except "other" (which includes items like *mommy* and *daddy* and a variety of animal sounds and social routines). One hypothesis about this finding is that, especially early on, parents are very generous in their interpretation of whether their child understands these words. 

In sum, we do not find evidence that function words are particularly bad. Rather, there are some low-performing items spread across all categories of the CDI form, and many of these likely perform poorly for the reasons described above -- especially difficulty in interpretation of very early behavior and variability in home experience.


<!-- ## Special cases by language -- RUSSIAN ANIMAL SOUNDS!! -->


## Stability of the CDI


```{r psycho-style_long_data}
longitudinal_admins <- admins %>% 
  mutate(langform = paste(language, form)) %>%
  group_by(langform, original_id) %>% 
  count() %>% 
  filter(n > 1) 

n_long_ws <- admins %>%
  filter(original_id %in% longitudinal_admins$original_id, 
         language %in% c("Norwegian", "English (American)"),
         form == "WS") %>%
  group_by(original_id, language, source_name) %>%
  mutate(n_admins = n()) %>%
  filter(n_admins > 1)

n_long_wg <- admins %>%
  filter(original_id %in% longitudinal_admins$original_id, 
         language %in% c("Norwegian", "English (American)"),
         form == "WG") %>%
  group_by(original_id, language, source_name) %>%
  mutate(n_admins = n()) %>%
  filter(n_admins > 1)

ms_ws <- n_long_ws %>%
  group_by(language, age) %>%
  summarise(production = median(production))

ms_wg <- n_long_wg %>%
  group_by(age) %>%
  summarise(production = median(production))
```

The first locus for individual differences in language acquisition is the rate of growth. As already discussed in Chapter \@ref(vocab), there is substantial variability between children in vocabulary size. As we showed there, across many languages there is a consistent -- and high -- rate of variability between children. 

Because these data were cross-sectional in nature rather than longitudinal, they cannot answer the question of whether the variability observed is *stable* as well as *substantial* (parphrasing Bates). In this and the next subsection, we make use of longitudinal data in the Wordbank dataset to investigate this idea. 

```{r psycho-style_long_data_table}
datatable(longitudinal_admins %>% 
            group_by(langform) %>%
            count %>% 
            arrange(desc(nn)))
```

In our first investigation of the longitudinal data, we look at how stable individual children are. There are only a small number of deeply longitudinal corpora in Wordbank, so we will limit our investigation to two languages: Norwegian and English. Furthermore, the largest group of longitudinal data cover the WS form so we restrict to these data for simplicity. Within each of these datasets, the modal number of observations is two, but there are some children with more than 10 CDIs on record. 

```{r psycho-style_long_data_spaghetti}
ggplot(filter(n_long_ws, n_admins > 10, 
              language == "Norwegian"), 
       aes(x = age, y = production, col = fct_reorder(original_id, production))) + 
  geom_line() + 
  # facet_wrap(~language) + 
  scale_colour_discrete(guide = FALSE) + 
  ylab("Words Produced") + 
  xlab("Age (Months)")
```


The figure above shows the trajectories of children who were measured a large number of times (>10), and includes Norwegian data only due to data sparsity issues in English. Thes trajectories appear quite stable; the ranking of individuals does not appear to change much over the course of several years. This conclusion is ratified by other studies using different datasets, for example @bornstein2012, who found substantial stability ($r = .84$) between latent constructs inferred from early language at 20 months and later language measured at 48 months. 

```{r psycho-style_long_data_ecdf}
n_cross_ws <- admins %>%
  filter(language %in% c("Norwegian", "English (American)"),
         form == "WS") %>%
  group_by(original_id, language) %>%
  mutate(n_admins = n()) %>%
  filter(n_admins == 1)

# gets percentiles for longitudinal based on cross-sectional. 
get_empirical_percentiles <- function (df) {
  # assumes ages are uniform in this sample
  this_age <- df$age[1]
  this_lang <- df$language[1]
  
  cross_data <- filter(n_cross_ws, 
                       age == this_age, 
                       language == this_lang)
  
  Fn <- ecdf(cross_data$production) 
  
  df$percentile <- Fn(df$production)
  
  return(df)
}

n_long_ws <- n_long_ws %>%
  split(list(.$age,.$language), drop = TRUE) %>%
  map_df(get_empirical_percentiles)
```

```{r psycho-style_long_data_norwegian_ecdf}
ggplot(filter(n_long_ws, n_admins > 10, 
              language == "Norwegian"), 
       aes(x = age, y = percentile, col = fct_reorder(original_id, production))) + 
  geom_line() + 
  scale_colour_discrete(guide = FALSE)
```
One way to operationalize the question of stability is how children's percentile ranks tend to change over time. We examine this question creating an empirical CDF for each age group. We could use a model-based method (e.g., the `gcrq` method used in the Wordbank app and Chapter \@ref(vocab) and \@ref(demographics)) but in practice we have enough data in each of these languages that this method should perform well. As shown above, these are visually quite stable. 
```{r psycho-style_pairwise_long_cors}
age_binsize <- 2
  
long_cors <- n_long_ws %>%
  unite("id", c("original_id", "source_name")) %>%
  mutate(age = round(age/age_binsize) * age_binsize) %>% # round age into two-month bins
  select(id, language, age, percentile) %>%
  group_by(id, age) %>%
  sample_n(size = 1) %>% # if there are multiple measurements from one age, remove
  ungroup 
  
long_cor_ns <- long_cors %>%
  split(.$language) %>%
  map_df(function (df) {
    language <- df$language[1] 
    df %>% 
      select(-language) %>%
      pairwise_count(age, id) %>%
      mutate(language = language) %>%
      rename(age1 = item1, 
             age2 = item2)
  })

long_cor_pairs <-  long_cors %>% 
  spread(age, percentile) %>%
  split(.$language) %>%
  map_df(function (df) {
    language <- df$language[1]
    cor_mat <- select(df, -language, -id) %>%
      cor(use='pairwise.complete.obs') 
    
    as_data_frame(cor_mat) %>%
      mutate(age2 = rownames(cor_mat)) %>%
      gather(age1, cor, -age2) %>%
      mutate(language = language,
             age1 = as.numeric(age1), 
             age2 = as.numeric(age2), 
             dist = age2 - age1) %>%
      filter(dist > 0)
  }) %>%
  left_join(long_cor_ns)

ggplot(filter(long_cor_pairs, n >= 50), 
       aes(x = dist, y = cor, col = age1)) +
  geom_point(aes(size = n)) + 
  geom_smooth(method = "lm", method.args = list(weight = n)) + 
  facet_wrap(~language) + 
  ylab("Correlation") + 
  xlab("Measurement Gap (Months)") + 
  scale_color_viridis(name = "Starting Age") + 
  scale_size_continuous(name = "N")  + 
  ylim(0,1)
    
```  

This transformation to percentile ranks allows us to assess the correlation between a child's percentile rank at time 1 and their rank at time 2, depending on the gap between these two. Because of sparsity, we bin children into two-month age bins and eliminate age bins with fewer than 50 children, then calculate between-bin correlations in percentiles. The figure above shows this analysis, which reveals that percentile ranks are quite stable; across a 2-4 month age gap they are correlated at better than .8. This stability declines to around .5 at 16 months, but this decline should be taken with a grain of salt. First, this range is a doubling of the child's age, so stability might be expected to be lower. But second, even with the use of percentile ranks, many childen who are measured longitudinally across a 16-month gap will be expected to move from the floor of the form to the ceiling, compromising measurement accuracy. 


```{r psycho-style_mirt_etc, eval=FALSE}
mirt_params <- function(dx) {
  language <- dx$language
  form <- dx$form
  print(paste(language, form))
  
  lang_data <- get_instrument_data(language = language, form = form, 
                                   administrations = TRUE, iteminfo = TRUE)
  
  d_lang <- lang_data %>%
    filter(!is.na(lexical_class)) %>%
    mutate(produces = value == "produces") %>%
    select(data_id, item_id, produces, age, production) 
  
  d_lang_wide <- d_lang %>%
    mutate(produces = ifelse(is.na(produces), 0, as.numeric(produces))) %>%
    select(item_id, produces, data_id) %>%
    spread(item_id, produces)
  
  d_lang_mat <- d_lang_wide %>%
    select(-data_id) %>% 
    data.frame %>%
    data.matrix
  
  colnames(d_lang_mat) <- sort(unique(d_lang$item_id))
  rownames(d_lang_mat) <- d_lang_wide$data_id
  
  d_lang_mat <- d_lang_mat[complete.cases(d_lang_mat),]
  mod_4pl <- mirt(d_lang_mat, 1, itemtype='4PL', verbose=TRUE)

  fscores <- data_frame(data_id = rownames(d_lang_mat), 
                        ability = fscores(mod_4pl, method = "MAP")[,1])
  fscores$language <- language
  fscores$form <- form
  
  fscores <- fscores %>%
    mutate(data_id = as.numeric(data_id)) %>%
    left_join(get_administration_data(language = language, 
                                      form = form))
  
  return(fscores)
}
  
mirt_params <- instruments %>% 
  filter(form %in% WSs, language %in% c("Norwegian", "English (American)")) %>% 
  mutate(idx = 1:n()) %>%
  split(.$idx) %>%
  map_df(possibly(mirt_params, otherwise = data_frame()))

write_feather(mirt_params, "data/long_mirt_params.feather")
```


```{r psycho-style_long_mirt_stability}
mirt_params <- read_feather("data/long_mirt_params.feather")

long_cors_mirt <- n_long_ws %>%
  left_join(select(mirt_params, data_id, ability, language, form)) %>%
  unite("id", c("original_id", "source_name")) %>%
  mutate(age = round(age/age_binsize) * age_binsize) %>% # round age into two-month bins
  select(id, language, age, ability) %>%
  group_by(id, age) %>%
  sample_n(size = 1) %>% # if there are multiple measurements from one age, remove
  ungroup 
  
long_cor_pairs_mirt <-  long_cors_mirt %>% 
  spread(age, ability) %>%
  split(.$language) %>%
  map_df(function (df) {
    language <- df$language[1]
    cor_mat <- select(df, -language, -id) %>%
      cor(use='pairwise.complete.obs') 
    
    as_data_frame(cor_mat) %>%
      mutate(age2 = rownames(cor_mat)) %>%
      gather(age1, cor, -age2) %>%
      mutate(language = language,
             age1 = as.numeric(age1), 
             age2 = as.numeric(age2), 
             dist = age2 - age1) %>%
      filter(dist > 0)
  }) %>%
  left_join(long_cor_ns)
```

```{r psycho-style_long_raw_stability}
long_cors_raw <- n_long_ws %>%
  unite("id", c("original_id", "source_name")) %>%
  mutate(age = round(age/age_binsize) * age_binsize) %>% # round age into two-month bins
  select(id, language, age, production) %>%
  group_by(id, age) %>%
  sample_n(size = 1) %>% # if there are multiple measurements from one age, remove
  ungroup 
  
long_cor_pairs_raw <-  long_cors_raw %>% 
  spread(age, production) %>%
  split(.$language) %>%
  map_df(function (df) {
    language <- df$language[1]
    cor_mat <- select(df, -language, -id) %>%
      cor(use='pairwise.complete.obs') 
    
    as_data_frame(cor_mat) %>%
      mutate(age2 = rownames(cor_mat)) %>%
      gather(age1, cor, -age2) %>%
      mutate(language = language,
             age1 = as.numeric(age1), 
             age2 = as.numeric(age2), 
             dist = age2 - age1) %>%
      filter(dist > 0)
  }) %>%
  left_join(long_cor_ns)
```


```{r psycho-style_cor_methods_comparisons}
min_n <- 50
all_cors <- left_join(long_cor_pairs_mirt %>% 
                        filter(n >= min_n) %>%
                      group_by(language, dist) %>% 
                      summarise(`IRT ability` = mean(cor, na.rm=TRUE)), 
                    long_cor_pairs %>% 
                      filter(n >= min_n) %>%
                      group_by(language, dist) %>% 
                      summarise(Percentile = mean(cor, na.rm=TRUE))) %>%
  left_join(long_cor_pairs_raw %>% 
              filter(n >= min_n) %>%
            group_by(language, dist) %>% 
            summarise(`Raw score` = mean(cor, na.rm=TRUE))) %>%
  filter(is.finite(`Raw score`)) %>%
  gather(measure, value, `IRT ability`, Percentile, `Raw score`)

ggplot(all_cors, aes(x = dist, y = value, col = measure)) +
  geom_line() + 
  facet_wrap(~language) + 
  ylim(0,1) + 
  ylab("Correlation") + 
  xlab("Measurement Gap (Months)") + 
  scale_color_ptol(name = "Measure")
```

To test this last hypothesis, we evaluated the longitudinal stability of correlations using the same analysis as above. We varied whether we used latent abilities derived from a 4-parameter IRT model as in Appendix \@ref(psychometrics), raw scores, or the percentiles used above. While the IRT model abilities realized a consistent improvement over the use of raw scores, percentiles realized a further gain over IRT ability scores. 

In sum, the variability between children that we observe in the CDI is quite stable longitudinally. It declines over time, but some of this decline may simply be due to the unavoidable limitations of CDI forms with respect to floor and ceiling effects. 
