# Theoretical Foundations {#theory-intro}

The ability to use language in its lexical and combinatorial richness -- with hundreds of thousands of words, and an infinity of possible meanings -- is uniquely human. The study of language acquisition has been a traditional locus of our search to understand the nature of this ability. As such, the nature of the abilities that allow human children to acquire a language is one of the historical "great debates," in which different proposals about the architecture of the human mind and the nature of human uniqueness have been discussed. Does language arise from domain-specific adaptations for syntactic structure [@chomsky1981;@chomsky2014]? Or does it arise from a combination of environmental input and sophisticated, general-purpose learning mechanisms [@elman1996]? Two poles have traditionally emerged in this discussion (Figure \@ref(fig:poles) ): from domain-general empiricist proposals to domain-specific, nativist proposals. 

In this chapter, we begin by presenting the perspective from these poles but contend that they are rarely helpful in the practice of understanding the scope and course of early language learning. Instead, we argue for the development of theories that describe the scope and course of language learning as a whole, as well as its quantative variation across children, langugaes, and cultures. These concerns lead us to frame our own study in terms of a set of distinct theoretical issues: capturing consistency and variability; drawing connections across timescales of learning; and the notion of process, rather than content, universals. We end by discussing the role of replication and larger datasets in building quantitative theories. 


## The Picture To Date

```{r poles, out.width="70%"}
knitr::include_graphics("images/axis.jpeg")
```

Empiricist proposals emphasize the ability of children to learn structure across domains, the richness of the distributional input that children are exposed to, and their ability to create appropriate abstractions from structured linguistic input. The components of these proposals are children's general statistical learning abilities [@saffran1996;@aslin2002],
When applied to linguistic input, even general statistical tools can recover some aspects of structure [@redington1998,@frank2012].
Such proposals have as their primary challenge the ability of children to create fast abstractions that allow them to create systematic word orders even in the absence of input [@goldin-meadow1981], interpret the arguments of novel verbs [@gertner2006], and show evidence of abstract category structure for syntactic categories such as determiners [@yang2013] or  structures such as the dative [@conwell2007], even at a very young age.

Nativist proposals in contrast tend to focus on the complexity of the grammatical structures that even young children appear to possess, relative to their rarity in children's environment. Such arguments emphasize the poverty of the linguistic stimulus [e.g., @legate2002] in contrast to the richness of the structual generalizations children make [e.g., @crain2000]. The type of proposals that are on offer within this space include "principles and parameters" theories, in which languages share a set of syntactic principles that govern syntactic combination but vary on a relatively small set of parameters that control how these structures vary [@baker2005,@yang2002]. But these proposals are challenged by the vast cross-linguistic differences in syntactic abstractions [@evans2009], by the character and scope of children's syntactic generalizations [which are often tied to specific lexical structures; @tomasello000], and by evidence of early input-sensitive learning and generalization both within specific domains [e.g., @meylan2017] and in artificial language tasks [@gomez1999]. 

The "great debate" between these viewpoints is philosophically appealing, but has also led to a polarization of the field of language acquisition. Typically researchers work in their own siloed traditions (empiricist or nativist), and focus on individual phenomena that do not make contact with one another -- a classic version of @kuhn1964's "paradigms." Research in the nativist tradition often focuses on particular syntactic phenomena that are largely neglected in the empiricist tradition [e.g., the "optional infinitive," @wexler1998; but cf. @freudenthal2010]. In contrast, research in the empiricist tradition has often used artificial language learning tasks that are argued not to reflect on the underlying structural properties that are claimed to be innate [e.g., @lany2010; cf. @yang2004]. By focusing on different paradigms and phenomena and theorizing using distinct vocabularies, these traditions make less and less contact. 

In addition, these theories are frameworks, rather than than actual hypotheses: Few proposals can be said to generate testable and clearly competing predictions, even within a specific domain. Any individual observation cannot be said to be inconsistent with any but the absolute strongest nativist or empiricist position. To be tested, they must be instantiated in specific computational mechanisms. Computational models are an important proving grouping, but in practice they end up less differentiated than rhetoric suggests. In order to get off the ground in performing a particular empirical task, theories must often help themselves to generous amounts of both innate structure -- in the form of structured inputs from social, cognitive, or perceptual domains -- and statistical learning abilities [@roy2003;@alishahi2008;@frank2009;@frank2010;@yang2004].^[The research on the nature of inflectional morphology -- the "past tense debate" -- is one place where computational models played a foundational role in instantiating theoretical claims about innateness and representational structure [e.g., @rumelhart1986;@pinker1988;@plunkett1993;@plunkett1991;@plunkett1996;@marcus1995;@marchman1996].]

Further, when these viewpoints come together, they are often instantiated in debates over phenomena that -- from a bird's eye, or even parents' eye, view -- are relatively trivial in the general course of language development. Abstraction debates have played out in the acquisition of the definite determiner "the" [@valian1986;@pine1997;@yang2013;@meylan2017], auxilliary inversion [@pullum2002;@legate2002], the use of anaphoric "one" [@akhtar2004;@regier2004;@lidz2003], for example. These phenomena are occasionally observable in the children of linguistically-trained parents, but even the closest observer would be forgiven for being more compelled by global changes in a child's language competence rather than the occasional syntactic slip. Further, all of these phenomena concern syntactic abstractions, reflecting a broader historical argument that syntactic structure is the heart of the uniquely human language faculty [@chomky1967], and that other aspects of language tend to be shared with other species [@hauser2002]. 

But syntax is far from the only unique or notable feature of human communication, from an evolutionary perspective [e.g., @pinker2005;@tomasello2010]. The nature and range of communciative gestures, the variety of sounds, and the diversity of lexical items all  are relatively unprecedented -- especially in the primate lineage. And these observable aspects of language -- as well as the emergence of syntactic structure more broadly -- are some of what makes  the broad course of language acquisition striking from the perspective of a clinician or a parent. We notice the first communicative signals, the emergence and rapid growth of of vocabulary, the beginning of the productive combination of words, increases in the length and complexity of utterances, and the patterns of error and overgeneralization that remain in early childhood. 

These broader patterns of language learning are the natural focus of investigations like ours that use parent report to learn about children's language. While parents are attentive and accurate observers of communicative gesture, vocabulary, and word combination, without linguistic training they may not even notice subtleties like non-productive determine use, auxilliary inversion, or anaphoric "one." Further, these investigations can in many case make productive contact with the rich literatures on early communication, speech perception [e.g., @kuhl2004], word learning [e.g., @bloom2002;@snedeker2009], and grammatical productivity through verb structure [@fisher2010]. While debates over the nature of syntactic knowledge and abstraction have raged, other subfields of language acquisition have prospered. 

Research in these subfields makes at most limited contact with broad questions of nativism and empiricism, in part because they deal with phenomena that are language specific -- sounds, lexical items, grammatical constructions -- and hence that children *must* learn from their input. The question is then about the mechanisms and constraints that guide this process of learning, rather than about any posited universal or innate content (even at the level of abstractions). 

## Making Progress

So what should a unifying theoretical framework for language learning look like? Despite our critiques above, we still believe in the importance of the search for core, universal aspects of language learning that elucidate the process by which children acquire this uniquely human ability. Yet we believe that the sort of theory that describes such universals will likely look radically different from its historical antecedents. Below and in the remainder of this chapter, we sketch some aspects of what such a theory will look like and how this vision connects to the remainder of our manuscript. 

First, any universal is likely to be a statistical or quantitative universal -- we refer to these as "consistencies." The variation across the world's languages is such that only the most tautological facts will be truly invariant [@evans2009]. Further, we are unlikely to be able to access the kinds of samples that would allow us to make claims of universality [@piantadosi2014]. Thus, we should talk about relative consistency and variability of particular phenomena rather than any sorts of absolutes.

Second, language learning takes place at the timescale of years. CDI forms provide a global snapshot of a child's language at a particular point in time, rather than demonstrating the operation of a particular mechanism or principle. Substantial reconstruction is necessary to understand how processes operating over seconds -- for example, online statistical learning or pragmatic inference -- would result in particular structures accreting over time in the vocabulary. Thus, consistencies we observe are at best the basis for abductive inferences about underlying mechanisms. 

Third, the regularities we observe are in the learning of vocabulary and constructions, rather than syntactic rules. These items *must* be learned from data. Thus any putative universals identified in our investigation must not be "content universals" that specific particular grammatical rules or linkages. They must be "process universals" in the sense that they specify mechanisms or processes that unfold over time and operate over children's interactional input in ways that produce the observed consistencies. 

<!-- Note that these got merged below.  -->

Finally, the project of this book is fundamentally about empirical unification of phenomena. By replication of analyses across different datasets, we provide a testbed for theoretical ideas that make contact with data. We believe our work here embodies the transformational potential of data. Data can provide precision in measurement, though -- as we are at pains to point out throughout -- doing so requires constant attention to sources of bias.


## Variability and Consistency

As we noted in the introduction, observing what "hangs together" in development can provide clues to the underlying system [@bates1988]. We think of these correlations as loose targets for theorists: a successful theory can gain support by providing an account for these observations. Crudely put, if a theory posits that some aspect of language acquisition is universal, it should be relatively more consistent in our data. 

What are the units over which we compute variability and consistency? We refer to these as "signatures" -- loosely, things that can vary across populations. In practice, a signature can be the output of any analysis, with the simplest being vocabulary size or variability itself (as in Chapter \@ref(vocab)). Signatures are linked to particular theoretical goals by arguments about the validity of an analysis -- for example, the argument of @bates1994 that the over-representation of nouns in early vocabulary is a meaningful dimension of variation between individuals. A signature for our purposes is thus an analysis that yields a set of numbers. In nearly every chapter of the book, we define one or several signatures whose variability we can measure. 

Different sources of variance provide different sorts of evidence. One sense of the notion of "universal" that dates to early generative syntax (CITE?) is the notion of typological invariance [@greenberg1963]. Following this general idea, in the majority of the book we focus on variability in some signature across languages. The implied inference is that consistency across languages points to the idea that a signature results from some mechanism (more on inferences about mechanism below) that is independent of the language being learned and the context in which it is learned. 

But when we assess the variability of some signature across datatsets, many things vary that are not the target of our inference. Although language and culture typically vary (except in the case of multiple instruments that are assessed on the same data source), many other things vary as well. Different datasets are constructed by different researchers with different goals. They use different instruments with different items -- and different length and composition. These instruments are administered to different samples, with different sampling strategies. And the nature of the administration is different as well. 

Thus, when a particular response appears to be consistent, we can say *a fortiori* that none of these sources of variation appear to have affected the consistency of the response. (Or at least that if they have, they have canceled each other out in a highly non-random way). But when variability does occur, we cannot make the opposite inference. Variability has many explanations, but consistency tends to point us towards a single inference. 

We focus on cross-*dataset* variabity as the primary source of variability in this book. We refer to this variability throughout as cross-*linguistic* variability, though in fact there are a number of caveats that must be stated. First, many things vary between datasets far beyond language (as noted above). And second, some datasets represent the same language in different dialects (e.g., Australian and British English). Some even reflect the same language and dialect, measured using two different instruments (e.g., the two Beijing Mandarin datasets). In some cases we will even leverage these parallels to help us rule out alternative explanations. The reasons we focus on cross-dataset variability are three.

First, datasets vary so much that -- assuming this variation is somewhat random -- claims of consistency are stronger when they emerge from this sort of data. Imagine counterfactully that all of the instruments we used had exactly the same structure and item set, and all were administered identically. Certainly this lack of variation would make our life easier in a number of ways when making quantitative comparisons between datasets! But it also then means that these consistencies would be confounded in our data -- particular item sets (plausibly) or administration instructions (somewhat less plausibly) could be the source of an observed consistency in the data. In contrast, while the messiness and inconsistency of the data in the Wordbank dataset make many aspects of our analysis much harder, it actually increases the strength of the inferences we can draw when -- despite this -- we see some phenomenon emerge with striking consistency.^[Of course, we also consider the confounds that do remain at the end of the book. In particular, confounding related to the parent report structure of the CDI is a major risk.]

Second, the genesis of the investigations documented in this book was in part the observation that several phenomena that we examined were strikingly consistent across languages. For example, the gender effects shown in Chapter \@ref(demographics) were much more consistent than any of us thought [being at the time ignorant about the previous literature in this particular area; @eriksson2012]. Empirically, we found a lot to look at that was both surprising and interpretable when we examined well-known signatures as they varied across languages. Thus, our motivation is in part the emergent success of this approach.  

The final reason we consider cross-language variability as our primary lever is a negative one. The obvious competitor as a source of variability is variation across individuals. We examine this variability briefly in Chapter \@ref(vocab) and more extensively in Chapter \@ref(style). While we document substantial and stable variability across individuals [echoing @bates 1994], this variability empirically proves to be less of a lever into theoretical issues of interest than we would hope. One part of this is data-related -- we have far more cross-sectional than longitudinal data in the Wordbank dataset -- and hence we cannot track stability and change over time as easily or powerfully as we would like. Further, we have very few additional measures on most children in the dataset (beyond the occasional demographic feature). In addition, as we show in Chapter \@ref(style), much apparent variability in children's style of language learning can be traced to variation in rate. Thus, and in contrast to the exciting emergent conclusions from cross-lingustic variation, individual variation appears to be a less powerful theoretical lever. 

In Chapter \@ref(theory), we bring together estimates of variability of individual signatures from each of the earlier constituent chapters. We combine these into a single, data-driven continuum from absolute consistency to high variability, and use this continuum to drive speculations about the sorts of mechanisms that would produce a particular set of consistencies.

## Process Universals

### Preconditions

Imagine we were to uncover an aspect of language development that was completely consistent across languages. (Surprisingly, as we'll see in Chapter \@ref(theory) there are some!). What could we then infer from this observation? Not much, it turns out. The observed regularity could be due to different sources in different datasets or it could be uninteresting from a theoretical perspective.  

First, even if the consistency is interesting, any inference from it will always be abductive -- an inference backwards from observation to cause These abductive inferences will always be underconstrained and tentative, thus they will always be at best empirically-grounded speculations that should be brought together with other data to make a test. In some sense, this is the fundamental caveat governing our entire enterprise here. The research design is correlational and so causal inferences are not available. 

But things can go wrong even within this more limited paradigm. For example, we could observe that, across languages, we saw hypothetically that some word X was always produced earliest. But it could be the case that the word happened to be learned earliest in some languages because it was short and easy to pronounce, while in other languages it was learned early due to a high frequency of usage in the input. This example is artificial of course, but illustrates the difficulties of reverse inference from consistency. Similarly, we could observe that a certain distributional form always described children's vocabulary estimates, across languages. This regularity could be due to the operation of the central limit theorem rather than any interesting or substantive mechanism that we might be interested in as psychologists. 

These problems mean that we need to have two (somewhat informal) conditions on the consistencies that we posit. First, we need to consider the possibility of multiple routes to the same observed consistency. To the extent that observed regularities are specific and surprising, it will be less likely that there are multiple routes across different languages to observing the same thing. Second, for any potential causal story that we posit, we need to be able to posit a plausible or interesting causal story that does *not* generate the observed regularity. The tightness of this comparsion with a counterfactual governs the strength of the inteference. 

### The nature of the universals

Suppose the consistency we identify meets the conditions we describe above: it is sufficiently surprising that we don't see a parsimonious story for how the data for different languages could have been generated by different processes, and there are close counterfactuals in which this consistency did not emerge. Further, in our example suppose we have a larger and more diverse set of languages and cultures represented in our dataset such that we can justify using the title "universal" rather than the more descriptive and limited "consistency." We can then imagine trying to constrain the nature of the sort of universal that could give rise to this type of consistency.  

What can we say about putatitive universals? By virtue of the learning problem that they arise from, they cannot be universals of *content*. A child's vocabulary is made up of individual words, each arising from a set of specific interactional circumstances (e.g., the trip to the zoo where a giraffe was seen for the first time). And each of these words is -- of course -- specific to a particular language. Thus, there is no viable sense in which any possible universals for vocabulary learning can be content universals: no particular content of this type could be innately given. For this reason, we describe these as "process" universals: they relate to the process by which each individual extracts a lexicon from their own idiosyncratic linguistic experiences.

Further, these 



From the perspective of the language learning literature, there are obvious candidates for the sort of universals we are talking about. The general idea of "statistical learning" is one  [@saffran1996a;@saffran2018]. 

Processes of generalization are also implicated. 


In addition, a number of process*ing* factors might lead to *processes* that are universal. Developmentalists will see 

<!-- COGNITIVE COMPONENTS, PROCESSING SPEED, ATTENTION, GENERALIZATION, LEARNING -->

<!-- Yet these process universals need not be internal to the child  -->

<!-- - IT'S ABOUT LEARNING - UPTAKE OF INPUT -->


<!-- - UNFOLDING OVER TIME - IT'S FUNDAMENTALLY DEVELOPMENTAL, ABOUT THE WAY THAT KNOWLEDGE ACCUMULATES -->

<!-- - IN THE CONTEXT OF CHIlD AND ENVIRONMENT -->

<!-- - BRONFENBRENNER-ESQ -->

## Alternatives


Back to the slobin/competition model argument. 
0-
Are some languages harder to learn than others. At some level yes, Danish is really hard [@bleses]

We're not trying to argue against the data on spanish clitics or agglutinatives in russian. 

On the other hand, the general developmental course is consistent. And degree of variation.

The universal is in the general aspects, not the details. 



To examine whether there is content to the claim of process universals, it is helpful to consider the alternative hypothesis. One alternative is that the process of language acquisition is specific and particular, rather than universal. Two prominent particulars pull against universal tendencies. 

The first is the vast semantic and syntactic variation across the world's languages. For example, as illustrated by @slobin1996 and others, languages vary dramatically in the ways that they assign semantic content to verbs. If the semantic partition of verbs led to large-scale differences in the timeline or mechanism of acquisition, we might see systematic differences in the predictors of age of acquisition for verbs in these languages, yet we do not. Further, languages are more and less morphologically complex; while the most morphologically-complex, polysynthetic languages are not represented in Wordbank, we do have data from both Mandarin (less complex) and Russian (more complex). If morphosyntax were relatively more or less important in the acquisition of particular languages, we might expect radical differences in the noun bias, the grammar-lexicon correlation, or the predictors of age of acquisition across languages, yet we do not observe these. Of course, there is always room for finer-grained predictions -- with more detailed predictive models and better typological coverage, perhaps we will discover such signatures. Our point here is merely that neither morphosyntactic nor semantic variability across languages dominates the process of vocabulary acquisition. 

The second set of language-specific particulars that might lead to variance across languages is the vast cultural variability across the communities represented in the Wordbank data. With N continents represented, we see both "individualist" and "collectivist" cultures [@markus1991,@nisbett] as well as both "loose fit" and "tight fit" cultures [@gelfand2011]. To the extent that parenting differs across these -- and there is good evidence that it does [CITES] -- we should see variance in the trajectory of language learning. For example, as mentioned previously, it would be quite reasonable to predict that the female advantage in vocabulary acquisition might vary as a function of cross-national gender biases [@nosek2009]. Yet it is strikingly consistent overall, again arguing that -- at the broadest level, at least -- cultural factors do not dominate other child-internal processes in the acquisition of vocabulary. 






## Replication and Theory-Building

Changing gears, we can consider the nature of the theory that emerges from the work we do here. One set of concepts that is subsumed in our interest in consistency is that theory be supported by observations that are reproducible, replicable, and robust [munafo2017]. A theory of consistencies is, again, *a fortiori* all of these. If a particular characteristic can be shown again and again across individuals, samples, and languages it is replicable. Indeed, one view of our enterprise is that its impact is fundamentally in the consolidation of knowledge through unifying -- replicating -- previous work. 

Crudely put, we have compiled all of the CDI data that we could, and all of the CDI analyses, and executed the cross of analyses and datasets. This project is thus a cross-linguistic replication study. And so, when we state that some phenomenon is consistent across languages, it is by definition replicated -- but it is additionally robust to a number of different procedural decisions (such as the design or administrtion of the CDI form) that end up varying widely in our data. 

Finally, this work is also fully computationally reproducible -- the analytic conclusions we draw here based on a set of open data and code that can be rerun to create the figures and tables in the manuscript. This characteristic alone does not guarantee their correctness, but their provenance is known. 

## Conclusion

In this chapter, we have sketched a bit of what we see to be the unique theoretical contributions of work with a much larger dataset than is usual in developmental language acquisition research. In a nutshell, the 

