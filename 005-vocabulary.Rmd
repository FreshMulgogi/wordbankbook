# Vocabulary Size {#vocabulary}
 
This chapter begins our substantive analysis of properties of language learning and their variation across languages and children. We begin with one canonical view of CDI data, in which each child is represented by a single vocabulary score: the proportion of words that child knows, out of the total in the form. We first quantify the median pattern of vocabulary growth observed in our data; we then turn to characterizing variability across individuals in these data. In these analyses as well as subsequent chapters, our inspiration comes from what we think of as the "Batesian" approach to variation.


> Far from simply reflecting noise in our measuring instruments or variability in low-level aspects of physiological maturation, the variations that we will document (in vocabulary development) are *substantial,* *stable,* and have their own *developmental course.* Because this variation is substantial, it is critical for defining the boundary between normal and abnormal development; because it is stable, it provides a window onto the correlates and (by inference) the causes of developmental change; and because it has its own developmental course, it can be used to pinpoint critical developmental transitions that form the basis for theories of learning and change. (Bates et al., 1995)

We are interested in these theoretical uses of variability. But variability is only meaningful in the case that it is stable; that is, that it reflects signal about individuals (or cultures) rather than measurement error. With respect to the CDI, the strong evidence for the reliability and validity of the forms -- reviewed in Chapter \@ref(psychometrics) and in @fenson2007 -- provides support for the contention that observed variability is meaningful. Such evidence has primarily been collected for the English CDI, however. In this chapter we examine variability across the full set of languages, and it is worth noting up front that we project the reliability and validity of the English instrument to its adaptations. 

One study nicely illustrates why such an approach might not be misguided. @bornstein2012 collected multiple language measures at 20 and 48 months in a sample of nearly 200 children and used a structural equation model to estimate the stability of a single latent construct, language ability. Essentially all measures related strongly to this latent variable and the coefficient on its stability over time was $r = .84$, suggesting that early language is quite stable, at least when measured appropriately. Notably, the ELI, a precursor to the CDI, was included in the measures at 20 months and was found to correlate with the 20-month latent construct at $r = .87$. 

This finding -- along with the other evidence, mentioned above -- justifies the implicit conceptual model of the following analyses. That model is that there is a single quantity, early language ability, that is stably measured by parent report and that can be approximated as the raw proportion of words a child "understands" or "understands and says" on CDI forms. 

```{r vocab-vocab_admins}
num_words <- items %>%
  filter(type == "word") %>%
  group_by(language, form) %>%
  summarise(n = n())

vocab_data <- admins %>%
  select(data_id, language, form, age, sex, 
         mom_ed, birth_order, production, comprehension) %>% 
  left_join(num_words) %>%
  mutate(no_production = n - production)

min_age <- min(vocab_data$age)
max_age <- max(vocab_data$age)

taus <-  c(0.1, 0.25, 0.5, 0.75, 0.9)
ages <- 8:18
```

## Central Tendencies 

The first question we can ask about CDI data is about its central tendency -- the median pattern of vocabulary growth. Our general expectation is shown in Figure \@ref(fig:vocab-schematic).

```{r vocab-schematic, fig.cap="Schematic true vocabulary growth and vocbaulary growth as measured by the CDI."}
include_graphics("images/vocab_growth_schematic.png")
```

This schematic reveals a number of patterns that are explored in this and subsequent chapters. The CDI necessarily captures a small fraction of any individual's true vocabulary, but even within the measured range there are a number of specific questions that can be addressed by different analyses. The question of the exact slope of children's growth, especially in the period immediately after the emergence of language, is treated in Chapter \@ref(style) -- this question is sometimes posed as whether children undergo a vocabulary "spurt" [@ganger2004]. On the other side of the CDI curve, the question of the divergence between CDI-measured vocabulary and true vocabulary (and whether true vocabulary can be recovered via a statistical correction) is treated in work by @mayor2011.^[Because of the English-specific nature of this work, we do not take up this issue here.] In the current chapter, we focus on the middle section of the CDI curve, in which children's vocabulary is neither at the floor or the ceiling of the instrument. 


### Commonalities across languages

```{r vocab-medians, fig.cap="Median production using Words and Gestures-type forms. Included are only languages where there are more than 200 administrations total."}
lang_ns <- admins %>% 
  group_by(language, form) %>% 
  summarise(n_admins = n())

medians <- vocab_data %>%
  group_by(form, language, age) %>%
  summarise(production = median(production), 
            comprehension = median(comprehension), 
            production_prop = median(production/n), 
            comprehension_prop = median(comprehension/n), 
            n = n()) %>%
  left_join(lang_ns) 

ggplot(filter(medians, form %in% WGs, n_admins > 200), 
       aes(x = age, y = production,
           col = interaction(language, form, sep = .inst_sep))) + 
  geom_point(aes(size = n), alpha = .5) + 
  geom_smooth(se=FALSE, span = 1, size = .5) +  
  xlab("Age (months)") + 
  ylab("Median Production") +
  theme(legend.position = "bottom",
        legend.text=element_text(size=6)) +
  scale_colour_manual(values = inst_colours, name = "") +
  scale_size(guide = FALSE)
```

Figure \@ref(fig:vocab-medians) shows the median patterns of growth for early production. Rather than showing proportions, as we will do more standardly throughout the book, here individual item totals are plotted.^[As @eriksson2012 write, "Using raw data assumes that each form is exhaustive, while using percentages assumes that each form is equally exhaustive. Neither is correct and the truth lies somewhere in between."] In general, the median child before the first birthday is reported to produce a small number of words. (These data raise a number of questions about the specific reliability of very early parent reports, which we take up below.) Overall, however, these curves accord relatively well with our intuitive sense of early vocabulary development: they reveal that most children tend to speak at most only a few words before their first birthday, but that production accelerates across the second year.

This analysis also motivates the decision made in most our large-scale analyses to omit early production from Words and Gestures forms. Many WG forms end at 16--18 months, meaning that the median production is only around 50 words. For analyses of vocabulary composition or predictive modeling, these numbers are often too small to yield reliable and meaningful estimates, although they can be combined with Words and Sentences data (see Appendix \@ref(appendix-stitching)). 

```{r vocab-medians-production-ws, fig.cap="Median production using Words and Sentences-type forms. Included are only languages where there are more than 200 administrations total."}

ggplot(filter(medians, form %in% WSs , n_admins > 200), 
       aes(x = age, y = production,
           col = interaction(language, form, sep = .inst_sep))) + 
  geom_point(aes(size = n), alpha = .5) + 
  geom_smooth(se=FALSE, span = 1, size = .5) +  
  xlab("Age (months)") + 
  ylab("Median Production") +
  geom_hline(yintercept = 50, lty = 2) +
  theme(legend.position = "bottom",
        legend.text=element_text(size=6)) + 
  scale_colour_manual(values = inst_colours, name = "") +
  scale_size(guide = FALSE)
```

The acceleration in early vocabulary is even clearer when looking at production reports from older children using Words and Sentences. Figure \@ref(fig:vocab-medians-production-ws) shows this pattern. In every language, the median child is reported to produce 50 words between 16--20 months (dashed line). As we will see below, this analysis masks the tremendous variability apparent during this period. In addition, as we discuss below, languages vary considerably in the absolute number of words reported. (We focus on Mandarin in particular, since it is a major outlier from other languages in this analysis). Nevertheless, there are still substantial consistencies in the shape and general numerical range across languages. 

During the period 24--30 months, we see children beginning to produce a large enough sample of words that curves are leveling out. Presumably this leveling does not reflect a slowing in the rate of acquisition, which most researchers assume continues unabated for many years [@bloom2001]. Instead, it reflects the limitations of the CDI instrument, in that there are many possible "more advanced" words that they could be learning, of which only a small subset are represented on any form. (See Appendix \@ref{appendix-vocabulary) for estimates of how many words children actually might know.)

```{r vocab-medians-comprehension-wg, fig.cap="Median comprehension using Words and Gestures-type forms. Included are only languages where there are more than 200 administrations total."}
ggplot(filter(medians, form %in% WGs, n_admins > 200), 
       aes(x = age, y = comprehension,
           col = interaction(language, form, sep = .inst_sep))) + 
  geom_point(aes(size = n), alpha = .5) + 
  geom_smooth(se=FALSE, span = 1, size = .5) +  
  theme(legend.position = "bottom") + 
  xlab("Age (months)") + 
  ylab("Median Comprehension") + 
  # geom_hline(yintercept = 50, lty = 2) + 
  theme(legend.position = "bottom",
        legend.text=element_text(size=6)) + 
  scale_colour_manual(values = inst_colours, name = "") +
  scale_size(guide = FALSE)

mins <- medians %>% 
  filter(n_admins > 200) %>%
  filter(form %in% WGs, age == 8) 
```

We next turn to comprehension medians, shown in Figure \@ref(fig:vocab-medians-comprehension-wg). Comprehension is only queried on the Words and Gestures form. Reported comprehension increases much faster than production; so much so that most parents are reporting that their children understand most words on the form by 18 months (Chapter \@ref(style) discusses differences in the balance between comprehension and production between children). As with the production data, we see substantial differences across languages in reported vocabulary, discussed below (we also examine the odd data for Taiwanese Mandarin in more depth). 

Outside of that particular dataset, one striking aspect of the comprehension data is how early comprehension is reported.  For example, from 8 months, we see parents reporting medians of `r min(mins$comprehension)` (`r mins$language[mins$comprehension == min(mins$comprehension)]`) and `r max(mins$comprehension)` (`r mins$language[mins$comprehension == max(mins$comprehension)]`) words. To many researchers (and some parents) these high numbers feel unlikely. We are largely agnostic on this issue, but the literature does provide some support for early comprehension reports. A spate of recent infancy experiments suggest that in fact, children in the second half of the first year do have some fragmentary representations of many common words available [e.g., @tincoff1999;@tincoff2012;@bergelson2012;@bergelson2017]. The representations revealed in these tasks are quite weak -- often amounting to a 2-5% difference in looking to a target on hearing a word uttered -- but, depending on the criterion used by parents, may be what is detected in these early reports. Thus, these estimates may not be as far off as we initially suppose.^[An alternative possibility is that *both* accounts are true, but unconnected: 8-month-olds could in fact know some common words, but parents could be overestimating their vocabulary based on observed behaviors -- in essence, parents could be right, but for the wrong reasons.]

### Difficult datasets

In the analyses above, our comparisons revealed two cases where datasets showed large disparities: Mandarin (Beijing) Words and Sentences production and Mandarin (Taiwain) Words and Sentences comprehension. 


```{r vocab-ords-mandarin, fig.cap="Median production vocabulary for 24-month-olds, with total item scores shown in the left panel and proportions on the right. Scores are sorted by total item score. To increase stability, the plotted value is the intercept of a linear model predicting vocabulary as a function of centered age between 18 and 30 months."}
ordering_raw <- filter(medians, 
                       form %in% WSs, 
                       n_admins > 200, 
                       age >= 18, 
                       age <= 30,
                       !(language == "Mandarin" & form == "WS"),
                       !(language == "Hebrew")) %>%
  do(broom::tidy(lm(production ~ scale(age, center = TRUE, scale=FALSE), 
                    data = .))) %>%
  filter(term == "(Intercept)") %>% 
  mutate(measure = "raw")

ordering_props <- filter(medians, form %in% WSs, 
                         n_admins > 200, 
                         age >= 18, 
                         age <= 30) %>%
  do(broom::tidy(lm(production_prop ~ scale(age, center = TRUE, scale=FALSE), 
                    data = .))) %>%
  filter(term == "(Intercept)") %>%
  mutate(measure = "proportion")

ords <- bind_rows(ordering_raw, ordering_props) %>%
  ungroup %>%
  mutate(language = fct_reorder(language, estimate), 
         measure = factor(measure, levels = c("raw","proportion")))

ggplot(ords, aes(x = language, y = estimate)) + 
  geom_pointrange(aes(ymin = estimate - std.error, ymax = estimate + std.error)) + 
  facet_wrap(~measure, scales = "free_x") +
  coord_flip() + 
  ylab("Median 24-month vocabulary estimate") + 
  xlab("")
```

Mandarin Words & Sentences data are reported by @tardif2009 in a study of both Mandarin- and Cantonese-learning children. The data reported there also show the pronounced Mandarin advantage plotted above, which are shown for 24-month olds specifically in Figure \@ref(fig:vocab-ords-mandarin). (This figure also reveals the high level of vocabulary reported for Hebrew speakers, which goes unnoticed in the figure above because of the relatively smaller number of items on the Hebrew form). 

To investigate the Mandarin disparities further, @tardif2009 discussed a number of possible explanations, given that the administration and sampling procedures were similar in these two languages. The children in the Mandarin sample are nearly all monolingual, only (first born) children; but these factors did not account for variation between samples. @tardif2009 therefore, speculate that structural factors regarding Mandarin (e.g., phonological structure relative to Cantonese) might be accounting for the Mandarin advantage.

These speculations seem unlikely in light of the data presented here. First, the same trajectory is not shown in the data from the analogous questionnaire of @hao2008. Second, this unusual trajectory is not apparent in the production data from the Mandarin Beijing WG data shown above. Finally, given the surprising difference between Mandarin and all other languages in the sample, pure phonological factors seem unlikely to account fully for the differences. These differences thus remain somewhat mysterious; perhaps some quirk of administration instructions led to relative over-reporting, or perhaps the populations being sampled truly were different. Alongside the Hebrew data, these data serve as an important caution against simple cross-linguistic comparison in raw scores or even percentiles. 

```{r vocab-taiwan-comprehension, fig.cap="Comprehension data from Taiwanese Mandarin."}
taiwan_comp_data <- vocab_data %>%
  filter(form == "WG", language == "Mandarin (Taiwanese)") %>%
  mutate(mean = comprehension)

ggplot(taiwan_comp_data,
       aes(x = age, y = mean)) +
  geom_jitter(width = .4, size = 1, alpha = .1) +
  geom_smooth(aes(col = interaction(language, form, sep = .inst_sep)))+ 
  scale_x_continuous(breaks = seq(8, 16, 2),
                     limits = c(8, 16),
                     name = "Age (months)") +
  scale_colour_manual(values = inst_colours, name = "") +
  ylab("Production (Number of Words)") +
  ylim(c(0, 380)) + 
  theme(legend.position = "bottom")
```

Turning to our second case study, Mandarin (Taiwanese) comprehension scores, we see that they are relatively flat and show very high medians very early in development. Deeper inspection of the full distributional pattern (Figure \@ref(fig:vocab-taiwan-comprehension)) suggests that there is relatively little developmental change in comprehension scores on this dataset. In contrast, production, seen above, appears to follow a more typical pattern. In our experience, this pattern results from parents who do not understand what is being asked on the comprehension section of a form; sometimes they report whether they think a child has heard a particular word, or whether they respond to language in more general ways. We have observed a population of "over-responders" of this sort in a number of self-report contexts -- often they are parents of very young children who appear loathe to return a form having checked essentially no items at all. But such an explanation is only speculation.

The datasets discussed in this section deviate from the normal pattern of CDI forms in a number of ways. While we have offered some tentative explanations, these are necessarily post hoc and rely on our assumption that they *should* be relatively similar to other datasets from other cultures and with other forms. Thus, in our further analyses we choose not to omit these data but instead consider them as a caution on making strong inferences from variability rather than from consistency. As we discussed in Chapter \@ref(intro-theory), variability may be caused by a wide variety of sources; consistency is somewhat more surprising. 

### Cross-language differences

While we have reason to believe that there are some outlier languages in our data, there is still other observed variation that does not seem unusual. What are the sources of this variation? In these anlayses, we examine production on the Words and Sentences form, as the data are the densest and most reliable for this instrument. Figure \@ref(fig:vocab-ords-mandarin)  shows estimates of the median 24-month vocabulary in both raw scores and proportions. Clearly there are substantial differences in raw scores, even leaving aside Mandarin and Hebrew. We consider a range of explanations for this pattern. 

```{r vocab-nums, fig.cap}
nums <- left_join(ordering_raw, num_words)
num_cor <- cor.test(nums$estimate, nums$n)

props <- left_join(ordering_props, num_words)
prop_cor <- cor.test(props$estimate, props$n)

num_prop <- ords %>%
  select(estimate, measure, language, form) %>%
  spread(measure, estimate) 

num_prop_cor <- cor.test(num_prop$raw, num_prop$proportion)
```

Differences could be due to differences in form length. As shown in the plot above, however, medians for production and raw scores are highly correlated ($r$(`r num_prop_cor$parameter`) = `r signif(num_prop_cor$estimate, 3)`, $p$ = `r signif(num_prop_cor$p.value, 3)`), suggesting that this ordering is not only a function of form length. Further, although raw scores are correlated with form length ($r$(`r num_cor$parameter`) = `r signif(num_cor$estimate, 3)`, $p$ = `r signif(num_cor$p.value, 3)`), this correlation changes direction and is no longer reliable for proportions ($r$(`r prop_cor$parameter`) = `r signif(prop_cor$estimate, 3)`, $p$ = `r signif(prop_cor$p.value, 3)`). In sum, it appears that there are form-length differences (motivating the use of proportions in general), but that there is still stratification between languages even correcting for this issue. Figure \@ref(fig:vocab-medians-production-ws-prop) shows the relevant proportion trajectories, highlighting remaining differences between English and Danish (two languages for which we have substantial datasets with full demographic information).


```{r vocab-medians-production-ws-prop, fig.cap="Cross-linguistic production data, proportions plotted by age. English (American) and Danish are highlighted."}
highlight_langs <- c("Danish", "English (American)")

highlight_data <- filter(medians, form %in% WSs, 
                     n_admins > 200, 
                     age >=18, 
                     age <=30,
                     !(language == "Mandarin" & form == "WS"),
                   !(language == "Hebrew"))  %>%
         mutate(highlight = ifelse(language %in% highlight_langs, 1, .2))

ggplot(highlight_data, 
       aes(x = age, y = production_prop, col = language)) + 
  geom_point(aes(size = n, alpha = highlight)) +
  geom_smooth(se=FALSE, span = 1, 
              data = filter(highlight_data, language %in% highlight_langs)) +
  theme(legend.position = "bottom") + 
  scale_alpha_continuous(guide=FALSE) + 
  xlab("Age (months)") + 
  ylab("Production Proportion") + 
  theme(legend.position = "bottom",
        legend.text=element_text(size=6)) + 
  scale_colour_manual(values = lang_colours, name = "") +
  scale_size(guide = FALSE) +
  ylim(0,1) + 
  xlim(18,30)
```

Differences could also be due to form construction. For example, the Czech form could contain harder words, leading to fewer words being checked. We cannot directly address questions about the difficulty distribution items without moving to psychometric models  (see Chapter \@ref(psychometrics)). These models in turn would need to be equated across forms in order to compare latent ability scores across instruments. While we have experimented with these procedures, there is a circularity to these procedures that makes us leery of proceeding. In particular, in order to equate across tests in standard item response theory models, it is critical to have test items that are share across instruments. But although we have *concepts* that are shared across instruments (see Chapter \@ref(items-prediction)), we do not believe the words that represent these concpets are equally difficult across languages -- in fact, the premise of our later analyses is that they are not. Thus, assessing form difficulty across languages is a complex proposition that we do not address directly here.

```{r vocab-select-medians-DE, fig.cap="Proportion production plotted by age for Danish and English samples, now subsetting to the first-born female children of college-aged mothers."}
select_medians <- vocab_data %>%
  filter(form %in% WSs, language %in% highlight_langs, 
         mom_ed == "College", sex == "Female", birth_order == "First") %>%
  group_by(language, age) %>%
  summarise(production_prop = median(production/n), 
            comprehension_prop = median(comprehension/n), 
            n = n()) 

ggplot(select_medians, 
       aes(x = age, y = production_prop, col = language)) + 
  geom_point(aes(size = n)) + 
  geom_smooth(se=FALSE, span = 1) +
  theme(legend.position = "bottom") + 
  scale_alpha_continuous(guide=FALSE) + 
  scale_colour_manual(values = lang_colours, name = "") +
  xlab("Age (months)") + 
  ylab("Median Production Proportion") + 
  ylim(0,1) + 
  xlim(18,30)
```


Differences could also be due to demographic differences across samples. We can examine sample composition in Chapter \@ref(methods) and see that -- to the extent we have access to demographic data -- sample composition does vary in features that affect vocabulary (e.g., maternal education, birth order; see Chapter \@ref(demographics) for fuller analysis). We are not yet in a position to conduct a full analysis of these differences, controlling for demographics, as data are sparse and demographic differences *also* vary across cultures. But we can examine the difference between Danish, and English (American) for example, and note that these differences look quite similar (though noisier) in the female, first-born children of college-educated mothers (Figure \@ref(fig:vocab-select-medians-DE)). Thus, we do not believe that demographic differences fully explain the cross-linguistic differences observed. 

Differences could be due to cohort effects, in which older sets of data show differences from newer datasets. Most of our data date from the period 2005-2015, but some of our English and Spanish data are older, as they date to the period in which CDI forms were first being designed [the early 1990s; @fenson1994]. Unfortunately we do not have reliable information about the collection data for many datasets, so we cannot do regression analyses straightforwardly. Naively, we would expect later cohorts to show higher vocabularies, consistent with the Flynn effect [@flynn1987]. Yet, the Danish data, for example are relatively recent and were collected online using standardized instructions. Danish is subject to its own issues, howver, but the Norwegian data are also relatively recent and are quite comparable to the English data. 

Differences may relate not to demographics of the sample but to the procedure at administration. These differences are not transparent to us in all cases, and so, similar to cohort effects, we cannot control for statistically. For example, instructions at administration -- whether written on the form or given by experimenters -- might have been more liberal in the case of Slovak or English (American) samples. Such instructions could have emphasized completeness in reporting vocabulary. Or the circumstances of administration could have been different -- for example, Danish data were collected online while most English data were collected using paper and pencil forms. English (American) data are contributed by many different labs, so there are likely many different administration styles represented.

Differences could be due to cultural or experimental differences in reporting bias. Slovak parents might have a lower criterion for reporting knowledge of a word. Recalling our discussion of these issues in Chapter \@ref(intro-practical), their model of children's overall competence might be shifted up. (Such an explanation could be true in principle for the case of the Mandarin and Hebrew data discussed above, as well, though this would be a case of extreme differences!) This explanation is as an extension of the discussion above of administration and instructions -- perhaps cultural expectations for what it means to be producing a word or cultural expectations for how verbal children are expected to be. 

Finally, differences could be due to true differences in language acquisition. While this explanation is a possiblity, we hope we have emphasized that it is only one among the many enumerated above. Many researchers working on Danish believe that, due to its phonological properties, it is truly a difficult language to learn [@bleses2008;@bleses2011]. In particular, Danish is characterized by some highly distinctive phonological reduction processes which greatly reduce the frequency of obstruents, and more generally lead to "an indistinct syllable structure which in turn results in blurred vowel-consonant, syllable and word boundaries [where]... word endings are often indistinctly pronounced" (Bleses et al., 2008, p. 623).  The authors of this study also able to provide evidence against the alternative view that Danish parents are simply more reluctant to respond "yes’"– there were no differences on either gestures or word production.  They concluded that the phonological structure of Danish produces an initial obstacle to breaking into the stream of speech and is reflected in overall patterns of vocabulary development. While this generalization may in fact be true, it does not answer the question of why children learning other languages are equally slow by both raw and percentile metrics. 

In summary, differences between languages in the sheer number of words reported are unlikely to be accounted for purely by differences in form size or demographic differences between samples. In our very speculative view, they likely result from a combination of cultural attitudes towards children's language, differences in administration instructions, and real differences in learning across languages. Partialling out these differences would likely require better-controlled data that included constant administration and sampling methods. For this reason, in the remainder of our analyses, we attempt to avoid interpreting overall differences in vocabulary size wherever possible and limit ourselves to quantities that can be effectively normalized. 


## Variability between individuals

```{r vocab-production_ws}
ws_prod_data <- vocab_data %>%
  filter(form %in% WSs) %>%
  mutate(mean = production / n)

ws_prod_models <- ws_prod_data %>%
  split(.$language) %>%
  map(fit_gcrq) 

ws_prod_preds <- ws_prod_data %>%
  group_by(language, age) %>%
  summarise(n = n()) %>%
  data.frame() %>%
  split(.$language) %>%
  map_df(function(x) pred_gcrq(x, ws_prod_models)) 

# ggplot(ws_prod_data,
#        aes(x = age, y = mean)) +
#   facet_wrap(~language) +
#   geom_jitter(width = .4, size = 1, alpha = .02) +
#   geom_line(data = ws_prod_preds, 
#             aes(y = pred, col = percentile, group = percentile)) + 
#   .scale_colour_discrete(name="Percentile") +
#   scale_x_continuous(name = "Age (months)", 
#                      breaks = seq(16, 36, 4),
#                      limits = c(16, 36)) +
#   scale_y_continuous(breaks = c(0, .5, 1), lim = c(0, 1), 
#                      name = "Production Vocabulary") +
#   theme(legend.position = "bottom", 
#         strip.text.x = element_text(size = 8))
```

We next turn from the question of central tendencies in early vocabulary to the question of variability. One of the most important features of early vocabulary development is its variability [@fenson1994]. To examine variability, we switch to a view of the data that reveals the full range of variation across the samples in the database. Examining Words and Sentences data, we can see that across every language in the database there is a tremendous range of vocabulary sizes reported. How systematic is this variability? That is the question addressed by our next set of analyses. 


### Quantifying Variability

```{r vocab-eng-prod, fig.cap="Raw production scores for English (American) production data. Dots show individual participants, while lines give standardized percentiles, computed via spline-based quantile-regression."}
eng_prod_data <- vocab_data %>%
  filter(form == "WS", language == "English (American)") %>%
  mutate(mean = production)

eng_prod_model <- list()
eng_prod_model$`English (American)` <- eng_prod_data %>%
  fit_gcrq

eng_prod_preds <- eng_prod_data %>%
  group_by(age, language) %>%
  summarise(n = n()) %>%
  data.frame() %>% 
  pred_gcrq(eng_prod_model)

ggplot(eng_prod_data,
       aes(x = age, y = mean)) +
  geom_jitter(width = .4, size = 1, alpha = .1) +
  geom_line(data = eng_prod_preds, 
            aes(y = pred, col = percentile, group = percentile)) + 
  .scale_colour_discrete(name="Percentile") +
  scale_x_continuous(breaks = seq(16, 30, 4),
                     limits = c(16, 30),
                     name = "Age (months)") +
  ylab("Production (Number of Words)") +
  ylim(c(0, 680)) + 
  theme(legend.position = "bottom")
```

As an example, we zoom in on the English (American) production data from the Words and Sentences form. The canonical view of these data is given in Figure \@ref(fig:vocab-eng-prod). It is very clear that variability is the norm! Children are all over the map at almost all age groups. 

```{r vocab-histogram, fig.cap="Histogram of English (American) production values for 24-month-olds."}
ggplot(filter(eng_prod_data, age == 24), 
       aes(x = production)) + 
  geom_histogram(binwidth=30) + 
  # geom_density(col = "red", aes(y = ..density..)) +
  xlim(0,680) + 
  xlab("Number of words produced")
```

Next consider just a single age group, 24-month-olds. A histogram of two-year-old production vocabulary is show in Figure \@ref(fig:vocab-histogram). The distribution of vocabularies across children is far from normal, with many children at the very bottom of the scale and almost as many at the top. Quite a few two-year-olds on their second birthday are producing only a handful of words (or at least their parents say they are) and others are producing nearly all of the 680 listed on the form (as well as others, in all likelihood). (It will quickly get tiresome to acknowledge ceiling effects and parent report biases in every analyses, so we acknowledge them up front and then mention them only when relevant throughout.) 

One way to describe these data is to consider the relationship of the variance to the central tendency. The "coefficient of variation" (CV) is a common measure used for this purpose: 

$$CV = \frac{\sigma}{\mu}$$

\noindent This statistic allows standardized comparison of variability across measurements with different scales, an important concern when we want to compare forms with very different numbers of vocabulary items. For example, for two-year-olds, the mean productive vocabulary is `r round(mean(eng_prod_data$production[eng_prod_data$age == 24]))` words, and the standard deviation is `r round(sd(eng_prod_data$production[eng_prod_data$age == 24]))`, words, leading to a CV of `r signif(sd(eng_prod_data$production[eng_prod_data$age == 24])/mean(eng_prod_data$production[eng_prod_data$age == 24]), digits = 2)`. 

But, again as seen in Figure \@ref(fig:vocab-histogram), the distribution of productive vocabulary scores is far from normal. And distributional forms deviate even more from the standard normal distribution at younger and older ages. Thus, a non-parametric approach is more appropriate. Accordingly, we compute the MADM statistic, the non-parametric equivalent of the CV. In MADM, the mean $\mu$ is replaced by the median ($m(x)$, and the standard deviation $\sigma$ is replaced by the mean absolute deviation (which captures how far away values are from the median):

$$MADM(x) = \frac{\frac{1}{n} \sum_{i = 1..n}{|x_i - m(x)|}}{m(x)}$$
Appendix \@ref(appendix-variability) demonstrates that, although MADM is more appropriate for our data, CV and MADM are very highly correlated with one another. 

```{r vocab-madm1, fig.cap="MADM values plotted by age for English (American) production data, across forms. The smoothing line is produced by a loess smoothing function."}
madm_data <- vocab_data %>%
  filter(form %in% c(WSs, WGs)) %>%
  group_by(language, form, age) %>%
  summarise(madm_p = madm(production), 
            madm_c = madm(comprehension),
            n = n()) %>%
  filter(n > 20) 

ggplot(filter(madm_data, language == "English (American)"), 
       aes(x = age, y = madm_p, col = form)) + 
  geom_point() + 
  geom_smooth(aes(group = 1)) + 
  ylim(0,2) +
  xlim(8,30) + 
  ylab("MADM (MAD / median)") + 
  xlab("Age (months)") + 
  .scale_colour_discrete(name = "Form")
```

Figure \@ref(fig:vocab-madm1) shows the MADM value for American English production data, plotted by age. In these data, the MADM is actually close to 1 from age one until almost age two, suggesting that the standard *difference* from the median is actually as big as the median itself! 

To get a sense of this variability, it can help to have a smaller dataset to consider. Imagine groups of three children. A group where one produced 30 words, one produced 100, and another produced 170 would have a MADM of `r round(madm(c(33,100,180)), digits = 2)`. In contrast, one where they were more closely grouped -- say 70, 100, 130 -- would have a MADM of `r round(madm(c(70,100,130)), digits = 2)`.

```{r vocab-madm2, fig.cap="MADM for production, plotted by age group, for the full sample of languages in our dataset."}

ggplot(madm_data, aes(x = age, y = madm_p)) + 
  geom_jitter(aes(size = n, col = language), alpha = .5) + 
  geom_smooth(aes(group = 1)) + 
  ylim(0,2) + 
  ylab("MADM (Production)") + 
  xlab("Age (months)") + 
  theme(legend.position = "bottom",
        legend.text=element_text(size=6)) + 
  scale_colour_manual(values = lang_colours, name = "") +
  scale_size(guide = FALSE)
```

Does the general level of variability observed in American English hold for other languages? Figure \@ref(fig:vocab-madm2) shows MADM, now plotted for production across languages and instruments. This similarity in variability structure is quite striking, such that between the first and second birthdays, children's early language is remarkably variable. Yet this variability itself is quite *consistent*.

```{r vocab-madm-summary, fig.cap="MADM values from 12-24 months for all languages and forms."}
madm_summary_prod <- madm_data %>%
  filter(!(language == "Kigiriama") & 
             !(language == "French (French)" & form == "WG")) %>% # missing data
  group_by(language, form) %>%
  filter(age >= 12, age <= 24) %>%
  summarise(madm_mean = mean(madm_p), 
            madm_ci_lower = mean(madm_p) - ci.t(madm_p),
            madm_ci_upper = mean(madm_p) + ci.t(madm_p)) 

ggplot(madm_summary_prod, aes(x = language, y = madm_mean, col = form)) + 
  geom_pointrange(aes(ymin = madm_ci_lower, ymax = madm_ci_upper), 
                  position = position_dodge(width = .2)) + 
  ylim(0,1.5) + 
  coord_flip() + 
  ylab("Language") + 
  xlab("MADM (Production 12-24 mo)") +
  # geom_hline(aes(yintercept = 0), lty = 2, color = "gray") + 
  # geom_hline(aes(yintercept = 1), lty = 2, color = "gray") + 
  .scale_colour_discrete()
```

We can summarize the MADM in the second year of life by taking its mean. This summary is shown in Figure \@ref(fig:vocab-madm-summary). This mean is close to 1 for almost every language and form for which we have data. Confirming the analysis above, we see cross-linguistic consistency in the variability of children.

One question that could be raised regarding the analysis above is the extent to which variability is caused by *variability across children* vs. *variability in reporting*. The extreme values seen in the English data, for example, could easily be the result of a mixture of lazy parents who stopped answering the form with overly diligent parents who misunderstood and checked every box for a word they thought the child had been exposed to. But to the extent these biases are the source of variability, they are extremely *consistent* across languages -- which, recall, is the exact opposite argument from the one we considered above (where parent diligence was supposed to be variable enough across samples to lead to differences between languages). Although there are certainly some reporting biases represented in the data, we do not believe that the particular results of this analysis are an artifact of reporting bias. 

```{r vocab-madm-comprehension, fig.cap="MADM for comprehension, plotted by age group, for the full sample of languages in our dataset."}
ggplot(filter(madm_data, form %in% WGs), 
       aes(x = age, y = madm_c)) + 
  geom_jitter(aes(size = n, col = language), alpha = .5) + 
  geom_smooth(aes(group  = 1)) + 
  ylim(0,2) + 
  ylab("MADM (Comprehension)") + 
  xlab("Age (months)") + 
  theme(legend.position = "bottom",
        legend.text=element_text(size=6)) + 
  scale_color_manual(values = lang_colours, name = "") + 
  scale_size(guide = FALSE)
```

We can complete the same analysis using comprehension data, shown in Figure \@ref(fig:vocab-madm-comprehension). In this measure, we see a gradual decrease in variability throughout development. The intercept for the 12-18 month period appears to be lower than that observed in production, despite (or because of?) the higher scores. This observation matches one made by @mayor2014, namely that production vocabulary appears more idiosyncratic in distribution of words than comprehension vocabulary. One speculative explanation for this difference would be the tremendous differences in speech-motor development (as well as general differences in loquacity) between toddlers. This variability would then carry over into production. Another possibility, however, is that true variability is masked by the overall lower reliability of comprehension items (see Chapter \@ref(psychometrics)). Our data do not allow us to distinguish between these two explanations.


```{r vocab-madm-summary-comprehension, fig.cap="MADM values from 12-18 months for all languages and forms."}
madm_summary_comp <- madm_data %>%
  filter(form %in% WGs, language != "Kigiriama", age < 19, age > 11) %>% #missing
  group_by(language, form) %>%
  summarise(madm_mean = mean(madm_c), 
            madm_ci_lower = mean(madm_c) - ci.t(madm_c),
            madm_ci_upper = mean(madm_c) + ci.t(madm_c)) 

ggplot(madm_summary_comp, aes(x = language, y = madm_mean)) + 
  geom_pointrange(aes(ymin = madm_ci_lower, ymax = madm_ci_upper)) + 
  ylim(0,1.5) + 
  coord_flip() + 
  ylab("Language") + 
  xlab("MADM (Comprehension)") 
  # geom_hline(aes(yintercept = 0), lty = 2, color = "gray") 
  # geom_hline(aes(yintercept = 1), lty = 2, color = "gray")
```

The final plot in this sequence is shown in Figure \@ref(fig:vocab-madm-summary-comprehension), which shows 12-18-month MADM values. These are slightly lower and slightly more variable than the production values shown above, but still display a quite consistent level of variability. 

### Is there a ceiling to variability?

The analysis above suggests that variability between individuals decreases. But this  conclusion is compromised by ceiling effects: once children begin to reach the ceiling of the CDI form, variability is necessarily truncated. No analysis can completely eliminate these effects, but the use of item response theory-based analyses can partially address the issue by estimating variation in latent ability rather than variation in raw scores themselves. The logic   

Chapter \@ref(psychometrics) provides a summary of our approach to using item response theory (IRT) with CDI data. In brief, IRT provides a framework in which the full test (the CDI) is broken down into a series of items, with each having its own logistic model predicting the response for a particular child on the basis of their latent ability. In Chapter \@ref(psychometrics), we examine the parameters of individual items with respect to their properties; but fitting an IRT model also implies estimating a set of latent ability parameters for individual participants. These latent ability parameters are logistic regression coefficients and hence are not bounded in the same way that individual responses (and hence raw scores) are. Thus, we can examine their variability as a way of dealing with ceiling effects.

```{r vocab-mirt, eval=FALSE}
mirt_params <- function(dx) {
  language <- dx$language
  form <- dx$form
  print(paste(language, form, sep = .inst_sep))
  
  lang_data <- get_instrument_data(language = language, form = form, 
                                   administrations = TRUE, iteminfo = TRUE)
  
  d <- lang_data %>%
    filter(!is.na(lexical_class)) %>%
    mutate(produces = value == "produces") %>%
    select(data_id, item_id, produces, age, production) 
  
  d_wide <- d %>%
    mutate(produces = as.numeric(produces)) %>%
    select(item_id, produces, data_id) %>%
    spread(item_id, produces)
  
  d_mat <- d_wide %>%
    select(-data_id) %>% 
    data.frame %>%
    data.matrix
  
  colnames(d_mat) <- sort(unique(d$item_id))
  rownames(d_mat) <- d_wide$data_id
  
  d_mat <- d_mat[complete.cases(d_mat),]
  mod_4pl <- mirt::mirt(d_mat, 1, itemtype='4PL', verbose=TRUE)

  fscores <- data_frame(data_id = rownames(d_mat), 
                        ability = fscores(mod_4pl, method = "MAP")[,1])
  fscores$language <- language
  fscores$form <- form
  
  fscores <- fscores %>%
    mutate(data_id = as.numeric(data_id)) %>%
    left_join(get_administration_data(language = language, 
                                      form = form))
  
  return(fscores)
}
  
mirt_params <- instruments %>% 
  filter(form %in% WSs) %>% 
  mutate(idx = 1:n()) %>%
  split(.$idx) %>%
  map_df(possibly(mirt_params, otherwise = data_frame()))

write_feather(mirt_params, "data/vocabulary/mirt_params.feather")
```


```{r vocab-variability_IRT_slopes, fig.cap="Standard deviation of latent ability scores from IRT models fit to each Words and Sentences-type dataset. Panels show individual languages. Smoothing lines are linear model fits."}
mirt_params <- read_feather("data/vocabulary/mirt_params.feather")

mirt_sd <- mirt_params %>%
  left_join(mirt_params %>% group_by(language, form) %>%
              summarise(langform_sd = sd(ability))) %>%
  mutate(langform = interaction(language, form, sep = .inst_sep)) %>%
  group_by(langform, age) %>%
  summarise(mean = mean(ability), 
            sd = sd(ability), 
            sd_normed = sd(ability) / mean(langform_sd),
            n = n()) %>%
  filter(n > 20) 
  
ggplot(mirt_sd, 
       aes(x = age, y = sd, col = langform)) + 
  facet_wrap(~langform) +
  # geom_line() + 
  # geom_smooth(se=FALSE, span = 1) +
  geom_point() +
  geom_smooth(method = "lm", col = "black", lty=2 ) +
  # geom_label_repel(data = mirt_sd %>%
  #                    group_by(langform) %>%
  #                    filter(age == max(age)),
  #                  aes(label = langform), size = 3, segment.color = "grey") +
  # # geom_dl(aes(label = langform, col = langform),
  #         method = "last.points", ) + 
  scale_colour_manual(values = inst_colours, guide = FALSE) +
  ylab("Latent Ability SD") + xlab("Age (Months)") +
  xlim(12,40)
```

Figure \@ref(fig:vocab-variability_IRT_slopes) shows the normalized standard deviation of latent ability scores, plotted by age. The absolute size of the standard deviations is not easy to interpret -- the range of latent ability scores on a form is a function of how consistently difficult or easy the items are (and as we noted above, we do not think it is trivial to equate these scores across tests) Thus, the standard deviation of these scores will be larger or smaller based on this feature as well as the consistency of children's abilities within an age group and is not interpretable. On the other hand, age-related trends in the standard deviation are interpretable and can be used as an index of whether variability in ability stays constant, increases, or decreases. 

Interestingly, slopes for the variability of latent ability are fairly flat or increasing (with Mandarin, which has extreme ceiling effects, being the exception). This finding suggests that the decreases in variability observed above are very likely due to ceiling effects. In sum, when we remove ceiling effects, we find that variability is constant -- or perhaps even increasing -- throughout the full measured range of early language.

### Discussion 

We observed a striking consistency in the individual variability of children's vocabulary during their second year and perhaps beyond. Across languages and forms, it appears to be the norm that toddlers vary. 

What does it mean to have such a high level of variability? For one comparison, we compare age of walking onset (as measured by a Norwegian national survey with parent 47,515 respondents) and age of achieving production and comprehension milestones (also in Norwegian). Walking data are from @storvold2013. 

```{r vocab-walking_comparison}
p100 <- vocab_data %>%
  filter(language == "Norwegian") %>%
  group_by(age, form) %>%
  summarise(`produces 200 words` = mean(production > 200),
            `produces 100 words` = mean(production > 100), 
            `produces 50 words` = mean(production > 50), 
            `produces 10 words` = mean(production > 10),
            `understands 50 words` = mean(comprehension > 50)) %>%
  gather(response, prob, contains("words")) %>%
  bind_rows(data_frame(age = c(12,13,14), 
                       prob = c(.25, .50, .75), 
                       response= "walking", 
                       form = "MoBa"))
  
# ggplot(p100, aes(x = age, y = prob, col = response)) +
#   geom_line(aes(group = interaction(response, form))) + 
#   geom_line(data = filter(p100, response == "walking"), 
#                           aes(group = interaction(response, form)), size = 2) + 
#   ylim(0,1) + 
#   .scale_colour_discrete() + 
#   theme(legend.position = "bottom") + 
#   ylab("Probability") + xlab("Age (months)")
```

```{r vocab-walking-comparison-2, fig.cap="Comparison of language milestones with walking, in terms of month at which a percentile ranking is achieved."}
p100 %>%
  group_by(response) %>%
  summarise(`25th percentile` = min(age[prob >= .25]),
            `75th percentile` = min(age[prob >= .75]), 
            `Range (mo)` = `75th percentile` - `25th percentile`, 
            `Range (prop.)` = `Range (mo)` / min(age[prob >=.5])) %>%
  arrange(`Range (mo)`) %>%
  rename(Response = response) %>%
  kable(digits = 2) 
  
```

Table \@ref(fig:vocab-walking-comparison-2) shows the 25th and 75th percentiles for a variety of behaviors. The spread of achieving walking (defined as taking a step independently) is quite tight with a mean of 12.9 months and a spread of only a month between 25th and 75th percentile. Very early language comprehension and production are relatively similar with 2 and 3 month spreads. In contrast, production and comprehension at a higher level has quite a large spread in comparison to walking (even as a percentage of age). 

In sum, these results echo the conclusions of @bornstein2005 based their comparative study of Spanish, English, and Italian. They noted that “individual variability is probably a universal feature of early language acquisition” (p. 311). 


```{r vocab-save_covs}
prod <- madm_summary_prod %>%
  ungroup %>%
  summarise(prod_madm =  cv(madm_mean), 
            prod_madm_sem = cv_sem(madm_mean), 
            n = n())

comp <- madm_summary_comp %>%
  ungroup %>%
  summarise(comp_madm =  cv(madm_mean), 
            comp_madm_sem = cv_sem(madm_mean),
            n = n())

cvs <- data_frame(signature = c("MADM 12-24mo", 
                       "MADM 8-18mo"), 
                  measure = c("production","comprehension"), 
                  cv = c(prod$prod_madm, comp$comp_madm), 
                  sem = c(prod$prod_madm_sem, comp$comp_madm_sem),
                  n = c(prod$n, comp$n), 
                  category = "Vocabulary")

write_feather(cvs, "data/cvs/madm_cvs.feather")
```



<!-- We note that @feldman2000 discusses the possibility that some portion of the observed variability in her data as well as the CDI norming data [@fenson1994] is due to reporting issues. In their response, @fenson2000 cite an unpublished dataset comparing lab-based elicited production and parent report for 36 object names and show that the standard deviation is greater than the mean on both measures, suggesting that the variability found in CDI scores is not artifactual. -->
