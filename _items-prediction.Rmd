```{r aoapred-setup}
data_path <- "data/items-prediction"
data_files <- list.files(data_path, pattern = "*.RData", full.names = TRUE,
                         recursive = TRUE)
for (file in data_files) load(file)

num_langs <- n_distinct(lang_coefs$language)
predictors <- unique(lang_coefs$term)
num_coefs <- length(predictors)

langs_aoa <- unique(lang_coefs$language) %>% str_replace("Quebec", "Quebecois")
# hack for aoapred having cached data with old lang label

lang_colours_aoa <- lang_colours[names(lang_colours) %in% langs_aoa] %>%
  set_names(str_remove(names(.), " \\(.*\\)"))

label_caps <- function(value) {
  if_else(toupper(value) == value, value,
          paste0(toupper(substr(value, 1, 1)),
                 tolower(substr(value, 2, nchar(value))))) %>%
    str_replace_all("_", " ")
}

display_predictors <- function(predictors) {
  predictors %>%
    str_replace("num", "number") %>% str_replace("phons", "phonemes") %>%
    map_chr(label_caps)
}
```

```{r aoapred-coef_means}
mean_term_coefs <- lang_coefs %>%
  filter(effect == "main effect") %>%
  group_by(term) %>%
  summarise(mean_estimate = mean(estimate),
            n_sig = sum(signif),
            n_pos = sum(estimate > 0),
            n_neg = sum(estimate < 0)) %>%
  arrange(desc(abs(mean_estimate)))

mean_term_measure_coefs <- lang_coefs %>%
  filter(effect == "main effect") %>%
  group_by(measure, term) %>%
  summarise(mean_estimate = mean(estimate),
            n_sig = sum(signif),
            n_pos = sum(estimate > 0),
            n_neg = sum(estimate < 0)) %>%
  arrange(desc(abs(mean_estimate)))

mean_term_coef <- function(t, d = 2) {
 coef <- mean_term_coefs %>%
    filter(term == t) %>%
    pull(mean_estimate) %>%
    roundp(d)
 glue("$\\beta$ = {coef}")
}

mean_term_measure_coef <- function(meas, t) {
 coef <- mean_term_measure_coefs %>%
    filter(term == t, measure == meas) %>%
    pull(mean_estimate) %>%
    roundp()
 glue("$\\beta$ = {coef}")
}

most_opposite <- function(ex_terms) {
  terms <- mean_term_coefs %>%
    filter(!(term %in% ex_terms)) %>%
    mutate(larger = map2_dbl(n_pos, n_neg, max))
  min(terms$larger)
}

consistent_age <- lang_coefs %>%
  filter(effect == "interaction with age") %>%
  group_by(measure, term) %>%
  summarise(n_pos = sum(estimate > 0),
            n_neg = sum(estimate < 0)) %>%
  filter(n_pos >= 9 | n_neg >= 9)
```

```{r aoapred-coefs}
coef_order <- mean_term_coefs %>% pull(term)

plt_lang_coefs <- lang_coefs %>%
  mutate(term = term %>% factor(levels = rev(coef_order)) %>%
           fct_relabel(display_predictors),
         signif = if_else(signif, "significant", "non-significant") %>%
           fct_rev(),
         language = language %>% str_remove(" \\(.*\\)") %>% as_factor())

plt_lexcat_coefs <- lexcat_coefs %>%
  mutate(term = term %>% factor(levels = rev(coef_order)) %>%
           fct_relabel(display_predictors),
         # signif = if_else(signif, "significant", "non-significant"),
         lexical_category = lexical_category %>%
           fct_relevel("nouns", "predicates"),
         language = language %>% str_remove(" \\(.*\\)") %>% as_factor()) %>%
  mutate_at(vars(contains("signif")),
            ~if_else(., "significant", "non-significant") %>% fct_rev())

ref_coefs <- plt_lang_coefs %>% filter(language == "English")

mean_lexcat <- lexcat_coefs %>%
  group_by(lexical_category, term) %>%
  summarise(mean_estimate = mean(estimate),
            n_pos = sum(estimate > 0),
            n_neg = sum(estimate < 0)) %>%
  arrange(desc(abs(mean_estimate)))

mean_lexcat_coef <- function(t, lc) {
 coef <- mean_lexcat %>%
    filter(term == t, lexical_category == lc) %>%
    pull(mean_estimate) %>%
    roundp()
 glue("$\\beta$ = {coef}")
}
```

```{r aoapred-consistencies}
plt_coef_summary <- coef_summary %>%
  ungroup() %>%
  mutate(language = language %>% str_remove(" \\(.*\\)") %>% fct_rev())

plt_baseline_coef_summary <- baseline_coef_summary %>%
  ungroup() %>%
  mutate(language = language %>% str_remove(" \\(.*\\)") %>% fct_rev())

plt_lexcat_coef_summary <- lexcat_coef_summary %>%
  ungroup() %>%
  mutate(language = language %>% str_remove(" \\(.*\\)") %>% fct_rev(),
         lexical_category = lexical_category %>%
           fct_relevel("nouns", "predicates"))

plt_lexcat_baseline_coef_summary <- lexcat_baseline_coef_summary %>%
  ungroup() %>%
  mutate(language = language %>% str_remove(" \\(.*\\)") %>% fct_rev(),
         lexical_category = lexical_category %>%
           fct_relevel("nouns", "predicates"))

lexcat_mean_cor <- function(lc) {
  lexcat_cor <- lexcat_coef_summary %>%
    ungroup() %>%
    group_by(lexical_category) %>%
    summarise(mean_cor = mean(mean_cor)) %>%
    filter(lexical_category == lc) %>%
    pull(mean_cor) %>%
    roundp()
  glue("_r_ = {lexcat_cor}")
}
```

```{r aoapred-polysemy}
# polysemously split uni_lemmas
poly <- uni_model_data %>%
  ungroup() %>%
  distinct(language, uni_lemma) %>%
  filter(str_detect(uni_lemma, "\\(.*\\)")) %>%
  mutate(homonym = str_replace(uni_lemma, "(.*) \\(.*\\)", "\\1")) %>%
  group_by(language, homonym) %>%
  filter(n() > 1) %>%
  distinct(language, homonym) %>%
  ungroup() %>%
  count(language)
```

```{r aoapred-overlap}
# how much do uni_lemmas overlap across languages

overlap <- uni_model_data %>%
  ungroup() %>%
  distinct(language, uni_lemma) %>%
  group_by(uni_lemma) %>%
  summarise(n_langs = n()) %>%
  group_by(n_langs) %>%
  summarise(n = n()) %>%
  mutate(prop = n / sum(n))

in_range <- function(min_langs, max_langs) {
  round(sum(
    filter(overlap, n_langs >= min_langs, n_langs <= max_langs)$prop * 100
  ))
}
```

```{r aoapred-correlations}
# pairwise correlations among predictors

predictor_cors <- uni_model_data %>%
  ungroup() %>%
  select(language, uni_lemma, !!predictors) %>%
  distinct() %>%
  gather(predictor, value, !!predictors) %>%
  group_by(language) %>%
  nest() %>%
  mutate(cors = map(data, ~.x %>%
                    widyr::pairwise_cor(predictor, uni_lemma, value,
                                        upper = TRUE))) %>%
  select(-data) %>%
  unnest(cols = c(cors)) %>%
  rename(predictor1 = item1, predictor2 = item2)

mean_predictor_cors <- predictor_cors %>%
  group_by(predictor1, predictor2) %>%
  summarise(mean_cor = mean(correlation)) %>%
  arrange(desc(abs(mean_cor)))

mean_pair_cor <- function(p1, p2) {
  pair_cor <- mean_predictor_cors %>%
    filter(predictor1 == p1 & predictor2 == p2) %>%
    pull(mean_cor) %>%
    roundp()
  glue("_r_ = {pair_cor}")
}
```

```{r aoapred-collinearity}
# multicollinearity check

predictor_data <- uni_model_data %>%
  ungroup() %>%
  select(language, !!predictors) %>%
  distinct() %>%
  nest(data = c(frequency, MLU, final_frequency, solo_frequency, num_phons, 
    concreteness, valence, arousal, babiness))

predictor_vif <- function(lang_data, predictor) {
  others <- paste(predictors[predictors != predictor], collapse = ' + ')
  predictor_model <- glue("{predictor} ~ {others}") %>%
    as.formula() %>%
    lm(data = lang_data)
  1 / (1 - summary(predictor_model)$r.squared)
}

vifs <- predictor_data %>%
  mutate(vifs = map(data, function(lang_data) {
    tibble(predictor = predictors,
           vif = map_dbl(predictors,
                         ~predictor_vif(lang_data, .x)))
  })) %>%
  select(-data) %>%
  unnest(cols = c(vifs))
```
