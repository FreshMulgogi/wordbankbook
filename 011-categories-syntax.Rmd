# Vocabulary Composition: Syntactic^[An earlier version of this work was reported to the Boston University Conference on Language Development in 2015 by Braginsky, Marchman, Yurovsky, & Frank.] {#categories-syntactic}

This chapter focuses on splitting vocabulary data into syntactic categories and analyzing consistency and variability across languages in the acquisition of these. We quantify the “noun bias” across languages. In addition, we report the degree of bias for or against verbs and closed-class words. In Chapter \@ref(style), we consider variation of this sort within individuals. 

## Introduction

Over the first few years, young children are exposed to a "sea of words" across many different contexts and from many different people [p. 516, @goodman2008]. And despite the fact that children vary tremendously in the rate at which they learn, the first words that children utter are strikingly consistent [@tardif2008,@schneider2015]: They tend to talk about important people in their life (*mom*, *dad*), social routines (*hi*, *uh oh*), animals (*dog*, *duck*), and foods (*milk*, *banana*) [@goodman2008, @bates1995; @nelson1973; @clark1979b]. Soon thereafter, they begin to add verbs (*go*) and adjectives (*pretty*) in greater proportions than earlier in development and may even begin to use closed-class forms, such as determiners (*the*). These patterns seem to suggest a developmental course that follows distinct "waves" of learning for words from different classes. That is, along with early social routines, nouns tend to predominate early vocabularies, while other types of words, such as predicates and closed class forms, are learned later.

The composition of early vocabulary is complicated by the fact that we categorize words by their adult syntactic category. We do so in the discussion below without presupposing that children themselves do this categorization, however [@tomasello2000]. Children may be sensitive to these categories very early in development [@valian1986,@yang2013] or they may discover them either gradually [@pine1997] or more quickly [@meylan2017]. Importantly, though, we treat adult syntactic categories as an analytic convenience that describes certain regularities in how groups of words are distributed in language samples and how they function in different contexts, rather than as an ontological fact about children's knowledge. Chapter \@ref(aoapred) breaks down these categories further, asking what sorts of information predicts the order of acquisition for individual words, both within and across categories. 

```{r catsyn-bates, fig.cap="Figure from Bates et al. (1994), showing developmental trends in the categorical composition of early vocabulary"}
knitr::include_graphics("images/bates1994.png")
```


@bates1994 characterized these patterns of vocabulary composition in the following way. The figure above (reprinted from that paper) shows average vocabulary composition of nominals, predicates and closed class forms as a function of children's vocabulary size for English-speaking children from the original norming study of CDI: Words & Sentences form [@fenson1994]. Note that when children only know a few words (e.g., fewer than 50 words), the nominals comprise the greatest proportion of the words that children are reported to produce, with very few predicates or closed class forms (< 10%). As the children learn the next hundred words or so, the proportion of nominals increases even more dramatically with a gradual increase in the proportion of children's vocabularies that are predicates. Closed class forms remain a much smaller proportion over the period. Yet after about 300 words or so, children tend not add nouns to their vocabularies at the same pace that they did earlier in development, reflected in the proportion of nominals tending to decrease.^[This effect may also reflect aspects of CDI form design, e.g. "running out of nouns" to learn: children may increasingly learning nouns that are not on the forms.] It is during this developmental period that proportion of predicates tend to increase, followed by a growing proportion of closed class forms. 

Why do children learn nouns before verbs and other types of words? This question has received a great deal of attention in the literature, and we can briefly summarize some of the major issues here. One reason for this "noun bias" could be that nouns are simply more frequent in the talk to young children. It is well-established that children learn the words that they hear more often [e.g., @hart1995]. Many observational studies of English-speaking caregivers have demonstrated that caregivers use more nouns than verbs (types or tokens) with their children [e.g., @fernald1993; @goldfield1993; @gopnik1996; @kim2000; @poulindubois1995; @tardif1997].

Other researchers have framed the "noun bias" in terms of universals about what and how different words "partition" things in the world.For example, @gentner1978 has argued that children learn nouns before verbs because the meanings of nouns are easier to encode since they identify things that can be differentiated in the world (e.g., common everyday objects). Verbs and other predicates, in contrast, express *relations* among things in the world. Hence, the meanings of verbs are less accessible to children through common, everyday experiences and hence, are more difficult to map onto word forms without additional linguistic or social support. 

Other reasons that nouns might be easier than verbs for young children is that nouns tend to be less morphologically complex than verbs [e.g., @tardif1997]. For example, in many languages, nouns are typically marked only for number, whereas, verbs carry both person and tense information. In English, at least, verbs might also be harder to learn because they tend to occur in sentence-medial position (rather than sentence final), which make verbs less salient in the input that children hear [@slobin1985; @caselli1995].

Finally, differences in children's preferences for nouns vs. verbs might result from differences in what contexts children hear nouns vs. verbs in the speech from caregivers [e.g., @choi1995; @tardif1999]. Several researchers have examined what caregivers talk about using naturalistic data of caregiver-child interactions. For example, caregivers in some cultures tend to emphasize the names for objects, spending a great deal of time labeling objects for their children. In other cultures, caregivers do so much less frequently, instead focusing on the actions in which those objects engage [e.g., @fernald1993; @gopnik1996]. These differences in input to children can influence the words that are salient for children, and hence, the words that they are most likely to learn.

What is the evidence that a noun bias is a universal feature of children's vocabularies? Documenting the extent to which the noun bias is universal is relevant to understanding mechanisms of language learning, in particular, the presence of conceptual biases in early acquisition and the role of cross-cultural variability in the input that children receive from caregivers. The evidence varies across languages, as well as across methodologies (for example, naturalistic observation vs. parent report). Some studies find consistent evidence for a noun bias in English, as well as in Korean and Italian [@bates1994; @au1994; @caselli1995; @kim2000]. Other studies do not find evidence of a noun bias in languages as varied as French, German, Chinese, Estonian, and Korean [@bassano2000; @bloom1993; @choi1995; @kauschke2002; @tardif1996; @tardif1999; @schults2016]. 

<!-- Crosslinguistic variation suggests that the words that young children learn is less a function of universal conceptual biases and more likely due to structural features of the language and characteristics of the input that children receive when engaging with caregivers.  -->
In sum, identifying the extent of cross-linguistic variation vs. universals has been difficult since variation across studies may be due to the different methodologies that are used. For example, even within a single language, for example, Korean, parent reports of children's first words find a noun bias [e.g., @au1994], whereas, studies using direct observational methods find less evidence for this pattern [e.g., @gopnik1996]. Further, few studies have had the scope to directly compare the extent of the noun bias across multiple languages using a common methodology. 

One notable exception in a literature where samples have been small -- in terms of both languages and children -- is @bornstein2004, in which the researchers compared vocabulary composition in seven different languages. In this chapter, we follow this comparative approach [see also @tardif2008]. Since we have access to many more observations, our approach offers a more comprehensive approach than these earlier studies. Moreover, we attempt to quantify the estimates of the extent to which languages show a noun bias: we develop a statistical method for quantifying the extent of the noun bias across the entire developmental range in which a particular form is used. 

## Methods and Data

Each CDI form contains a mixture of words in different classes. We adopt the categorization of @bates1994, categorizing words into nouns, predicates (both verbs and adjectives), function words (also referred to as "closed class" words), and other words. For each child's vocabulary, we compute the proportion of the total words in each of these categories that they are reported to produce. Following the approach developed by @bates1994, for each of the languages in our sample, we plot these proportions against total vocabulary. Each dot represents a child's knowledge of a particular class, while curves show the relationship between a class and the whole vocabulary. If categories grow independently of one another, these curves should approximate the diagonal. 

```{r catsyn-items}
items <- items %>%
  filter(type == "word") %>%
  mutate(num_item_id = as.numeric(substr(item_id, 6, nchar(item_id))))
```

```{r catsyn-vocab_comp_fun}
get_vocab_comp <- function(input_language, input_form) {
  # print(paste(input_language,input_form))
  
  lang_vocab_items <- filter(items, 
                             language == input_language, 
                             form == input_form) %>%
    filter(lexical_category %in% c("nouns", "predicates", "function_words"))
  
  lang_vocab_data <- get_instrument_data(language = input_language,
                                         form = input_form,
                                         items = lang_vocab_items$item_id, 
                                         iteminfo = lang_vocab_items) %>%
    mutate(value = ifelse(is.na(value), "", value),
           produces = value == "produces",
           understands = value == "produces" | value == "understands") %>%
    select(-value) %>%
    gather(measure, value, produces, understands)
  
  num_words <- nrow(lang_vocab_items)
  
  lang_vocab_summary <- lang_vocab_data %>%
    group_by(data_id, measure, lexical_category) %>%
    summarise(num_true = sum(value),
              prop = sum(value) / n())
  
  lang_vocab_sizes <- lang_vocab_summary %>%
    summarise(vocab_num = sum(num_true),
              vocab = sum(num_true) / num_words)
  
  lang_vocab_summary %>%
    left_join(lang_vocab_sizes) %>%
    mutate(prop_vocab = num_true / vocab_num) %>%
    select(-num_true) %>%
    mutate(language = input_language, form = input_form)
}
```

```{r catsyn-vocab_comp, eval=FALSE}
instruments <- instruments %>%
  filter(form %in% c(WSs,WGs)) %>%
  select(language, form) %>%
  distinct()

vocab_comp_data <- map2(instruments$language,
                        instruments$form, get_vocab_comp) %>%
  bind_rows()

write_feather(vocab_comp_data, "data/vocab_comp_data.feather")
```


```{r catsyn-sample_sizes}
vocab_comp_data <- read_feather("data/vocab_comp_data.feather")

sample_sizes <- vocab_comp_data %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  select(language, form, n) %>%
  distinct() 

sample_sizes %>%
  datatable
```


We limit our analysis to traditional WS and WG forms (along with variants in these classes) because short forms like the British English TEDS don't typically include category information. The sample sizes included in this analysis are given above. 

```{r catsyn-schematic, fig.cap="Schematic of our vocabulary composition analysis."}
knitr::include_graphics("images/vocab_comp_schematic.png")
```

Building on the method of @bates1994, our analyses relies on plotting the proportion of words known in a particular category (e.g., nouns) in a child's vocabulary by the proportion of all words in the child's vocabulary. This approach is shown schematically above. If every time a child learns a word, that word is sampled randomly from the different words available on the form, then the proportion of nouns in vocabulary should track perfectly with the proportion of total vocabulary (the diagonal). In contrast, if nouns are over-represented, the child will be represented by a point above the diagonal; if nouns are under-represented, they will be under the diagonal.

```{r catsyn-area_funs}
get_lang_lexcat_predictions <- function(lang, lexcat) {
  model <- filter(models, language == lang, lexical_category == lexcat)$model[[1]]
  data.frame(vocab = pts,
             prop = predict(model, newdata = data.frame(vocab = pts)),
             lexical_category = lexcat,
             language = lang)
}

get_lang_predictions <- function(lang) {
  bind_rows(sapply(unique(demo_data$lexical_category),
                   function(lexcat) get_lang_lexcat_predictions(lang, lexcat),
                   simplify = FALSE))
}
```

```{r catsyn-plot_area_demo, fig.asp=0.5}
demo_lang <- "English (American)"
demo_data <- filter(vocab_comp_data, form == "WS", 
                    language == demo_lang) %>%
  mutate(panel = paste(language, "(data)"),
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", "function_words"),
                                   labels = c("Nouns", "Predicates", "Function Words")))
pts <- seq(0, 1, 0.01)

models <- demo_data %>%
  group_by(language, lexical_category) %>%
  do(model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1, data = .))

predictions <- bind_rows(sapply(demo_lang, get_lang_predictions, simplify = FALSE))

diagonal <- expand.grid(vocab = rep(rev(pts)),
                        language = demo_lang,
                        lexical_category = unique(demo_data$lexical_category))
diagonal$prop <- diagonal$vocab

area_poly <- bind_rows(predictions, diagonal) %>%
  mutate(panel = paste(language, "(models)"))

ggplot(predictions, 
       aes(x = vocab, y = prop)) +
  geom_point(data = demo_data, aes(x = vocab, prop, col = lexical_category), 
             alpha = .03) +
  facet_grid(. ~ lexical_category) +
  geom_line(aes(colour = lexical_category), size = 1) +
  geom_polygon(data = filter(area_poly, language == "English"),
               aes(fill = lexical_category), alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") + 
  scale_y_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_x_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_colour_solarized(guide = FALSE) +
  scale_fill_solarized(guide = FALSE) 
```

The figure above shows this analysis, carried out with English (American) WS data. Each point shows an individual child's vocabulary, and each panel shows a different lexical class (thus each child is represented once in each panel). We capture the overall trend in this plot by estimating a linear model over the data, predicting category proportion as a function of total production. This model is fit with third-order polynomials (so as to allow both concave/convex functions and also changes in convexity). We fit these 
models with the constraint that they must predict the point [1,1] so that they are guaranteed to arrive at the diagonal point in the special case that all words on a form are checked. These model fits are shown by the lines. 

```{r catsyn-resample}
poly_area <- function(group_data) {
  model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1,
              data = group_data)
  return((model$solution %*% c(1/4, 1/3, 1/2) - 0.5)[1])
}

sample_areas <- function(d, nboot = 1000) {
  
  counter <- 1
  sample_area <- function(d) {
    d_frame <- d %>%
      group_by(language, form, measure) %>%
      sample_frac(replace = TRUE) %>%
      group_by(language, form, measure, lexical_category) %>%
      do(area = poly_area(.)) %>%
      mutate(area = area[1]) %>%
      rename_(.dots = setNames("area", counter))
    
    counter <<- counter + 1 # increment counter outside scope
    print(counter)
    return(d_frame)
  }
  
  areas <- replicate(nboot, sample_area(d), simplify = FALSE)
  
  Reduce(left_join, areas) %>%
    gather(sample, area, -language, -form, -measure, -lexical_category)
}
```

```{r catsyn-areas_compute, eval=FALSE}
areas <- sample_areas(vocab_comp_data, 1000)
write_feather(areas,"data/vocab_comp_areas.feather")
```


```{r catsyn-areas_read}
areas <- read_feather("data/vocab_comp_areas.feather")
```

```{r catsyn-area_summary}
area_summary <- areas %>%
  group_by(language, form, measure, lexical_category) %>%
  summarise(mean = mean(area),
            ci_lower = ci_lower(area),
            ci_upper = ci_upper(area)) %>%
  ungroup() %>%
  mutate(language = factor(language),
         instrument = paste(language, form), 
         lexical_category = factor(lexical_category,
                                   levels = c("nouns", "predicates", 
                                              "function_words"),
                                   labels = c("Nouns", "Predicates", 
                                              "Function Words")))
      
```

The final step in our method is to capture the overall bias in a particular sample by estimating the difference in area between the curve and the diagonal. If the curve is substantially above the diagonal, this difference will be positive (indicating e.g., a positive noun bias). In contrast, if the curve is below the diagonal, the difference will be negative. To capture uncertainty in this area estimate, we conduct a resampling analysis where we randomly resample the population 1000 times with replacement, then recompute the area measurement. Confidence intervals below are based on this resampling procedure. 

Critically, this analysis controls for a number of confounds in previous analyses. First, because our interest is in the shape of the overall curve, under-representation of children in some age-band should add uncertainty but not bias. Of course, if data are too sparse, estimates will be unconstrained, but particulars of age sampling should not bias our estimates. Second, in principle, the analysis should not be biased by the number of items in a particular category, as the analysis is relative to the numerical representation of a particular class on the form. Thus, in principle we should be able to compare across forms with larger or smaller numbers of items in particular sections.

## Results

We present results of this analysis across languages, beginning with comprehension on WG-type forms and moving to production in WS-type forms. We do not analyze WG production data here. For the most part, production estimates on WG forms are quite low, and hence curves are relatively unconstrained (or determined by a small number of children who are reported to have very large early vocabulary). 

```{r catsyn-base_plot}
base_plot <- function(input_forms, input_measure) {
  vocab_comp_data %>%
    filter(form %in% input_forms, 
           measure == input_measure) %>%
    mutate(lexical_category = factor(lexical_category,
                                     levels = c("nouns", "predicates",
                                                "function_words"),
                                     labels = c("Nouns ", "Predicates ", 
                                                "Function Words")), 
           langform = interaction(language, form)) %>%
    ggplot(aes(x = vocab, y = prop, colour = lexical_category)) +
    facet_wrap(~langform) +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.5),
                       name = "Proportion of Category\n") +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.5),
                       name = "\nVocabulary Size") +
    scale_colour_solarized(name = "") +
    theme(legend.position = "top",
          legend.key = element_blank(),
          legend.background = element_rect(fill = "transparent"), 
          strip.text.x = element_text(size = 7))
}
```

### Comprehension: Words and Gestures


```{r catsyn-plot_points_wg_comp, fig.asp=1.25}
base_plot(WGs, "understands") + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3) +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE) 
```

Comprehension results are shown above. Overall, the largest trend that is visible in these plots is the under-representation of function words, with nouns and predicates appearing quite close to one another. For further comparison, we show summaries of curve areas for each language below. 


```{r catsyn-plot_areas_WG}
plot_areas_wg <- filter(area_summary, form %in% WGs, measure == "understands") %>%
         unite(langform, language, form, sep = " ") %>%
         mutate(langform = fct_reorder2(langform, mean, lexical_category, 
                                        .fun = function (x, y) {
                                          x[y=="Nouns"]}, .desc=FALSE))
                
ggplot(filter(plot_areas_wg, lexical_category != "Function Words"), 
       aes(x = langform, y = mean, colour = lexical_category)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~lexical_category) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  coord_flip() + 
  ylab("Relative representation (Comprehension)") +
  xlab("") + 
  ylim(-.15, .15)

npcor_wg <- cor.test(plot_areas_wg$mean[plot_areas_wg$lexical_category == "Nouns"],
                          plot_areas_wg$mean[plot_areas_wg$lexical_category == "Predicates"])
```

Nouns are over-represented in many -- but not all -- languages. Portuguese, Turkish, Korean, and Slovak, a set of typologically- and culturally-distinct languages,  show slight under representation, with BSL and British English showing the largest over-representation of nouns. Predicates are under-represented in some languages and over-represented in others.

```{r catsyn-prod_vs_comp_vocab_comp_wg}
area_scatter_data <- area_summary %>%
  filter(form %in% WGs, 
         lexical_category %in% c("Nouns","Predicates"), 
         measure == "produces") %>%
  unite(langform, language, form, sep = " ") %>%
  gather(variable, value, mean, ci_lower, ci_upper) %>%
  mutate(lex_var = str_c(lexical_category, variable, sep = "_")) %>%
  select(-lexical_category, -variable) %>%
  spread(lex_var, value)

ggplot(area_scatter_data, 
       aes(x = Nouns_mean, y = Predicates_mean)) + 
  geom_pointrange(aes(ymin = Predicates_ci_lower, ymax = Predicates_ci_upper)) + 
  geom_errorbarh(aes(xmin = Nouns_ci_lower, xmax = Nouns_ci_upper)) +
  geom_smooth(method = "lm") + 
  geom_label_repel(aes(label=langform), force = 4, alpha = .8, size = 3) +
  xlab("Noun bias") +
  ylab("Predicate bias") + 
  xlim(-.05,.15) + 
  ylim(-.2,.1)
```

As seen above, there is a strong anti-correlation between noun and predicate bias  measures ($r$(`r npcor_wg$parameter`) = `r signif(npcor_wg$estimate, 2)`). (This correlation should be interpreted with caution as nouns + predicates + function words are constrained to sum to 1, so some degree of correlation is built into the measure). 


```{r catsyn-function_words_comp}
ggplot(filter(plot_areas_wg, lexical_category == "Function Words"), 
       aes(x = langform, y = mean, colour = lexical_category)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~lexical_category) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  coord_flip() + 
  ylab("Relative representation (Comprehension)") +
  xlab("") + 
  ylim(-.5, .2)
```

Function words are substantially under-represented across nearly every language in our sample (except Slovak). Note the scale difference -- the function-word values are far more extreme than the noun and predicate values. These results likely reflect some combination of true under-representation of function-words as well as the difficulty of reporting on function-word comprehension in very early language (see Appendix \@ref(appendix-psychometrics) for more details on this issue). Such issues may also vary across cultures, languages, and administration methods. Function-word representation might plausibility differ due to linguistic factors such as morphological complexity, pronoun dropping, agreement, etc. However, it is also notable that the two lowest function-word scores come from Kiswahili and Kigiriama [@alcock2015], a study in which the parents may have had substantially less meta-linguistic awareness as a result of many being illiterate. 

### Production: Words and Sentences


```{r catsyn-plot_points_ws, fig.asp=1.25}
base_plot(WSs, "produces") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3) +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)
```

We next turn to production data from WS-type forms. We can immediately see the same function-word trend as was visible in comprehension. In addition, in many but not all languages, a noun bias is evident.

```{r catsyn-plot_areas_ws}
plot_areas_ws <- filter(area_summary, form %in% WSs, measure == "produces") %>%
         unite(langform, language, form, sep = " ") %>%
         mutate(langform = fct_reorder2(langform, mean, lexical_category, 
                                        .fun = function(x, y) {
                                          x[y=="Nouns"]}, .desc=FALSE))
                
ggplot(filter(plot_areas_ws, lexical_category != "Function Words"),
       aes(x = langform, y = mean, colour = lexical_category)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~lexical_category ) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  coord_flip() + 
  ylab("Relative representation (Production)") +
  xlab("") + 
  ylim(-.15, .15)

npcor <- cor.test(plot_areas_ws$mean[plot_areas_ws$lexical_category == "Nouns"],
                          plot_areas_ws$mean[plot_areas_ws$lexical_category == "Predicates"])
```
Turning to the language summaries, we see a larger pattern of variation in nouns and predicate representation. Every language has a relative over-representation of nouns, though the degree of this over-representation varies, with German and Korean especially high, and Mandarin and Cantonese especially low (we return to this trend below). Overall this trend is more extreme and more consistent in production data than comprehension. Predicate representation is both more variable and more negative than we observed for comprehension. Here we see Mandarin and Cantonese as the only two languages with substantial over-representation for predicates.

```{r catsyn-prod_vs_comp_vocab_comp}
area_scatter_data <- area_summary %>%
  filter(form %in% WSs, 
         lexical_category %in% c("Nouns","Predicates"), 
         measure == "produces") %>%
  unite(langform, language, form, sep = " ") %>%
  gather(variable, value, mean, ci_lower, ci_upper) %>%
  mutate(lex_var = str_c(lexical_category, variable, sep = "_")) %>%
  select(-lexical_category, -variable) %>%
  spread(lex_var, value)

ggplot(area_scatter_data, 
       aes(x = Nouns_mean, y = Predicates_mean)) + 
  geom_pointrange(aes(ymin = Predicates_ci_lower, ymax = Predicates_ci_upper)) + 
  geom_errorbarh(aes(xmin = Nouns_ci_lower, xmax = Nouns_ci_upper)) +
  geom_smooth(method = "lm") + 
  geom_label_repel(aes(label=langform), force = 4, alpha = .8, size = 3) +
  xlab("Noun bias") +
  ylab("Predicate bias")
```

Noun and predicate representation is again anti-correlated, at $r$(`r npcor$parameter`) = `r signif(npcor$estimate,2)`, though this correlation is somewhat weaker than for comprehension. 

```{r catsyn-function_words_prod}
ggplot(filter(plot_areas_ws, lexical_category == "Function Words"),
       aes(x = langform, y = mean, colour = lexical_category)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~lexical_category ) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  scale_colour_solarized(name = "", guide = FALSE) +
  coord_flip() + 
  ylab("Relative representation (Production)") +
  xlab("") + 
  ylim(-.5, .2)
```

The negative representation of function words is relatively consistent in overall magnitude with that seen in comprehension. Across all languages, children are reported to produce fewer function words than would be expected by chance sampling. 

### Reliability of bias estimates

```{r catsyn-consistency}
consistency <- area_summary %>%
  filter((form %in% WGs & measure == "understands") |
           (form %in% WSs & measure == "produces"),
         !(language == "Mandarin (Beijing)" & form == "WS")) %>% 
  # TC and IC are matched in Mandarin
  select(language, measure, lexical_category, mean) %>%
  spread(measure, mean) %>%
  filter(!is.na(produces), !is.na(understands)) 
  
 ggplot(consistency,
        aes(x = produces, y = understands, col = lexical_category)) + 
  geom_point() + 
  # facet_wrap(~lexical_category) + 
  geom_smooth(method = "lm") + 
  xlab("Relative Production") + 
  ylab("Relative Comprehension") + 
  scale_color_solarized(name = "Lexical Category") +
  theme(legend.position = "bottom")

cors <- consistency %>% 
  group_by(lexical_category) %>% 
  do(broom::tidy(cor.test(.$produces, .$understands)))
```

One natural question is how consistent estimates of bias are across comprehension and production. The plot above shows -- for the sample of languages in which we have data from matched WG- and WS-type instruments -- the relative bias we recovered in the analysis above. Somewhat surprisingly, correlations between these different instruments are quite low. Function word bias is negatively correlated between production and comprehension ($r$(`r cors$parameter[3]`) = `r signif(cors$estimate[3],2)`, $p$ = `r signif(cors$p.value[3],2)`). This result is likely due to Kiswahili and Kigiriama, discussed above, the lowest points for function word comprehension. But predicate bias estimates are close to uncorrelated with one another ($r$(`r cors$parameter[2]`) = `r signif(cors$estimate[2],2)`, $p$ = `r signif(cors$p.value[2],2)`), and the correlation between noun bias estimates is modest though positive ($r$(`r cors$parameter[1]`) = `r signif(cors$estimate[1],2)`, $p$ = `r signif(cors$p.value[1],2)`). 

This analysis is relatively low power; it only conducted with `r cors$parameter[1]` languages and hence is relatively low power (despite the many thousands of children necessary to carry it out). Despite this, it raises some important questions. A number of explanations of the data are consistent:

1. Bias differs between comprehension and production, such that differences are related to type of response. 

2. Estimates of bias are influenced by the composition of specific forms, so much so that WS- and WG-type forms yield radically different estimates of bias.

3. Bias differs developmentally. Perhaps different biases are evident earlier vs. later in acquisition. 

\noindent We assess each of these explanations in turn.


```{r catsyn-vocab_comp_plot}
vocab_comp_data %>%
    filter(form %in% "Oxford CDI") %>%
    mutate(lexical_category = factor(lexical_category,
                                     levels = c("nouns", "predicates",
                                                "function_words"),
                                     labels = c("Nouns ", "Predicates ", 
                                                "Function Words"))) %>%
    ggplot(aes(x = vocab, y = prop, colour = lexical_category)) +
    facet_wrap(~measure) +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.5),
                       name = "Proportion of Category\n") +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.5),
                       name = "\nVocabulary Size") +
    scale_colour_solarized(name = "") +
    theme(legend.position = "top",
          legend.key = element_blank(),
          legend.background = element_rect(fill = "transparent")) + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed") + 
  geom_jitter(size = 0.7, alpha = .3) +
  geom_smooth(method = "clm", formula = y ~ I(x ^ 3) + I(x ^ 2) + x - 1, 
              size = 1, se = FALSE)

```

In order to assess comprehension/production differences as a source of bias, we examine the Oxford CDI, which is relatively unique in that it includes comprehension questions even later in development. Data on production from standard WG forms is simply too sparse to perform our bias assessment method; since most children do not produce half of the words on the form, the shape of the bias curves is driven primarily by older children. In contrast, for the Oxford CDI, it is possible to compare directly. The measured noun bias for comprehension is  `r signif(filter(area_summary, form == "Oxford CDI", lexical_category == "Nouns", measure == "understands")$mean, digits = 2)`; for production it is `r signif(filter(area_summary, form == "Oxford CDI", lexical_category == "Nouns", measure == "produces")$mean, digits = 2)`. These values for predicates are `r signif(filter(area_summary, form == "Oxford CDI", lexical_category == "Predicates", measure == "understands")$mean, digits = 2)` and `r signif(filter(area_summary, form == "Oxford CDI", lexical_category == "Predicates", measure == "produces")$mean, digits = 2)`, respectively. These values are somewhat similar to one another, but they do vary beyond the average confidence interval on each (+/- `r signif(mean(filter(area_summary, form=="Oxford CDI")$mean - filter(area_summary, form=="Oxford CDI")$ci_lower), digits = 2)`). Thus, there is some evidence for production/comprehension asymmetries. 

<!-- ADD OTHER POTENTIAL SOURCES, including verb morphology (interacts with language), and reporting bias.  -->

What might be the mechanism for these aysmmetries? For languages that do not allow argument dropping, producing a predicate typically requires producing other words as well -- English-speaking children do not often produce bare predicates, for example. Thus, predicate production may be limited by other constraints on production in such languages; in contrast, predicate comprehension has no such limits. Further, felicitous predicate production requires some ability to combine words syntactically; in contrast, comprehension of predicates (especially verbs) can often be accomplished by guessing based on known arguments [e.g., @gillette1999]. For these reasons, there may be a greater bias against predicates in production compared with comprehension. This explanation is consistent with our data, in which the average production predicate bias is `r signif(mean(filter(area_summary, form %in% WSs, measure == "produces", lexical_category == "Predicates")$mean), digits = 2)`, while the average for comprehension is `r signif(mean(filter(area_summary, form %in% WGs, measure == "understands", lexical_category == "Predicates")$mean), digits = 2)`. Thus, production comprehension asymmetries likely explain some part of the differences we observed above.

Another potential explanation is that form composition relates to bias estimates. For example, a form with more predicates might actually show a *lower* degree of predicate bias -- more predicates on the form would imply that some of those predicates are relatively more difficult (simply because the form designer had "run out" of easy predicatess) and hence would not be checked as frequently by parents. We assess this hypothesis in two ways. First, we examine the relationship between predicate bias and predicate representation (making use of predicates because they are a minority on the form). Second, we consider the case of Mandarin where we have data from two forms with different compositions.

```{r catsyn-pred_prod}
pred_prod <- filter(area_summary, form %in% WSs, measure == "produces", 
                    lexical_category == "Predicates") %>%
  left_join(filter(items, form %in% WSs) %>% 
              group_by(language, form) %>% 
              summarise(prop_pred = mean(lexical_category == "predicates", na.rm=TRUE))) %>%
  mutate(langform =  paste(language, form)) 

ggplot(pred_prod,
       aes(x = prop_pred, y = mean, col = langform)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  geom_smooth(method = "lm", aes(group = 1)) + 
  ylab("Predicate Bias") + 
  xlab("Proportion Predicates on Form") + 
  scale_color_discrete(guide = FALSE) + 
  geom_label_repel(data = filter(pred_prod, prop_pred > .25), 
                   aes(label = langform))
```

As shown above, there is no reliable relationship between the proportion of predicates on a form and the predicate bias that is demonstrated ($r(`r cor.test(pred_prod$mean, pred_prod$prop_pred)$parameter`) = `r round(cor.test(pred_prod$mean, pred_prod$prop_pred)$estimate, digits = 2)`$, $p = `r round(cor.test(pred_prod$mean, pred_prod$prop_pred)$p.value, digits = 2)`$). Thus, a simple relationship between form composition and bias is not supported. On the other hand, it does appear that there is greater variance in this area for those languages with larger numbers of predicates on the form, and those languages with the highest predicate representation do have the highest number of predicates on the form as well. Perhaps the causality is reversed: Greater numbers of predicates have been included in forms for languages like Cantonese, Mandarin, and Korean where the predicate bias is an open theoretical question (or where the acquisition of predicates is of special interest). 

```{r catsyn-mandarin_form_comparison_munging}

mandarin_both_forms_items <- filter(items, 
                                    language == "Mandarin (Beijing)", 
                                    form %in% c("WS","TC")) %>%
  filter(lexical_category %in% c("nouns", "predicates", "function_words")) %>%
  group_by(definition) %>%
  filter(any(form == "TC") & any(form == "WS")) 

tc_items <- filter(mandarin_both_forms_items, form == "TC")
ws_items <- filter(mandarin_both_forms_items, form == "WS")
  
tc_vocab_data <- get_instrument_data(language = "Mandarin (Beijing)",
                                       form = "TC",
                                       items = tc_items$item_id, 
                                       iteminfo = tc_items) %>%
  mutate(value = ifelse(is.na(value), "", value),
         produces = value == "produces",
         understands = value == "produces" | value == "understands") %>%
  select(-value) %>%
  gather(measure, value, produces, understands)

ws_vocab_data <- get_instrument_data(language = "Mandarin (Beijing)",
                                       form = "WS",
                                       items = ws_items$item_id, 
                                       iteminfo = ws_items) %>%
  mutate(value = ifelse(is.na(value), "", value),
         produces = value == "produces",
         understands = value == "produces" | value == "understands") %>%
  select(-value) %>%
  gather(measure, value, produces, understands)

tc_num_words <- nrow(tc_items)
ws_num_words <- nrow(ws_items)

tc_vocab_summary <- tc_vocab_data %>%
  group_by(data_id, measure, lexical_category) %>%
  summarise(num_true = sum(value),
            prop = sum(value) / n())

ws_vocab_summary <- ws_vocab_data %>%
  group_by(data_id, measure, lexical_category) %>%
  summarise(num_true = sum(value),
            prop = sum(value) / n())


tc_vocab_sizes <- tc_vocab_summary %>%
  summarise(vocab_num = sum(num_true),
            vocab = sum(num_true) / tc_num_words)

ws_vocab_sizes <- ws_vocab_summary %>%
  summarise(vocab_num = sum(num_true),
            vocab = sum(num_true) / ws_num_words)

mandarin_all <- bind_rows(tc_vocab_summary %>%
                            left_join(tc_vocab_sizes) %>%
                            mutate(prop_vocab = num_true / vocab_num) %>%
                            select(-num_true) %>%
                            mutate(language = "Mandarin (Beijing)", 
                                   form = "TC"),
                          ws_vocab_summary %>%
                            left_join(ws_vocab_sizes) %>%
                            mutate(prop_vocab = num_true / vocab_num) %>%
                            select(-num_true) %>%
                            mutate(language = "Mandarin (Beijing)", 
                                   form = "WS"))
```

```{r catsyn-mandarin_form_comparison}
pts <- seq(0, 1, 0.01)

predictions <- mandarin_all %>%
  group_by(language, form, lexical_category) %>%
  do(model = clm(prop ~ I(vocab ^ 3) + I(vocab ^ 2) + vocab - 1, data = .)) %>%
  ungroup() %>%
  mutate(idx = 1:n()) %>%
  split(.$idx) %>%
  map_df(function(x) {
    data.frame(vocab = pts,
               prop = predict(x$model[[1]], newdata = data.frame(vocab = pts)),
               lexical_category = x$lexical_category,
               form = x$form,
               language = x$language)
  })
  

diagonal <- expand.grid(vocab = rep(rev(pts)),
                        language = "Mandarin (Beijing)",
                        form = c("WS","TC"),
                        lexical_category = unique(mandarin_all$lexical_category))
diagonal$prop <- diagonal$vocab

area_poly <- bind_rows(predictions, diagonal) %>%
  mutate(panel = paste(language, "(models)"))

mandarin_areas <- predictions %>%
  group_by(form, lexical_category) %>%
  summarise(area = sum(prop - vocab)/100)

ggplot(predictions, 
       aes(x = vocab, y = prop)) +
  geom_point(data = mandarin_all, aes(x = vocab, prop, col = lexical_category), 
             alpha = .03) +
  facet_grid(form ~ lexical_category) +
  geom_line(aes(colour = lexical_category), size = 1) +
  geom_polygon(data = filter(area_poly, language == "English"),
               aes(fill = lexical_category), alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") + 
  scale_y_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_x_continuous(limits = c(0, 1), breaks = c(),
                     name = "") +
  scale_colour_solarized(guide = FALSE) +
  scale_fill_solarized(guide = FALSE) 
```


The existence of two different forms for Mandarin opens the possibility of a further, more direct test of this issue. The Mandarin WS form [@tardif2009] and the Mandarin TC [toddler checklist; @hao2008] are completely independent forms but represent large datasets collected on Beijing Mandarin specifically. The Mandarin WS form is `r round(filter(pred_prod, language == "Mandarin (Beijing)", form == "WS")$prop_pred, 2)*100`% predicates and shows overall a `r round(filter(pred_prod, language == "Mandarin (Beijing)", form == "WS")$mean, 2)` predicate preference, while the TC form is `r round(filter(pred_prod, language == "Mandarin (Beijing)", form == "TC")$prop_pred, 2)*100`% predicates and shows overall a `r round(filter(pred_prod, language == "Mandarin (Beijing)", form == "TC")$mean, 2)` predicate preference. The intersection of these forms yields `r length(unique(mandarin_both_forms_items$definition))` items, with `r round(mean(mandarin_both_forms_items$lexical_category == "predicates"), digits = 2)*100`% predicates. Interestingly, the predicate representation for the TC and WS samples, analyzing *only* shared predicates still differs, if anything more substantially: for WS it is `r signif(filter(mandarin_areas, form=="WS", lexical_category=="predicates")$area, 2)` and for TC, `r signif(filter(mandarin_areas, form=="TC", lexical_category=="predicates")$area, 2)`. 

This result is worrisome -- these are samples from the same city and using the same items. They even include very similar age ranges: 16--30 month-olds and 17--30 month olds respectively, with approximately uniform sampling. The suggestion is then that differences in predicate bias can be substantial based on relatively minor details, such as specifics of administration or form context or specifics of sample composition. Despite the apparent stability of these estimates under resampling (confidence intervals for bias estimates are around +/- .005 in the analysis above), we should be cautious in over-estimating our degree of certainty in particular bias estimates. Further, these data provide more data against the notion that form composition (or at least the specific sample of predicates being assessed) is the primary determinant of bias. 

```{r catsyn-age_areas}
age_areas <- vocab_comp_data %>%
  filter(form %in% WSs, measure == "produces") %>%
  left_join(select(admins, data_id, age)) %>%
  filter(!is.na(age)) %>%
  mutate(langform = paste(language, form)) %>%
  group_by(langform) %>%
  mutate(age_group = ifelse(age > median(age), "older","younger")) %>%
  group_by(langform, lexical_category, age_group) %>%
  do(area = poly_area(.)) %>%
  mutate(area = area[1])

age_areas$langform = fct_reorder2(as.factor(age_areas$langform), age_areas$area, 
                                  age_areas$lexical_category, 
                                        .fun = function(x, y) {
                                          mean(x[y=="predicates"])}, .desc=FALSE)


ggplot(age_areas, aes(x = langform, y = area, col = age_group)) + 
  geom_point() + 
  facet_wrap(~lexical_category) +
  coord_flip()  + 
  xlab("")
```


The final hypothesis that we examine is that there are development differences in bias. Such differences would help to explain the observed differences between bias in early comprehension and later production.  To address this question, we split data from each language and form into older and younger groups at the median of the data for that sample. We then recomputed our bias estimates, shown above. There was a developmental difference such that older children showed less of a negative function word bias, but differences in noun and predicate bias were very slight for most languages. Thus, overall we do not see evidence that bias estimates for nouns and verbs are globally different for older vs. younger children. 

In sum, we did not find strong support for effects of form composition or age on bias estimates. Each of these factors of course could contribute in part to the mismatch between WG comprehension and WS production estimates, but neither one was particularly. On the other hand, data from the Oxford CDI suggested some differences in bias estimates between comprehension and production on the same form, with the bias against predicates and for nouns being substantially more pronounced for production than for comprehension. Further, data from two different Beijing Mandarin datasets suggest possible factors relating to population and administration. 


## Discussion

This chapter presented a comprehensive examination of the issue of biases for and against particular syntactic categories in acquisition. Building on earlier work by @bates1994, we created a quantitative measure of noun, predicate, and function word bias and examined variability in these measures across languages. Overall, a number of generalizations emerged. 

+ Nearly every language showed a positive bias for nouns, though the degree of this bias varied. + Every language showed a substantial bias against function words, supporting the generalization that these are acquired much later than content words, despite their typically higher frequency (see \@ref(aoa-pred)). This bias was larger on average than biases in type of content words. 
+ In comprehension there was variability in the degree of predicate representation; in production, as has previously been reported, languasges were mostly biased against predicates. There were a few notable exceptions among the East-Asian languages. 
+ Measures of bias in production and comprehension were not highly correlated with one another, especially for predicates and function words. There are likely many causes of these, but greater predicate comprehension compared with production appears to be one likely culprit. 




```{r catsyn-export_vocab_comp}
cvs <- area_summary %>%
  filter((form %in% WGs & measure == "understands") |
           (form %in% WSs & measure == "produces")) %>%
  group_by(measure, lexical_category) %>%
  summarise(cv = abs(sd(mean) / mean(mean))) %>%
  ungroup %>%
  unite(measure, lexical_category, measure, sep = " bias, ")

write_feather(cvs,"data/cvs/vocab_comp.feather")
```

