# Individual Words: Consistencies {#items}

## Baby's First Words are Consistent Across Languages

```{r load_consistency_data}
if(local_data) {
  wg_data <- read_feather("data/wg_data.feather")
  ws_sub_data <- read_feather("data/ws_sub_data.feather")
}
else {
  wg_langs <- instruments %>%
    filter(form == "WG") %>%
    distinct(language) %>%
    pull(language)
  
  ws_langs <- instruments %>%
    filter(form == "WS") %>%
    distinct(language) %>%
    pull(language)
  
  ws_sub_langs <- intersect(wg_langs, ws_langs)
  
  wg_data <- map( wg_langs, function(lang) {
    get_instrument_data(lang, "WG") %>%
      mutate(language = lang)
    }) %>%
    bind_rows
  
  ws_sub_data <-  map(ws_sub_langs, function(lang) {
    get_instrument_data(lang, "WS") %>%
      mutate(language = lang)
    }) %>%
    bind_rows
  
  write_feather(wg_data, "data/wg_data.feather")
  write_feather(ws_sub_data, "data/ws_sub_data.feather")
}

kid_wg_data <- wg_data %>%
  left_join(items) %>%
  left_join(admins) %>%
  filter(form == "WG", type == "word", !is.na(uni_lemma), !is.na(age)) %>%
  select(data_id, value, age, language, uni_lemma, category, lexical_category, lexical_class)

stitch_items <- items %>%
  filter(language %in% unique(ws_sub_data$language)) %>%
  group_by(language, lexical_category, uni_lemma) %>%
  summarise(n = n()) %>%
  filter(n > 1) %>%
  select(-n) %>%
  mutate(form = "WS") %>%
  left_join(items)

stitched_data <- ws_sub_data %>%
  left_join(stitch_items) %>%
  left_join(admins) %>%
  filter(!is.na(age)) %>%
  select(data_id, value, age, language, uni_lemma, category, 
         lexical_category, form) %>%
  bind_rows(kid_wg_data %>%
              filter(language %in% unique(ws_sub_data$language)) %>%
              mutate(form = "WG"))
```

Estimate aoas
```{r compute_aoa}
fit_rglm <- function(data) {
  
  if(any(str_detect(names(data), "produces")))
    model <- robustbase::glmrob(cbind(produces, total - produces) ~ age,
               family = "binomial",
               data = data)
  else
    model <- robustbase::glmrob(cbind(understands, total - understands) ~ age,
               family = "binomial",
               data = data)
  
  aoa <- -model$coefficients[["(Intercept)"]] / model$coefficients[["age"]]
  
  data.frame(uni_lemma = data$uni_lemma[1],
             language = data$language[1],
             lexical_category = data$lexical_category[1],
             rglm_aoa = aoa)

}

production_data <- stitched_data %>%
  group_by(form, language, lexical_category, age, uni_lemma) %>%
  summarise(produces = sum(value == "produces", na.rm = T), 
            total = n()) %>%
  filter(!is.na(uni_lemma)) %>%
  filter(!language %in% c("French (Quebec)", "Hebrew"))

both_form_data <- production_data %>%
  group_by(language, uni_lemma) %>%
  summarise(num_forms = length(unique(form))) %>%
  filter(num_forms > 1) %>%
  select(-num_forms) %>%
  left_join(production_data)

understanding_data <- kid_wg_data %>%
  group_by(language, lexical_category, age, uni_lemma) %>%
  summarise(understands = sum(value %in% c("understands", "produces"), 
                              na.rm = T),  
            total = n()) %>%
  filter(!is.na(uni_lemma)) %>%
  filter(!language %in% c("French (Quebec)", "Hebrew"))

production_aoas <- both_form_data%>%
  split(paste(.$language, .$uni_lemma, sep = "_")) %>%
  map(fit_rglm) %>%
  bind_rows %>%
  mutate(measure = "produces")

understanding_aoas <- understanding_data %>%
  split(paste(.$language, .$uni_lemma, sep = "_")) %>%
  map(fit_rglm) %>%
  bind_rows %>%
  mutate(measure = "understands")

all_aoas <- bind_rows(production_aoas, understanding_aoas)
```

```{r uni_lemma_completeness}
lemma_completeness <- all_aoas %>%
  group_by(measure, uni_lemma) %>%
  summarise(in_langs = sum(!is.na(rglm_aoa)))

lemma_completeness %>%
  group_by(measure, in_langs) %>%
  summarise(n = n()) %>%
  arrange(measure, desc(in_langs)) %>%
  mutate(prop = n/sum(n)) %>%
  mutate(cum_prop = cumsum(prop),
         cum_n = cumsum(n)) %>%
  datatable
```

```{r top10}
top10 <- all_aoas %>%
  group_by(measure,language) %>%
  arrange(rglm_aoa) %>%
  slice(1:10) %>%
  select(-rglm_aoa, -lexical_category) %>%
  mutate(order = 1:n()) %>%
  spread(language, uni_lemma) 

datatable(top10)
```

Understands and Produces
```{r understands_and_produces}
median_aoas <- all_aoas %>%
  group_by(measure, uni_lemma) %>%
  summarise(n = sum(!is.na(rglm_aoa)),
            rglm_aoa = median(rglm_aoa, na.rm = T)) %>%
  filter(n >= 6) %>%
  arrange(measure, rglm_aoa)


wide_medians <- median_aoas %>%
  spread(measure, rglm_aoa) %>%
  group_by(uni_lemma) %>%
  summarise_at(vars(n, produces, understands), mean, na.rm=T)

ggplot(wide_medians, aes(x = produces, y = understands, label = uni_lemma)) + 
  geom_text() +
  theme_bw()
```

Consistency across languages
```{r aoa_consistency}

aoa_corr <- function(df, lang1, lang2) {

 corrs <- df %>%
    select(-lexical_category, -measure) %>%
    filter(language == lang1 | language == lang2) %>%
    spread(language, rglm_aoa) %>%
    select(-uni_lemma) %>%
    cor(., use = "complete") 
 
 return(corrs[2,1])
}

compute_corrs <- function(aoas, top_n = NA) {
  
  if(!is.na(top_n)) {
    
    sub_words <- median_aoas %>%
      filter(measure == unique(aoas$measure)) %>%
      slice(median_aoas, 1:top_n)
    
    filtered_aoas <- aos %>%
      filter(uni_lemma %in% sub_words$uni_lemma)
  } 
  else 
    filtered_aoas <- aoas
  
  
  langs <- unique(filtered_aoas$language)
  
  lang_pairs <- combn(langs, 2) %>%
    t() %>%
    as_data_frame() %>%
    rename(lang1 = V1, lang2 = V2)

  
  lang_pairs %>%
    mutate(cor = unlist(map(1:nrow(lang_pairs), function(x) {
      aoa_corr(filtered_aoas,
               as.character(lang_pairs[x,"lang1"]), 
               as.character(lang_pairs[x,"lang2"]))})))

}


production_corrs <- compute_corrs(filter(all_aoas, measure == "produces"))

ggplot(production_corrs, aes(x = lang1, y = lang2, fill = cor)) +
  geom_tile() + 
  geom_text(aes(label = round(cor, 2))) +
  theme_bw()


understanding_corrs <- compute_corrs(filter(all_aoas, measure == "understands"))

ggplot(understanding_corrs, aes(x = lang1, y = lang2, fill = cor)) +
  geom_tile() + 
  geom_text(aes(label = round(cor, 2))) +
  theme_bw()
```

```{r step_corrs, eval = FALSE}
step_cors <- map(5:nrow(filter(median_aoas, measure == "produces")), 
                 function(x) mutate(compute_corrs(filter(all_aoas, measure == "produces"),
                                                  x), words = x)) %>%
  bind_rows

running_avg_corr <- step_cors %>%
  group_by(words, lang1) %>%
  summarise(cor = mean(cor, na.rm = T)) %>%
  summarise(mean = mean(cor), sd = sd(cor))


ggplot(running_avg_corr, aes(x = words, y = mean)) + 
  geom_ribbon(aes(ymin = mean-sd, ymax = mean+sd), alpha = .5, fill = "red") + 
  geom_line() +
  theme_bw()
```

<!-- Plot cross-linguistic acquisition order -->
<!-- ```{r} -->
<!-- aq.order.data <- baseline.order.data %>% -->
<!--   filter(measure == "produces") -->

<!-- quartz(width = 4,height = 3.3) -->
<!-- ggplot(aq.order.data,  -->
<!--        aes(x = mean.order, y = mean.order.diff,  -->
<!--            color = language, fill = language, -->
<!--            label=language)) + -->
<!--   geom_point(size=.8) + -->
<!--   geom_smooth(method="loess")+ -->
<!--   geom_dl(method = list(dl.trans(x=x +.15), "last.bumpup", cex=.7))+  -->
<!--   scale_color_brewer(palette = "Dark2") + -->
<!--   scale_fill_brewer(palette = "Dark2") + -->
<!--   scale_x_continuous(limits = c(0,150),breaks = seq(0,140,20), -->
<!--                      name = "Mean Cross-Linguistic Acquisition Order")+ -->
<!--   scale_y_continuous(name = "Cumulative Difference from Mean Order", -->
<!--                      limits = c(0,3200),breaks=seq(0,3200,400))+ -->
<!--   theme_bw(base_size=11) + -->
<!--   theme(panel.grid = element_blank(), legend.position="none") -->
<!-- ``` -->

<!-- Get frequency and phoneme data and merge. -->
<!-- ```{r} -->
<!-- eng.phons <- read_delim('mrc.phons.txt',delim='\t') -->

<!-- eng.freqs <- read.csv('english.freqs.csv') %>% -->
<!--   gather(word, count) %>% -->
<!--   group_by(word) %>% -->
<!--   summarize(frequency = mean(count)) %>% -->
<!--   ungroup() %>% -->
<!--   mutate(word = str_trim(tolower(gsub('\\.', ' ', word)))) %>% -->
<!--   left_join(eng.phons) -->

<!-- eng.cats <- all.data %>% -->
<!--   filter(language=="English") %>% -->
<!--   select(uni_lemma,category,lexical_category) %>% -->
<!--   distinct() %>% -->
<!--   filter(uni_lemma %in% common.lemmas) -->

<!-- held.out.order.preds <- order.mean.data %>% -->
<!--   filter(language == "English") %>% -->
<!--   select(uni_lemma,measure,held.out.order,held.out.prop) -->

<!-- freqs.data <- order.mean.data %>% -->
<!--  # select(-order,-mean.order,-lexical_category) %>% -->
<!--   select(language,order,uni_lemma,measure) %>% -->
<!-- #  spread(language,prop) %>% -->
<!--   spread(language,order) %>% -->
<!--   left_join(held.out.order.preds) %>% -->
<!--   left_join(eng.cats) %>% -->
<!--   mutate(word = gsub(' \\(.*\\)', '', uni_lemma)) %>% -->
<!--   left_join(eng.freqs) %>% -->
<!--   filter(!is.na(frequency)) %>% -->
<!--   group_by(measure) %>% -->
<!--   arrange(English) -->
<!--   #arrange(desc(English)) -->

<!-- ``` -->

<!-- Construct acquisition order models  -->
<!-- ```{r} -->
<!-- freqs.models <- freqs.data %>% -->
<!--   do(lm.freq = lm(English ~ category + log(1000*frequency) + phones,  -->
<!--                    data=.), -->
<!--      lm.lang = lm(English ~ held.out.order, data=.)) -->

<!-- get.model <- function(select.measure, model_type) { -->
<!--   filter(freqs.models, measure == select.measure)[[model_type]][[1]] -->
<!-- } -->

<!-- get.prediction.lm.cor <- function(model_type, meas, group_data) { -->
<!--     filtered_data <- filter(group_data, measure == meas) -->
<!--     predicted = (predict(get.model(meas, model_type), -->
<!--                                   newdata = filtered_data)) -->
<!-- #     predicted = predict(get.model(meas, model_type), -->
<!-- #                               newdata = filtered_data) -->
<!--     cor(filtered_data$English, predicted) -->
<!-- } -->

<!-- cum.model <- function(order.num) { -->

<!--   order.model.data <- freqs.data %>% -->
<!--     group_by(measure) %>% -->
<!--     filter(row_number() <= order.num) %>% -->
<!-- #       filter(row_number() >= order.num - 10, -->
<!-- #              row_number() <= order.num + 10) %>% -->
<!--     summarise(freq = get.prediction.lm.cor("lm.freq", unique(measure), .), -->
<!--               lang = get.prediction.lm.cor("lm.lang", unique(measure), .)) %>% -->
<!--   mutate(order.num = order.num) -->
<!-- } -->

<!-- cors <- bind_rows(sapply(seq(10, length(common.lemmas), 1), -->
<!--                          cum.model, simplify = FALSE)) %>% -->
<!--     gather(model,cor,freq,lang) -->


<!-- cors.plot.data <- cors %>% -->
<!--   filter(measure == "produces") %>% -->
<!--   select(-measure) %>% -->
<!--   mutate(model = factor(model,levels = c("lang","freq"),  -->
<!--                         labels = c("Cross-Linguistic","Distributional"))) -->
<!-- ``` -->

<!-- Plot predictive models -->
<!-- ```{r} -->
<!-- quartz(width = 4,height = 3.3) -->
<!-- ggplot(cors.plot.data,  -->
<!--        aes(x = order.num, y = cor,  -->
<!--            color = model, fill=model,label=model)) + -->
<!--   geom_point(size=1) + -->
<!--   geom_smooth(method="loess",span=.25)+ -->
<!--   geom_dl(method = list(dl.trans(x=x +.2), "smart.grid", cex=.8))+  -->
<!--   scale_color_brewer(palette = "Set1") + -->
<!--   scale_fill_brewer(palette = "Set1") + -->
<!--   scale_x_continuous(limits = c(0,140),breaks = seq(0,140,20), -->
<!--                      name = "English Acquisition Order")+ -->
<!--   scale_y_continuous(name = "Cumulative Correlation with English Order", -->
<!--                      limits = c(-.1,1),breaks=seq(-1,1,.25))+ -->
<!--   theme_bw(base_size=11) + -->
<!--   theme(panel.grid = element_blank(), legend.position="none") -->
<!-- ``` -->

<!-- Top words -->
<!-- ```{r} -->
<!-- top.words <- baseline.order.data %>% -->
<!--   group_by(language) %>% -->
<!--   arrange(desc(prop)) %>% -->
<!--   slice(1:10) %>% -->
<!--   select(language,uni_lemma,prop) -->

<!-- tab.words <- top.words %>% -->
<!--   mutate(order = 1:10) %>% -->
<!--   rowwise() %>% -->
<!--   select(language,uni_lemma,order) %>% -->
<!--   spread(language,uni_lemma) %>% -->
<!--   select(-order) -->

<!-- kable(tab.words) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- prod.data <- filter(freqs.data, measure=="produces") %>% -->
<!--   mutate(freq = inv.logit(predict(get.model("produces","lm.freq"))), -->
<!--          nor = inv.logit(predict(get.model("produces","lm.lang")))) %>% -->
<!--   gather(model,predicted,freq:nor) %>% -->
<!--   group_by(measure,model) %>% -->
<!--   mutate(resid = (English - predicted), -->
<!--          order = rank(-English)) -->

<!-- ggplot(prod.data, aes(x=order,y=resid,color=model))+ -->
<!--   geom_point() + -->
<!--   theme_bw() -->

<!-- model <- lm(English~ frequency*category,data=filter(eng.data,measure=="produces")) -->
<!-- model2 <- lm(English~ Norwegian,data=filter(eng.data,measure=="produces")) -->

<!-- cor.test(filter(eng.data,measure=="produces")$order,filter(eng.data,measure=="produces")$freq.order,method="kendall") -->

<!-- ``` -->



<!-- ```{r} -->
<!-- measure.diff <- order.mean.data %>% -->
<!--   select(-mean.order, -prop) %>% -->
<!--   spread(measure, order) %>% -->
<!--   group_by(language, uni_lemma) %>% -->
<!--   mutate(measure.diff = abs(produces - only.understands)) %>% -->
<!--   group_by(language) %>% -->
<!--   arrange(only.understands) %>% -->
<!--   mutate(mean.measure.diff = cumsum(measure.diff)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ggplot(measure.diff, aes(x = only.understands, y = mean.measure.diff, color = language)) + -->
<!--   geom_point() + -->
<!--   scale_color_brewer(palette = "Set1") + -->
<!--   theme_bw() + -->
<!--   theme(text = element_text(family = "Open Sans")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- kids.by.lang <- vocab.data %>% -->
<!--   group_by(language) %>% -->
<!--   summarise(num.kids = n()) -->

<!-- produced.data <- all.data %>% -->
<!--   filter(value == "produces") %>% -->
<!--   group_by(language,category,definition,gloss) %>% -->
<!--   summarise(mean.age = mean(age), -->
<!--             n = n()) %>% -->
<!--   group_by(language) %>% -->
<!--   left_join(kids.by.lang) %>% -->
<!--   mutate(prop = n/num.kids) %>% -->
<!--   arrange(desc(prop)) -->
<!-- ``` -->


<!-- Models -->
<!-- ```{r,} -->
<!-- model.words <- items %>% -->
<!--   select(definition,language,gloss,category) %>% -->
<!--   filter(language %in% unique(produced.data$language)) %>% -->
<!--   left_join(produced.data) %>% -->
<!--   rowwise() %>% -->
<!--   mutate(n = as.numeric(ifelse(is.na(n),0,n)), -->
<!--          prop = as.numeric(ifelse(is.na(prop),0,prop)), -->
<!--          len = nchar(definition)) -->

<!-- outputs <- NULL -->
<!-- components = c("count","zero") -->
<!-- params = c("len") -->

<!-- languages <- unique(model.words$language)  -->

<!-- predict.params <- expand.grid(language = languages, -->
<!--                               component = components, -->
<!--                               param = params)%>% -->
<!--   arrange(language,component,param) -->

<!-- for(lang in languages) { -->

<!--   hurd <- hurdle(n ~ len,  -->
<!--                  data = filter(model.words,language==lang)) -->



<!--   for(component in components) { -->

<!--     model.outs <- summary(hurd)$coefficients[as.character(component)][[1]] -->

<!--     outputs <- rbind(outputs,model.outs[c("len"), -->
<!--                                         c("Estimate", "Std. Error","Pr(>|z|)")]) -->
<!--   } -->

<!-- } -->
<!-- colnames(outputs) = c("estimate","se","p") -->

<!-- predict.params <- cbind(predict.params,outputs) %>% -->
<!--   mutate(ci = 1.96*se) %>% -->
<!--   mutate(component = factor(component, labels = c("Count Estimate", "Hurdle"))) -->
<!-- ``` -->

<!-- Plot parameters -->
<!-- ```{r, fig.width=5.5, fig.height=5} -->
<!-- ggplot(data = predict.params,  -->
<!--        aes(x = language, y = estimate, fill=component))+ -->
<!--   geom_histogram(stat="identity",position="identity")+ -->
<!--   geom_linerange(aes(ymax =estimate+ci, -->
<!--                       ymin = estimate-ci)) + -->
<!--   facet_grid(component ~ param)+ -->
<!--   ylab("Parameter Estimate (+/- 95% CI)")+ -->
<!--   xlab("Dataset") +  -->
<!--   geom_hline(yintercept=0, lty=2,size=.7) +  -->
<!--   #scale_color_brewer(name="Dataset",palette="Set1")+ -->
<!--   scale_fill_brewer(palette="Set1")+ -->
<!--   theme_bw(base_size=14) + -->
<!--   theme(legend.position="none", -->
<!--         axis.text.x = element_text(angle=-45, hjust = 0), -->
<!--         axis.title.x = element_text(vjust=-0.5), -->
<!--         panel.grid = element_blank()) -->
<!-- ``` -->



Look at age distributions.

```{r,fig.width=3,fig.height=8}
# all.by.age <- vocab.admins %>%
#   group_by(language,age) %>%
#   summarise(all.n = n())
# 
# age.hists <- vocab.data %>%
#   group_by(language,age) %>%
#   summarise(n = n()) %>%
#   left_join(all.by.age) %>%
#   mutate(prop = n/all.n) %>%
#   mutate(prop = prop/sum(prop))
# 
# ggplot(data = age.hists,aes(x=age,y=prop)) +
#   facet_grid(language ~ .) +
#   geom_histogram(stat="identity",
#                  fill="steelblue") +
#   geom_vline(xintercept=12,linetype="dashed")+
#   scale_x_continuous(limits=c(8,16),breaks=seq(8,16),name="Age (months)") +
#   scale_y_continuous(name = "Proportion of Children") +
#   theme_bw()+
#   theme(panel.grid=element_blank()) 
```

## Demographic Effects on Individual Words

Demographic effects. In this part, we focus on demographic differences in the learning of individual words. Braginsky et al. (2016b) reported on gender differences in the particular words in early vocabulary; we generalize this analysis to birth order and socio-economic status



## Predicting Which Words are Harder or Easier

AOA PRED

Predicting early learning. Braginsky et al. (2016a) provides an analysis in which independent data sources (including transcripts of child-directed speech) are brought to bear as predictors of acquisition differences between words. This “age of acquisition prediction” model allows us to quantify the consistency and variability in predictor values across languages for particular words, using the UL approach to cross-linguistic analysis. 

By-item stuff like overall item trajectories, `gender-input`, and `uni_lemma` based cross-linguistic consistency things.
